<?xml version="1.0" encoding="utf-8" standalone="yes"?><search><entry><title>Docker 搭建 Keepalived 实现 Nginx 双机热备</title><url>/2022/07/10/docker-keepalived-nginx-ha/</url><categories><category>架构</category></categories><tags><tag>Docker</tag><tag>Keepalived</tag><tag>Nginx</tag><tag>HA</tag></tags><content type="html"><![CDATA[
docker 搭建 keepalived 实现 nginx 双机热备
❯ docker run --privileged -d --name node1 debian:11 top -b ❯ docker run --privileged -d --name node2 debian:11 top -b –-privileged 是指以特权模式启动容器，否则 keepalived 无法成功生成虚拟 IP
分别进入 node1、node2 容器节点（docker exec -it node1 /bin/bash 和 docker exec -it node2 /bin/bash ）
安装以下软件 apt update &amp;&amp; apt install curl vim iproute2 inetutils-ping psmisc net-tools systemctl nginx keepalived -y
创建以下两个文件：
vi /etc/keepalived/chk_nginx.sh
#!/bin/bash echo $(date) &#34;start check nginx...&#34; &gt;&gt; /etc/keepalived/check_nginx.log counter=$(systemctl status nginx | grep running | wc -l) if [ &#34;${counter}&#34; = &#34;0&#34; ]; then echo $(date) &#34;nginx is not running, restarting...&#34; &gt;&gt; /etc/keepalived/check_nginx.log systemctl start nginx sleep 2 counter=$(systemctl status nginx | grep running | wc -l) if [ &#34;${counter}&#34; = &#34;0&#34; ]; then echo $(date) &#34;nginx is down, kill all keepalived...&#34; &gt;&gt; /etc/keepalived/check_nginx.log killall keepalived fi fi 脚本作用是查看是否存在 nginx 进程，不存在就重启 nginx，然后再查看一次，还不存在就杀掉 keepalived 进程（这样备既就会自动上线） 脚本权限需设置为 755（chmod 755 /etc/keepalived/chk_nginx.sh），否则 keepalived 会认为它不安全 注意首行一定是 #!/bin/bash，而不是 #/bin/bash，否则脚本不会被 keepalived 执行
node1 容器节点为虚拟 IP 172.17.0.201 的主节点、虚拟IP 172.17.0.202 的备用节点， keepalived 配置文件如下：
vi /etc/keepalived/keepalived.conf
! Configuration File for keepalived global_defs { router_id localhost script_user root enable_script_security } vrrp_script chk_http_port { script /etc/keepalived/chk_nginx.sh interval 10 # 间隔几秒执行脚本（注意最好大于脚本的执行时间） weight -20 } vrrp_instance VI_1 { state MASTER # 1、备机改为：BACKUP interface eth0 # 2、替换为实际网卡名称，可使用 ifconfig 或 ip addr 查看 virtual_router_id 51 priority 100 # 3、备机优先级设置小一点，例如：90 advert_int 1 authentication { auth_type PASS auth_pass 1111 } track_script { chk_http_port } virtual_ipaddress { 172.17.0.201 # 虚拟 IP（和 node1、node2 容器节点在同一网段的任意闲置 IP 即可） } } vrrp_instance VI_2 { state BACKUP interface eth0 # virtual_router_id 52 priority 90 advert_int 1 authentication { auth_type PASS auth_pass 1111 } track_script { chk_http_port } virtual_ipaddress { 172.17.0.202 } } node2 容器节点为虚拟 IP 172.17.0.202 的主节点、虚拟IP 172.17.0.201 的备用节点， keepalived 配置文件如下：
vi /etc/keepalived/keepalived.conf
! Configuration File for keepalived global_defs { router_id localhost script_user root enable_script_security } vrrp_script chk_http_port { script /etc/keepalived/chk_nginx.sh interval 10 weight -20 } vrrp_instance VI_1 { state BACKUP interface eth0 virtual_router_id 51 priority 90 advert_int 1 authentication { auth_type PASS auth_pass 1111 } track_script { chk_http_port } virtual_ipaddress { 172.17.0.201 } } vrrp_instance VI_2 { state MASTER interface eth0 virtual_router_id 52 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 1111 } track_script { chk_http_port } virtual_ipaddress { 172.17.0.202 } } node1 和 node2 容器节点互为主备， keepalived 配置文件除过注释标注的 3 处不同外，其余配置项保持一致即可 可使用 keepalived -t 检测配置文件是否有误
为了验证主备切换效果，可分别在主备 nginx 默认界面添加不同信息加以区分（vi /var/www/html/index.nginx-debian.html）
1、分别启动 node1 和 node2 容器节点的 keepalived（systemctl start keepalived）
2、在 node1 节点容器使用 ifconfig 或 ip addr 命令可看到绑定的虚拟 IP 172.17.0.201，此时通过访问虚拟 IP：curl http://172.17.0.201 展示的是 node1 节点的 nginx 信息
3、在 node2 节点容器使用 ifconfig 或 ip addr 命令可看到绑定的虚拟 IP 172.17.0.202，此时通过访问虚拟 IP：curl http://172.17.0.202 展示的是 node2 节点的 nginx 信息
4、故意改错 node1 上的 nginx 配置文件让其无法启动，并且停止 nginx（systemctl stop nginx）（模拟宕机情况），node1 节点的 keepalived 检测到 nginx 进程不存在，然后尝试启动 nginx，发现启动失败，所以会杀掉 keepalived 进程，释放虚拟 IP 172.17.0.201
5、node2 节点 keepalived 感知到 node1 节点下线后会绑定虚拟 IP 172.17.0.201 接管请求，完成主备切换。此时在 node2 节点容器使用 ifconfig 或 ip addr 命令可看到绑定的虚拟 IP 172.17.0.201 和 172.17.0.202，此时通过访问 curl http://172.17.0.201 和 curl http://172.17.0.201 展示的都是 node2 节点的 nginx 信息
6、当恢复 node1 节点 nginx 配置文件并重新加载，然后启动 keepalived，在 node1 容器节点使用 ifconfig 或 ip addr 命令可再次看到绑定的虚拟 IP 172.17.0.201，此时通过访问 curl http://172.17.0.201 展示的又是 node1 节点的 nginx 信息，访问 curl http://172.17.0.201 展示的是 node2 节点的 nginx 信息
]]></content></entry><entry><title>快速排序理解</title><url>/2022/03/19/quick-sort/</url><categories><category>排序算法</category></categories><tags><tag>排序算法</tag><tag>快速排序</tag></tags><content type="html"><![CDATA[快速排序算法核心思想，取待排序序列中的某个元素作为分区点，大于分区点的元素挪到分区点右边（从小到大排序），小于分区点的元素挪到分区点左边。然后分区点左右两边的子序列循环以上操作，直至子序列长度为 1。
左右指针法实现思路
1、首先定义分区点（pivot）p，p 一般为数组 a 的第一个元素或最后一个元素 2、然后定义左（l）、右（r）两个指针分别指向数组的第一个元素（a[0]）和最后一个元素 (a[a.length - 1]) 3、如果 a[l] &gt; a[p]，l、p 下标元素互换，l 前进 1 位 4、如果 a[r] &lt; a[p]，r、p 下标元素互换，r 后退 1 位 5、如果 l &gt;= r，排序结束
快速排序左右指针法图解过程
代码实现
以递归方式实现编码，首先找出分析出递归条件：
1、递归方程：quickSort(a[l..r]) = quickSort(a[l..p-1]) + quickSort(a[p+1..r]) 2、递归退出条件：l &gt;= r
注意点：如果选择分区点 p = l，必须先从右边找到小于 a[p] 的第一个元素开始
/** * 快速排序 * * @param a 待排序数组 * @param l 第一个元素下标 * @param r 最后一个元素下标 */ public static void sort(int[] a, int l, int r) { if (a == null || l &gt;= r) { return; } int i = l, j = r; int p = l; // 选择最左边的元素为 pivot while (l &lt; r) { // 如果选择 p = l 必须先从右边找到小于 a[p] 的第一个元素 while (l &lt; r &amp;&amp; a[r] &gt;= a[p]) { r--; } swap(a, r, p); p = r; // 从左边找到大于 a[p] 的第一个元素 while (l &lt; r &amp;&amp; a[l] &lt;= a[p]) { l++; } swap(a, l, p); p = l; System.out.println(Arrays.toString(a)); } sort(a, i, p - 1); sort(a, p + 1, j); } public static swap(int[] a, int i, int j) { int tmp = a[i]; a[i] = a[j]; a[j] = tmp; } ]]></content></entry><entry><title>堆排序理解</title><url>/2022/03/19/heap-sort/</url><categories><category>排序算法</category></categories><tags><tag>排序算法</tag><tag>堆排序</tag></tags><content type="html"><![CDATA[堆排序的关键是构建大（小）顶堆，堆顶元素就是最大（小）的元素，然后堆顶元素和末尾元素交换位置，再次堆化除最后一个元素外的其它元素，循环次过程即可完成排序。
翻译成代码如下：
public void sort(int a) { for(int i = a.length - 1; i &gt; 0; i--) { buildHeap(a, i); // 堆顶元素和最后一个元素交换，除过最后一个元素外其它元素再次构建大顶堆 swap(a, 0, i); } } 堆化过程
1、从序列最后一个非叶子节点开始 2、堆化规则：替换节点为当前节点和叶子节点中值最大的节点 3、倒序依次处理其它非叶子节点 4、须保证叶子节点也满足堆化规则
前置知识
1、堆是完全二叉树 2、满二叉树使用数组存储最省空间 3、若节点在数组中的下标为 n，其左叶子节点在数组中的下标为 2 * n + 1，右子节点下标为 2 * n + 2，父节点下标为 (n - 2)/2
堆化过程图解
翻译成代码如下：
/** * 构建大顶堆 * * @param a 数组 * @param n 最后一个元素下标 */ public static void buildHeap(int[] a, int n) { // 从最后一个非叶子节点开始，倒序依次处理其它非叶子节点 for(int i = (n -1) / 2; i &gt;=0; i--) { heapify(a, n, i); } } /** * 堆化 * * @param a 数组 * @param n 最后一个元素下标 * @param i 需要调整的父节点下标 */ public static void heapify(int[] a,int n, int i) { while (true) { // 大顶堆规则：替换节点为当前节点和叶子节点中值最大的节点 int maxPos = i; int l = 2 * i + 1; if (a[l] &gt; a[maxPos]) { maxPos = l; } int r = 2 * i + 2; if (a[r] &gt; a[maxPos]) { maxPos = r; } // 当前节点最大时退出 if (maxPos == i) { return; } swap(a, maxPos, i); // 保证叶子节点也满足堆化规则 i = maxPos; } } public static void swap(int[] a, int i, int j) { int tmp = a[i]; a[i] = a[j]; a[j] = tmp; } ]]></content></entry><entry><title>KMP 算法理解</title><url>/2022/03/12/the-kmp/</url><categories><category>算法</category></categories><tags><tag>KMP</tag><tag>字符串匹配</tag></tags><content type="html"><![CDATA[字符串前缀 与字符串后缀 字符串前缀(Proper prefix) ：包含第一个字符，不包含最后一个字符的所有子串 例如：abababca 的前缀：a、ab、aba、abab、ababa、ababab、abababc
字符串后缀(Proper suffix)：不包含第一个字符，包含最后一个字符的所有子串 例如：abababca 的后缀：a、ca、bca、abca、babca、ababca、bababca
字符串部分匹配表 字符串部分匹配表 (Partial Match Table) 也称为 next 数组，例如：abababca 的部分匹配表为：
char a b a b a b c a index 0 1 2 3 4 5 6 7 value 0 0 1 2 3 4 0 1 每列 value 值表示前 index + 1 个字符子串的最大字符串前缀与字符串后缀相匹配的长度，例如：
index 为 0 的子串为 a，没有字符串前缀和字符串后缀，所以 value 为 0 index 为 1 的子串为 ab，字符串前缀为 a，字符串后缀为 b，没有相匹配的字符串前缀与字符串后缀，所以 value 为 0 index 为 2 的子串为 aba，字符串前缀为 a、ab，字符串后缀为 a、ba，字符串前缀与字符串后缀相匹配的子串为 a，长度为 1，所以 value 为 1 index 为 3 的子串为 abab，字符串前缀为 a、ab、aba，字符串后缀为 b、ab、bab，字符串前缀与字符串后缀相匹配的子串为 ab，长度 2，所以 value 为 2 index 为 4 的子串为 ababa，字符串前缀为 a、ab、aba、abab，字符串后缀为 a、ba、aba、baba，字符串前缀与字符串后缀相匹配的子串为 a、aba，长度最大的为 aba ，所以 value 为 3 &hellip; index 为 7 的子串为 abababca，字符串前缀为 a、ab、aba、abab、ababa、ababab、abababc，字符串后缀为 a、ca、bca、abca、babca、ababca、bababca，字符串前缀与字符串后缀相匹配的子串为 a，所以 value 为 1 KMP 算法思路 KMP 算法就是利用字符串部分匹配表可以计算出当模式串与主串不匹配时，模式串可以多后移几位 (默认后移 1 位)
当模式串与主串不匹配时，如果不匹配字符对应模式串下标大于 0 (非首个模式串字符)，取此字符前一个字符对应字符串部分匹配表中的 value ，如果 value 大于 0，模式串中不匹配字符之前 (不含不匹配字符) 子串长度减去 value 即模式串为后移的位数。
暴力匹配算法当模式串和主串不匹配时，主串匹配下标 +1，模式串匹配下标置为 0，KMP 算法优化点在于模式串匹配下标置为 value。
例如在主串 bacbababaabcbab 中查找模式串 abababca
第一次符合以上规则的情况如下，模式串与主串不匹配字符 (b) 前一个字符为 a，对应字符串部分匹配表 index 为 0，value 为 0，所以不存在多后移情况 bacbababaabcbab | abababca 第二次符合以上规则的情况如下 bacbababaabcbab | abababca 模式串与主串不匹配字符 (b) 前一个字符是 a，对应字符串部分匹配表 index 为 4，value 为 3，不匹配字符之前模式串为 ababa 长度为 5， 所以后移 5-3=2 位 bacbababaabcbab | abababca 模式串与主串不匹配字符 (b) 前一个字符是 a，对应字符串部分匹配表 index 为 2，value 为 1，不匹配字符之前模式串为 aba 长度为 3， 所以后移 3-1=2 位 bacbababaabcbab | abababca 此时，模式串长度已经比剩余主串长，匹配结束。 代码实现 next 数组的代码实现思路参考 KMP 算法的 Next 数组详解 Golang 暴力匹配代码实现
// 暴力匹配 func Bf(s, p string) int { n := len(s) m := len(p) if n &lt; m { return -1 } for i := 0; i &lt;= n-m; i++ { j := 0 for j &lt; m { // 如果主串与模式串不匹配，则主串向右移动一个字符,模式串从头开始匹配 if s[i+j] != p[j] { break } j++ } if j == m { return i } } return -1 } Golang KMP 代码实现
func Kmp(s, p string) int { n := len(s) m := len(p) if n &lt; m { return -1 } next := GetNext(p) for i := 0; i &lt;= n-m; i++ { j := 0 for j &lt; m { // 暴力匹配算法当模式串和主串不匹配时，主串匹配下标 +1，模式串匹配下标置为 0， // KMP 算法优化点在于将模式串下标置为不匹配字符前一个字符对应 next 数组的值 if s[i+j] != p[j] { // 当模式串与主串不匹配时，如果不匹配字符对应模式串下标大于 j &gt; 0 (非首个模式串字符)， // 并且此字符前一个字符对应字符串部分匹配表中的值 next[j - 1] 也大于 0， // j - next[j - 1] 即模式串为后移的位数，等价于 j 置为 next[j - 1] if j != 0 &amp;&amp; next[j-1] != 0 { // j 后移 j-next[j-1]，等价于 j = next[j-1] j = next[j-1] } else { break } } j++ } if j == m { return i } } return -1 } func GetNext(p string) []int { n := len(p) next := make([]int, n) next[0] = 0 k := 0 // 根据已知 next 数组的前 i-1 位推测第 i 位 for i := 1; i &lt; n; i++ { for k &gt; 0 &amp;&amp; p[k] != p[i] { // k 为 p[0, i) 子串最大匹配前后缀长度 // p[0, k) 为 p[0, i) 子串最大匹配前缀子串 // 若：1、p[k] != p[i]，则求 p[0, i] 子串最大匹配前后缀长度问题 // 转换成了求 p[0, k) 子串最大匹配前后缀长度问题 // 循环直到 p[k] == p[i] (下一步处理) 或 k == 0 k = next[k] } // 若：2、p[k] == p[i]，则 p[0, i] 子串最大匹配前后缀长度为 k + 1 if p[k] == p[i] { k++ } next[i] = k } return next } Java KMP 代码实现
package io.github.ehlxr.algorithm.match; import java.util.Arrays; /** * 字符串匹配算法 KPM * * @author ehlxr * @since 2022-03-13 10:06. */ public class Kmp { public static void main(String[] args) { String s = &#34;bacbababaabcbab&#34;; String p = &#34;abab&#34;; System.out.println(Arrays.toString(getNexts(p))); System.out.println(kmp(s, p)); } public static int kmp(String s, String p) { char[] scs = s.toCharArray(); char[] pcs = p.toCharArray(); int m = s.length(); int n = p.length(); int[] next = getNexts(p); for (int i = 0; i &lt;= m - n; i++) { int j = 0; for (; j &lt; n; j++) { if (scs[i + j] != pcs[j]) { // 暴力匹配算法当模式串和主串不匹配时，主串匹配下标 +1，模式串匹配下标置为 0， // KMP 算法优化点在于将模式串下标置为不匹配字符前一个字符对应 next 数组的值 if (j &gt; 0 &amp;&amp; next[j - 1] &gt; 0) { // 当模式串与主串不匹配时，如果**不匹配字符**对应模式串下标大于 j &gt; 0 (非首个模式串字符)， // 并且此字符前一个字符对应字符串部分匹配表中的值 next[j - 1] 也大于 0， // j - next[j - 1] 即模式串为后移的位数，等价于 j 置为 next[j - 1] j = next[j - 1]; } else { break; } } } if (j == n) { return i; } } return -1; } private static int[] getNexts(String p) { int m = p.length(); char[] b = p.toCharArray(); int[] next = new int[m]; next[0] = 0; int k = 0; // 表示前后缀相匹配的最大长度 // 根据已知 next 数组的前 i-1 位推测第 i 位 for (int i = 1; i &lt; m; ++i) { while (k != 0 &amp;&amp; b[k] != b[i]) { // k 为 b[0, i) 子串最大匹配前后缀长度 // b[0, k) 为 b[0, i) 子串最大匹配前缀子串 // 若：1、b[k] != b[i]，则求 b[0, i] 子串最大匹配前后缀长度问题 // 转换成了求 b[0, k) 子串最大匹配前后缀长度问题 // 循环直到 b[k] == b[i] (下一步处理) 或 k == 0 k = next[k]; } // 若：2、b[k] == b[i]，则 b[0, i] 子串最大匹配前后缀长度为 k + 1 if (b[k] == b[i]) { ++k; } next[i] = k; } return next; } } 参考 The Knuth-Morris-Pratt Algorithm in my own words ]]></content></entry><entry><title>JVM TLAB</title><url>/2021/07/27/jvm-tlab/</url><categories><category>JVM</category></categories><tags><tag>JVM</tag><tag>TLAB</tag><tag>JAVA</tag></tags><content type="html">TLAB（Thread Local Allocation Buffer） 线程本地分配缓存区
由于对象一般分配在堆上，而堆是线程共用的，因此可能会有多个线程在堆上申请空间，而每一次的对象分配都必须加锁保证线程同步，会使分配的效率下降。考虑到对象分配几乎是 Java 中最常用的操作，因此 JVM 使用了 TLAB 这样的线程专有区域来避免多线程冲突，提高对象分配的效率。 我们说 TLAB 是线程独享的，但是只是在 “分配” 这个动作上是线程独享的，至于在读取、垃圾回收等动作上都是线程共享的。而且在使用上也没有什么区别 JVM 为了提升对象内存分配的效率，对于所创建的线程都会分配一块独立的空间 TLAB，其大小由 JVM 根据运行的情况计算而得，在 TLAB 上分配对象时不需要加锁，因此 JVM 在给线程的对象分配内存时会尽量的在 TLAB 上分配，在这种情况下 JVM 中分配对象内存的性能和 C 基本是一样高效的，但如果对象过大的话则仍然是直接使用堆空间分配 在 TLAB 分配之后，并不影响对象的移动和回收，也就是说，虽然对象刚开始可能通过 TLAB 分配内存，存放在 Eden 区，但是还是会被垃圾回收或者被移到 Survivor Space、Old Gen 等。 “堆是线程共享的内存区域” 这句话并不完全正确，因为 TLAB 是堆内存的一部分，它在读取上确实是线程共享的，但是在内存分配上，是线程独享的。 TLAB 的空间其实并不大（默认是 eden 区空间的 1%），所以大对象还是可能需要在堆内存中直接分配。那么，对象的内存分配步骤就是先尝试 TLAB 分配，空间不足之后，再判断是否应该直接进入老年代，然后再确定是再 eden 分配还是在老年代分配。 TLAB 对象分配过程
参考链接
尚硅谷-宋红康-详解 Java 虚拟机 JVM 关于对象分配在堆、栈、TLAB 的理解 TLAB（Thread Local Allocation Buffer）</content></entry><entry><title>MySQL InnoDB 事务隔离级别总结</title><url>/2021/01/16/mysql-innodb-tx-isolation/</url><categories><category>SQL</category></categories><tags><tag>MySQL</tag></tags><content type="html"><![CDATA[SQL 事务隔离级别说明 SQL 标准定义了 4 类隔离级别，包括了一些具体规则，用来限定事务内外的哪些改变是可见的，哪些是不可见的。低级别的隔离级一般支持更高的并发处理，并拥有更低的系统开销。
Read Uncommitted（读取未提交内容）
在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少。读取未提交的数据，也被称之为脏读（Dirty Read）。
Read Committed（读取提交内容）
这是大多数数据库系统的默认隔离级别（但不是 MySQL 默认的）。它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。这种隔离级别 也支持所谓的不可重复读（Nonrepeatable Read），因为同一事务的其他实例在该实例处理其间可能会有新的 commit，所以同一 select 可能返回不同结果。
Repeatable Read（可重读）
这是 MySQL 的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。不过理论上，这会导致另一个棘手的问题：幻读 （Phantom Read），简单的说，幻读指当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的 “幻影” 行。
InnoDB 存储引擎通过 MVCC 机制（快照读）和 next-key lock（当前读）解决了该问题。
Serializable（可串行化）
这是最高的隔离级别，它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁。在这个级别，可能导致大量的超时现象和锁竞争。
事务隔离带来的问题 这四种隔离级别采取不同的锁类型来实现，若读取的是同一个数据的话，就容易发生问题。例如：
脏读 (Drity Read)：一个事务读取到另一事务未提交的更新数据。当一个事务正在访问数据，并且对数据进行了修改，而这种修改还没有提交到数据库中（这个数据在有可能会回滚），这时，另外一个事务也访问这个数据，然后使用了这个数据。
不可重复读 (Non-repeatable read)：在一个事务内，前后两次读到的数据是不一样。在 T1 事务两次读取同一数据之间，T2 事务对该数据进行了修改，就会发生 T1 事务中的两次数据读取不一样的结果。相反， 可重复读：在同一事务中多次读取数据时，能够保证所读数据一样，也就是后续读取不能读到另一事务已提交的更新数据。
幻读 (Phantom Read)：指当事务不是独立执行时发生的一种现象，例如：T1 事务对表中的 &ldquo;全部数据行&rdquo; 进行了修改，同时 T2 事务向表中插入了一行 &ldquo;新数据&rdquo;，操作 T1 事务的用户发现表中还存在没有修改的数据行，就好象发生了幻觉一 样。一般解决幻读的方法是增加范围锁 RangeS，锁定检锁范围为只读，这样就避免了幻读。
不可重复读和幻读的异同
两者都表现为两次读取的结果不一致 不可重复读是由于另一个事务对数据的更改所造成的 幻读是由于另一个事务插入或删除引起的 对于不可重复读，只需要锁住满足条件的记录 对于幻读，要锁住满足条件及其相近的记录 不可重复读表达的是：记录（一行或多行）的值在同一次事务中出现两个不同的结果 幻读表达的是：同一事务中查询两次得到两个不同的结果集 不可重复读侧重表达 读 - 读 幻读则是说 读 - 写，用写来证实读的是鬼影 关于幻读 这里给出 MySQL 在 Repeatable Read 隔离界别下幻读的比较形象的场景：
时间 事务 1 事务 2 T1 start transaction;
select * from users where id = 1;
结果为：0 T2 start transaction;
insert into users(id, name) values (1, &lsquo;big cat&rsquo;);
commit; T3 insert into users(id, name) values (1, &lsquo;big cat&rsquo;);
主键冲突，插入失败 T4 select * from users where id = 1;
结果为：0 T5 rollback; 假设 users 表中 id 为主键
T1 的时间点事务 1 检测表中没有 id 为 1 的记录
T2 时间点事务 2 插入 id 为 1 的记录并提交事务
T3 时间点事务1 尝试插入 id 为 1 的数据时提示主键冲突
T4 时间点再去检查表中还是没有 id 为 1 的记录（由于 Repeatable Read 隔离级别，事务 2 的插入提交事务 1 读取不到）
MVCC MVCC 多版本并发控制（Multiversion Concurrency Control）。
每一条记录都有一些隐藏字段，其中 trx_id（事务 id）和 roll_pointer（回滚指针）字段就和 MVCC 密切相关
版本链 每次对数据修改都会使 roll_pointer 指向生成 undo 日志，即形成了数据修改的 版本链，版本链的头节点就是当前数据的最新值。
Read View 对于 RC、RR 隔离级别来说，事务启动后在不同的时机会生成 Read View，用于判断当前记录是否在事务中可见。Read View 包含创建时刻所有活跃的 trx_id 的集合 m_ids。当判断数据是否在当前事务中显示时，需要从头到尾遍历版本链，依次取得数据的历史记录判断，直到取到符合条件的历史版本，如果所有的历史记录都不满足则忽略此条数据。
当前版本数据记录的 trx_id 如果小于 m_ids 中最小的 trx_id，说明当前版本数据是在当前事务 Read View 生成之前生成的，所以对当前事务可见。 当前版本数据记录的 trx_id 如果大于 m_ids 中最大的 trx_id，说明当前版本数据是在当前事务 Read View 生成之后生成的，所以对当前事务不可见。 当前版本数据记录的 trx_id 如果在 m_ids 中，说明当前版本数据在当前事务 Read View 生成之前还未提交，所以对当前事务不可见。 当前版本数据记录的 trx_id 如果不在 m_ids 中，说明当前版本数据是在当前事务 Read View 生成之前已提交，所以对当前事务可见。 RC 隔离级别在每次读取数据前都会生成 Read View，RR 隔离级别在第一次读取数据时生成 Read View，运用以上可见规则就很容易推断出这两种隔离级别的隔离能力了。
另外，undo 日志会在系统判断没有比它更早的 Read View 存在时就会被删除。所以当系统中存在大量长事务的时候，会导致 undo 日志不能被及时清理而占用大量的存储空间。
MySQL 隔离级别 隔离级别 脏读（Dirty Read） 不可重复读（NonRepeatable Read） 幻读（Phantom Read） 未提交读（Read uncommitted） 可能 可能 可能 已提交读（Read committed） 不可能 可能 可能 可重复读（Repeatable read） 不可能 不可能 可能 可串行化（SERIALIZABLE） 不可能 不可能 不可能 MySQL 事务隔离级别设置 InnoDB 默认是可重复读的（REPEATABLE READ）
修改全局默认的事务级别，在 my.inf 文件的 [mysqld] 节里类似如下设置该选项（不推荐）
transaction-isolation = {READ-UNCOMMITTED | READ-COMMITTED | REPEATABLE-READ | SERIALIZABLE} 改变单个会话或者所有新进连接的隔离级别（推荐使用）
SET [SESSION | GLOBAL] TRANSACTION ISOLATION LEVEL {READ UNCOMMITTED | READ COMMITTED | REPEATABLE READ | SERIALIZABLE} 查询全局和会话事务隔离级别方法
#查询全局的事务隔离级别 SELECT @@global.tx_isolation; #查询当前会话的事务级别 SELECT @@session.tx_isolation; ]]></content></entry><entry><title>Docker 安装 Gitea/Gogs 与主机共享 22 端口</title><url>/2021/01/06/docker-gitea-share-port-22-with-host/</url><categories><category>Git</category></categories><tags><tag>Gitea</tag><tag>Gogs</tag><tag>Git</tag></tags><content type="html"><![CDATA[如果主机的 22 端口已被使用，使用 Docker 安装 Gitea 时只能把容器的 22 端口映射到主机的其它端口（如：10022），这是没有任何问题的。但是以 SSH 方式 clone 项目时，URL 长这样 ssh://git@git.example.com:10022:username/project.git
如果我们想要类似以下这样的 URL 时就需要把 Gitea 容器的和主机共享 22 端口 git@git.example.com:username/project.git
下面总结一下使用 Docker 安装 Gitea 共享主机 22 端口的主要步骤，Gogs 应该是同理。
创建 git 用户 # Create git user adduser git # Make sure user has UID and GID 1000 usermod -u 1000 -g 1000 git # Create docker group groupadd docker # Add git user to docker group usermod -aG docker git # Create the gitea data directory mkdir -p /home/git/gitea/data 安装 Gitea docker run -d --name=gitea -p 10022:22 -p 10080:3000 -v /home/git/gitea/data:/data --restart=always gitea/gitea:latest # Create a symlink between the container authorized_keys and the host git user authorized_keys ln -s /home/git/gitea/data/git/.ssh /home/git/ 生成 SSH key sudo -u git ssh-keygen -t rsa -b 4096 -C &#34;Gitea Host Key&#34; echo &#34;no-port-forwarding,no-X11-forwarding,no-agent-forwarding,no-pty $(cat /home/git/.ssh/id_rsa.pub)&#34; &gt;&gt; /home/git/.ssh/authorized_keys chmod 600 /home/git/.ssh/authorized_keys 配置 SSH passthrough 配置 passthrough 连接到 Gitea 容器的 SSH 映射端口 10022
mkdir -p /app/gitea/ cat &gt;/app/gitea/gitea &lt;&lt;&#39;END&#39; #!/bin/sh ssh -p 10022 -o StrictHostKeyChecking=no git@127.0.0.1 \ &#34;SSH_ORIGINAL_COMMAND=\&#34;$SSH_ORIGINAL_COMMAND\&#34; $0 $@&#34; END chmod +x /app/gitea/gitea Caddy 反向代理配置 这里使用 Caddy 反向代理配置域名，Caddyfile 配置信息如下：
git.example.com { encode zstd gzip reverse_proxy localhost:10080 header / Strict-Transport-Security &#34;max-age=31536000;&#34; } 配置完域名之后，输入域名进行安装，现在就可以修改 【SSH 服务域名】 为 git.example.com，【Gitea 基本 URL】 为 https://git.example.com/，也可以后通过 /home/git/gogs/data/gogs/conf/app.ini 配置文件修改相关配置。
注意事项 由于 docker 启动容器的默认 uid 和 gid 是 1000，所以 git 用户的 uid、gid 必须为 1000，如果 git 用户的 uid 和 gid 不是 1000（比如：1002），尝试通过 docker run --user 1002:1002、 docker run -e &quot;PUID=1002&quot; -e &quot;PGID=1002&quot; 等方式启动 docker 容器都不管用。
保证 git 用户下的所有文件都属于 git 用户和 git 组
[git]$ ls -la /home/git/gitea/data total 20 drwxrwxr-x 5 git git 4096 Jan 5 14:14 . drwxrwxr-x 3 git git 4096 Jan 5 13:56 .. drwxr-xr-x 5 git git 4096 Jan 5 14:20 git drwxr-xr-x 10 git git 4096 Jan 5 14:40 gitea drwx------ 2 git git 4096 Jan 5 14:14 ssh 参考文章 Gogs 与主机共享 22 端口 docker-gitea ]]></content></entry><entry><title>使用 Lambda 优雅的处理 Java 异常</title><url>/2020/12/06/replace-java-try-catch-with-lambda/</url><categories><category>Java</category></categories><tags><tag>Java</tag></tags><content type="html"><![CDATA[使用过 Java 的函数接口，就会被简介的语法深深的吸引，苦于代码中大量的 try...catch 繁琐代码，最近借鉴 java.util.Optional 的实现写了个简化的小工具。
以 Long.valueOf() 为例，假如需要把一个字符串转换为long，如果转换失败则设置默认值为 -1，一般会作如下处理：
String param = &#34;10s&#34;; long result; try { result = Long.parseLong(param); } catch (Exception e) { // 捕获异常处理 result = -1L; } 如果使用简化工具：
Long result = Try.of(() -&gt; Long.valueOf(param)).trap(e -&gt; { // 自行异常处理 }).get(-1L); 或者：
Long result = Try.&lt;String, Long&gt;of(Long::valueOf).trap(Throwable::printStackTrace).apply(param).get(-1L); 如果不需要异常处理：
Long result = Try.of(() -&gt; Long.valueOf(param)).get(-1L); // Long result = Try.&lt;String, Long&gt;of(Long::valueOf).apply(param).get(-1L); 如果处理没有返回值的代码，如下：
ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(); Try.&lt;String&gt;of(v -&gt; list.add(10, v)) .trap(e -&gt; System.out.println(e.getMessage())) .accept(&#34;test&#34;); 其它情况的简单使用示例：
public static void main(String[] args) { // 有返回值，无入参 String param = &#34;s&#34;; Long result = Try.of(() -&gt; Long.valueOf(param)).get(0L); System.out.println(&#34;Long.valueOf 1: &#34; + result); result = Try.of(() -&gt; Long.valueOf(param)).get(); System.out.println(&#34;Long.valueOf 2: &#34; + result); // 有返回值，有入参 result = Try.&lt;String, Long&gt;of(s -&gt; Long.valueOf(s)) .apply(param) .trap((e) -&gt; System.out.println(&#34;Long.valueOf exception: &#34; + e.getMessage())) .andFinally(() -&gt; System.out.println(&#34;Long.valueOf finally run code.&#34;)) .finallyTrap((e) -&gt; System.out.println(&#34;Long.valueOf finally exception: &#34; + e.getMessage())) .get(); System.out.println(&#34;Long.valueOf 3: &#34; + result); ArrayList&lt;String&gt; list = null; // 无返回值，无入参 Try.of(() -&gt; Thread.sleep(-1L)) .andFinally(() -&gt; list.clear()) // .andFinally(list::clear) //https://stackoverflow.com/questions/37413106/java-lang-nullpointerexception-is-thrown-using-a-method-reference-but-not-a-lamb .run(); // 无返回值，有入参 Try.&lt;String&gt;of(v -&gt; list.add(0, v)).accept(&#34;test&#34;); } 小工具的实现链接 ]]></content></entry><entry><title>HTTPS 笔记</title><url>/2020/03/11/https-note/</url><categories><category>Java</category></categories><tags><tag>Java</tag></tags><content type="html">随着互联网的迅速发展，网络安全问题日益凸显，现在 Chrome 浏览器已经开始阻止非 https 网站的访问了。对于 https 的流程一直不是十分清晰，借着还没有完全复工有时间，大概画了个图总结一下。
想要了解 https 流程，CA 的相关知识，加密方式（对称加密、非对称加密），以及哈希计算（例如：MD5、sha256）等技术必须得掌握，这里先不做介绍，后续有时间再进行归纳总结。
https 是在 http 的基础上加入了 SSL 协议，SSL 依靠证书来验证服务器的身份，并为浏览器和服务器之间的通信进行加密。
HTTPS 的验证流程 客户端发起 https 请求 服务端返回数字证书文件（X.509 格式） 客户端验证数字证书，并且提取服务端公钥 如果客户端验证数字证书通过，则随机生成一个对称加密的 key，并使用服务器公钥对 key 加密 客户端发送加密后的 key 到服务端 服务端使用私钥解密拿到 key 客户端与服务端使用该 key 对称加解密通讯信息 客户端验证数字证书 浏览器安装后会自带一些权威 CA 公钥 使用相匹配的 CA 公钥对数字证书中的数字签名解密，如果能够解密则得到数字证书的摘要，由此证明数字证书是可信的 根据数字证书中的散列算法对网站信息进行哈希运算，将得到的结果与上一步得到的摘要对比，如果两者一致，就证明证书未被修改过 数字证书的签发过程</content></entry><entry><title>Maven 配置文件 settings.xml 详解</title><url>/2020/03/02/maven-setting-config/</url><categories><category>Java</category></categories><tags><tag>Java</tag></tags><content type="html"><![CDATA[settings.xml 是 maven 的配置文件，用户配置文件存放于 ${user.home}/.m2/ 目录下，系统全局配置文件放置于 ${maven.home}/conf/ 目录下，pom.xml 是 maven 的项目的配置文件。
配置文件的优先级从高到低为：pom.xml、用户配置 settings.xml、全局系统 settings.xml。如果这些文件同时存在，在应用配置时，会合并它们的内容，如果有重复的配置，优先级高的配置会覆盖优先级低的。
现抽空把 settings.xml 相关配置属性总结一下。
&lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&gt; &lt;settings xmlns=&#34;http://maven.apache.org/SETTINGS/1.0.0&#34; xmlns:xsi=&#34;http://www.w3.org/2001/XMLSchema-instance&#34; xsi:schemaLocation=&#34;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&#34;&gt; &lt;!-- 构建系统本地仓库的路径。其默认值：~/.m2/repository --&gt; &lt;!-- &lt;localRepository&gt;${user.home}/.m2/repository&lt;/localRepository&gt; --&gt; &lt;!-- 是否需要和用户交互以获得输入。默认为 true --&gt; &lt;!-- &lt;interactiveMode&gt;true&lt;/interactiveMode&gt; --&gt; &lt;!-- 是否需要使用 ~/.m2/plugin-registry.xml 文件来管理插件版本。默认为 false --&gt; &lt;!-- &lt;usePluginRegistry&gt;false&lt;/usePluginRegistry&gt; --&gt; &lt;!-- 是否需要在离线模式下运行，默认为 false。当由于网络设置原因或者安全因素，构建服务器不能连接远程仓库的时候，该配置就十分有用 --&gt; &lt;!-- &lt;offline&gt;false&lt;/offline&gt; --&gt; &lt;!-- 当插件的 groupId 没有显式提供时，供搜寻插件 groupId 的列表。使用某个插件，如果没有指定 groupId 的时候，maven 就会使用该列表。 默认情况下该列表包含了 org.apache.maven.plugins 和 org.codehaus.mojo --&gt; &lt;!-- &lt;pluginGroups&gt; --&gt; &lt;!-- plugin 的 groupId --&gt; &lt;!-- &lt;pluginGroup&gt;org.codehaus.mojo&lt;/pluginGroup&gt; --&gt; &lt;!-- &lt;/pluginGroups&gt; --&gt; &lt;!-- 配置服务端的一些设置。如安全证书之类的信息应该配置在 settings.xml 文件中，避免配置在 pom.xml 中 --&gt; &lt;!-- &lt;servers&gt; --&gt; &lt;!-- &lt;server&gt; --&gt; &lt;!-- 这是 server 的 id（注意不是用户登陆的 id），该 id 与 distributionManagement 中 repository 元素的 id 相匹配 --&gt; &lt;!-- &lt;id&gt;server001&lt;/id&gt; --&gt; &lt;!-- 鉴权用户名 --&gt; &lt;!-- &lt;username&gt;my_login&lt;/username&gt; --&gt; &lt;!-- 鉴权密码 --&gt; &lt;!-- &lt;password&gt;my_password&lt;/password&gt; --&gt; &lt;!-- 鉴权时使用的私钥位置。默认是 ${user.home}/.ssh/id_dsa --&gt; &lt;!-- &lt;privateKey&gt;${usr.home}/.ssh/id_dsa&lt;/privateKey&gt; --&gt; &lt;!-- 鉴权时使用的私钥密码 --&gt; &lt;!-- &lt;passphrase&gt;some_passphrase&lt;/passphrase&gt; --&gt; &lt;!-- 文件被创建时的权限。如果在部署的时候会创建一个仓库文件或者目录，这时候就可以使用该权限。其对应了 unix 文件系统的权限，如：664、775 --&gt; &lt;!-- &lt;filePermissions&gt;664&lt;/filePermissions&gt; --&gt; &lt;!-- 目录被创建时的权限 --&gt; &lt;!-- &lt;directoryPermissions&gt;775&lt;/directoryPermissions&gt; --&gt; &lt;!-- &lt;/server&gt; --&gt; &lt;!-- &lt;/servers&gt; --&gt; &lt;!-- 下载镜像列表 --&gt; &lt;mirrors&gt; &lt;!-- 设置多个镜像只会识别第一个镜像下载 jar 包--&gt; &lt;mirror&gt; &lt;!-- 该镜像的唯一标识符。id 用来区分不同的 mirror 元素 --&gt; &lt;id&gt;aliyunmaven&lt;/id&gt; &lt;!-- 被镜像的服务器的 id。如果我们要设置了一个 maven 中央仓库（http://repo.maven.apache.org/maven2/）的镜像，就需要将该元素设置成 central。 可以使用 * 表示任意远程库。例如：external:* 表示任何不在 localhost 和文件系统中的远程库，r1,r2 表示 r1 库或者 r2 库，*,!r1 表示除了 r1 库之外的任何远程库 --&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;!-- 镜像名称 --&gt; &lt;name&gt;阿里云公共仓库&lt;/name&gt; &lt;!-- 镜像的 URL --&gt; &lt;url&gt;https://maven.aliyun.com/repository/public&lt;/url&gt; &lt;/mirror&gt; &lt;/mirrors&gt; &lt;!-- 用来配置不同的代理 --&gt; &lt;proxies&gt; &lt;proxy&gt; &lt;!-- 代理的唯一定义符，用来区分不同的代理元素 --&gt; &lt;id&gt;myproxy&lt;/id&gt; &lt;!-- 是否激活。当我们声明了一组代理，而某个时候只需要激活一个代理的时候 --&gt; &lt;active&gt;false&lt;/active&gt; &lt;!-- 代理的协议 --&gt; &lt;protocol&gt;http&lt;/protocol&gt; &lt;!-- 代理的主机名 --&gt; &lt;host&gt;proxy.somewhere.com&lt;/host&gt; &lt;!-- 代理的端口 --&gt; &lt;port&gt;8080&lt;/port&gt; &lt;!-- 代理的用户名，用户名和密码表示代理服务器认证的登录名和密码 --&gt; &lt;username&gt;proxyuser&lt;/username&gt; &lt;!-- 代理的密码 --&gt; &lt;password&gt;somepassword&lt;/password&gt; &lt;!-- 不该被代理的主机名列表。该列表的分隔符由代理服务器指定；例子中使用了竖线分隔符，逗号分隔也很常见 --&gt; &lt;nonProxyHosts&gt;*.google.com|ibiblio.org&lt;/nonProxyHosts&gt; &lt;/proxy&gt; &lt;/proxies&gt; &lt;!-- 根据环境参数来调整构建配置的列表。对应 pom.xml 中 profile 元素（只包含 id、activation、repositories、pluginRepositories 和 properties 元素） 如果一个 settings.xml 中的 profile 被激活，它的值会覆盖任何定义在 pom.xml 中带有相同 id 的 profile --&gt; &lt;profiles&gt; &lt;profile&gt; &lt;!-- profile 的唯一标识 --&gt; &lt;id&gt;test&lt;/id&gt; &lt;!-- 自动触发 profile 的条件逻辑。也可通过 activeProfile 元素以及使用 -P 标记激活（如：mvn clean install -P test） 在 maven 工程的 pom.xml 所在目录下执行 mvn help:active-profiles 命令可以查看生效的 profile --&gt; &lt;activation&gt; &lt;!-- 默认是否激活 --&gt; &lt;activeByDefault&gt;false&lt;/activeByDefault&gt; &lt;!-- 当匹配的 jdk 被检测到，profile 被激活。例如：1.4 激活 JDK1.4、1.4.0_2，而 !1.4 激活所有版本不是以 1.4 开头的 JDK --&gt; &lt;jdk&gt;1.8&lt;/jdk&gt; &lt;!-- 当匹配的操作系统属性被检测到，profile 被激活。os 元素可以定义一些操作系统相关的属性 --&gt; &lt;os&gt; &lt;!-- 激活 profile的 操作系统的名字 --&gt; &lt;name&gt;Windows XP&lt;/name&gt; &lt;!--激活 profile 的操作系统所属家族。如：windows --&gt; &lt;family&gt;Windows&lt;/family&gt; &lt;!--激活 profile 的操作系统体系结构 --&gt; &lt;arch&gt;x86&lt;/arch&gt; &lt;!--激活p rofile 的操作系统版本 --&gt; &lt;version&gt;5.1.2600&lt;/version&gt; &lt;/os&gt; &lt;!-- 如果 maven 检测到某一个属性（其值可以在 pom.xml 中通过 ${name} 引用），其拥有对应的 name=值，Profile 就会被激活。如果值字段是空的， 那么存在属性名称字段就会激活 profile，否则按区分大小写方式匹配属性值字段 --&gt; &lt;property&gt; &lt;!-- 激活 profile 的属性的名称 --&gt; &lt;name&gt;mavenVersion&lt;/name&gt; &lt;!-- 激活 profile 的属性的值 --&gt; &lt;value&gt;2.0.3&lt;/value&gt; &lt;/property&gt; &lt;!-- 通过检测该文件的存在或不存在来激活 profile--&gt; &lt;file&gt; &lt;!-- 如果指定的文件存在，则激活 profile --&gt; &lt;exists&gt;${basedir}/file2.properties&lt;/exists&gt; &lt;!-- 如果指定的文件不存在，则激活 profile --&gt; &lt;missing&gt;${basedir}/file1.properties&lt;/missing&gt; &lt;/file&gt; &lt;/activation&gt; &lt;!-- 对应 profile 的扩展属性列表。maven 属性和 ant 中的属性一样，可以用来存放一些值。这些值可以在 pom.xml 中的任何地方使用标记 ${X} 来使用，这里 X 是指属性的名称。 属性有五种不同的形式，并且都能在 settings.xml 文件中访问： 1. env.X：在一个变量前加上 &#34;env.&#34; 的前缀，会返回一个 shell 环境变量。例如：&#34;env.PATH&#34; 指代了 $path 环境变量（在 Windows 上是 %PATH%） 2. project.x：指代了 pom.xml 中对应的元素值。例如：&lt;project&gt;&lt;version&gt;1.0&lt;/version&gt;&lt;/project&gt; 通过 ${project.version} 获得 version 的值 3. settings.x：指代了 settings.xml 中对应元素的值。例如：&lt;settings&gt;&lt;offline&gt;false&lt;/offline&gt;&lt;/settings&gt; 通过 ${settings.offline} 获得 offline 的值 4. Java System Properties：所有可通过 java.lang.System.getProperties() 访问的属性都能在 pom.xml 中使用该形式访问，例如：${java.home} 5. x：在 &lt;properties/&gt; 元素中，或者外部文件中设置，以 ${someVar} 的形式使用 --&gt; &lt;properties&gt; &lt;!-- 如果该 profile 被激活，则可以在 pom.xml 中使用 ${user.install} --&gt; &lt;user.install&gt;${user.home}/our-project&lt;/user.install&gt; &lt;/properties&gt; &lt;!-- 远程仓库列表。它是 maven 用来填充构建系统本地仓库所使用的一组远程仓库 --&gt; &lt;repositories&gt; &lt;!--包含需要连接到远程仓库的信息 --&gt; &lt;repository&gt; &lt;!-- 远程仓库唯一标识 --&gt; &lt;id&gt;codehausSnapshots&lt;/id&gt; &lt;!-- 远程仓库名称 --&gt; &lt;name&gt;Codehaus Snapshots&lt;/name&gt; &lt;!-- 如何处理远程仓库里 releases 的下载 --&gt; &lt;releases&gt; &lt;!-- 是否开启 --&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;!-- 该元素指定更新发生的频率。maven 会比较本地 pom.xml 和远程 pom.xml 的时间戳。 这里的选项是：always（一直），daily（默认，每日），interval：X（这里 X 是以分钟为单位的时间间隔），或者 never（从不）。 --&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;!-- 当 maven 验证构件校验文件失败时该怎么做：ignore（忽略），fail（失败），或者 warn（警告）--&gt; &lt;checksumPolicy&gt;warn&lt;/checksumPolicy&gt; &lt;/releases&gt; &lt;!-- 如何处理远程仓库里快照版本的下载。有了 releases 和 snapshots 这两组配置，pom.xml 就可以在每个单独的仓库中，为每种类型的构件采取不同的策略。 例如：可能有人会决定只为开发目的开启对快照版本下载的支持 --&gt; &lt;snapshots&gt; &lt;enabled/&gt; &lt;updatePolicy/&gt; &lt;checksumPolicy/&gt; &lt;/snapshots&gt; &lt;!-- 远程仓库 URL --&gt; &lt;url&gt;http://snapshots.maven.codehaus.org/maven2&lt;/url&gt; &lt;!-- 用于定位和排序构件的仓库布局类型。可以是 default（默认）或者 legacy（遗留）--&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;!-- 插件的远程仓库列表。和 repositories 类似，repositories 管理 jar 包依赖的仓库，pluginRepositories 则是管理插件的仓库 --&gt; &lt;pluginRepositories&gt; &lt;!-- 每个 pluginRepository 元素指定一个 maven 可以用来寻找新插件的远程地址 --&gt; &lt;pluginRepository&gt; &lt;id/&gt; &lt;name/&gt; &lt;releases&gt; &lt;enabled/&gt; &lt;updatePolicy/&gt; &lt;checksumPolicy/&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled/&gt; &lt;updatePolicy/&gt; &lt;checksumPolicy/&gt; &lt;/snapshots&gt; &lt;url/&gt; &lt;layout/&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;/profile&gt; &lt;/profiles&gt; &lt;!-- 手动激活 profiles 的列表 --&gt; &lt;!-- &lt;activeProfiles&gt; --&gt; &lt;!-- 要激活的 profile id。例如：env-test，则在 pom.xml 或 settings.xml 中对应 id 的 profile 会被激活。如果运行过程中找不到对应的 profile 则忽略配置 --&gt; &lt;!-- &lt;activeProfile&gt;env-test&lt;/activeProfile&gt; --&gt; &lt;!-- &lt;/activeProfiles&gt; --&gt; &lt;/settings&gt; 参考资料
maven 官方文档之 settings ]]></content></entry><entry><title>[转]理解 Java 动态代理</title><url>/2020/01/02/java-dynamic-proxy/</url><categories><category>Java</category></categories><tags><tag>Java</tag></tags><content type="html"><![CDATA[ 原文地址 动态代理是 Java 语言中非常经典的一种设计模式，也是所有设计模式中最难理解的一种。本文将通过一个简单的例子模拟 JDK 动态代理实现，让你彻底明白动态代理设计模式的本质。
什么是代理 从字面意思来看，代理比较好理解，无非就是代为处理的意思。举个例子，你在上大学的时候，总是喜欢逃课。因此，你拜托你的同学帮你答到，而自己却窝在宿舍玩游戏&hellip; 你的这个同学恰好就充当了代理的作用，代替你去上课。
是的，你没有看错，代理就是这么简单！
理解了代理的意思，你脑海中恐怕还有两个巨大的疑问：
怎么实现代理模式 代理模式有什么实际用途 要理解这两个问题，看一个简单的例子：
public interface Flyable { void fly(); } public class Bird implements Flyable { @Override public void fly() { System.out.println(&#34;Bird is flying...&#34;); try { Thread.sleep(new Random().nextInt(1000)); } catch (InterruptedException e) { e.printStackTrace(); } } } 很简单的一个例子，用一个随机睡眠时间模拟小鸟在空中的飞行时间。接下来问题来了，如果我要知道小鸟在天空中飞行了多久，怎么办？
有人说，很简单，在 Bird-&gt;fly() 方法的开头记录起始时间，在方法结束记录完成时间，两个时间相减就得到了飞行时间。
@Override public void fly() { long start = System.currentTimeMillis(); System.out.println(&#34;Bird is flying...&#34;); try { Thread.sleep(new Random().nextInt(1000)); } catch (InterruptedException e) { e.printStackTrace(); } long end = System.currentTimeMillis(); System.out.println(&#34;Fly time = &#34; + (end - start)); } 的确，这个方法没有任何问题，接下来加大问题的难度。如果 Bird 这个类来自于某个 SDK（或者说 Jar 包）提供，你无法改动源码，怎么办？
一定会有人说，我可以在调用的地方这样写：
public static void main(String[] args) { Bird bird = new Bird(); long start = System.currentTimeMillis(); bird.fly(); long end = System.currentTimeMillis(); System.out.println(&#34;Fly time = &#34; + (end - start)); } 这个方案看起来似乎没有问题，但其实你忽略了准备这些方法所需要的时间，执行一个方法，需要开辟栈内存、压栈、出栈等操作，这部分时间也是不可以忽略的。因此，这个解决方案不可行。那么，还有什么方法可以做到呢？
使用继承，继承是最直观的解决方案，相信你已经想到了，至少我最开始想到的解决方案就是继承。 为此，我们重新创建一个类 Bird2，在 Bird2 中我们只做一件事情，就是调用父类的 fly 方法，在前后记录时间，并打印时间差：
public class Bird2 extends Bird { @Override public void fly() { long start = System.currentTimeMillis(); super.fly(); long end = System.currentTimeMillis(); System.out.println(&#34;Fly time = &#34; + (end - start)); } } 这是一种解决方案，还有一种解决方案叫做：聚合，其实也是比较容易想到的。 我们再次创建新类 Bird3，在 Bird3 的构造方法中传入 Bird 实例。同时，让 Bird3 也实现 Flyable 接口，并在 fly 方法中调用传入的 Bird 实例的 fly 方法：
public class Bird3 implements Flyable { private Bird bird; public Bird3(Bird bird) { this.bird = bird; } @Override public void fly() { long start = System.currentTimeMillis(); bird.fly(); long end = System.currentTimeMillis(); System.out.println(&#34;Fly time = &#34; + (end - start)); } } 为了记录 Bird-&gt;fly () 方法的执行时间，我们在前后添加了记录时间的代码。同样地，通过这种方法我们也可以获得小鸟的飞行时间。那么，这两种方法孰优孰劣呢？咋一看，不好评判！
继续深入思考，用问题推导来解答这个问题：
问题一：如果我还需要在 fly 方法前后打印日志，记录飞行开始和飞行结束，怎么办？ 有人说，很简单！继承 Bird2 并在在前后添加打印语句即可。那么，问题来了，请看问题二。
问题二：如果我需要调换执行顺序，先打印日志，再获取飞行时间，怎么办？ 有人说，再新建一个类 Bird4 继承 Bird，打印日志。再新建一个类 Bird5 继承 Bird4，获取方法执行时间。
问题显而易见：使用继承将导致类无限制扩展，同时灵活性也无法获得保障。那么，使用聚合是否可以避免这个问题呢？ 答案是：可以！但我们的类需要稍微改造一下。修改 Bird3 类，将聚合对象 Bird 类型修改为 Flyable
public class Bird3 implements Flyable { private Flyable flyable; public Bird3(Flyable flyable) { this.flyable = flyable; } @Override public void fly() { long start = System.currentTimeMillis(); flyable.fly(); long end = System.currentTimeMillis(); System.out.println(&#34;Fly time = &#34; + (end - start)); } } 为了让你看的更清楚，我将 Bird3 更名为 BirdTimeProxy，即用于获取方法执行时间的代理的意思。同时我们新建 BirdLogProxy 代理类用于打印日志：
public class BirdLogProxy implements Flyable { private Flyable flyable; public BirdLogProxy(Flyable flyable) { this.flyable = flyable; } @Override public void fly() { System.out.println(&#34;Bird fly start...&#34;); flyable.fly(); System.out.println(&#34;Bird fly end...&#34;); } } 接下来神奇的事情发生了，如果我们需要先记录日志，再获取飞行时间，可以在调用的地方这么做：
public static void main(String[] args) { Bird bird = new Bird(); BirdLogProxy p1 = new BirdLogProxy(bird); BirdTimeProxy p2 = new BirdTimeProxy(p1); p2.fly(); } 反过来，可以这么做：
public static void main(String[] args) { Bird bird = new Bird(); BirdTimeProxy p2 = new BirdTimeProxy(bird); BirdLogProxy p1 = new BirdLogProxy(p2); p1.fly(); } 看到这里，有同学可能会有疑问了。虽然现象看起来，聚合可以灵活调换执行顺序。可是，为什么聚合可以做到，而继承不行呢。我们用一张图来解释一下：
静态代理 接下来，观察上面的类 BirdTimeProxy，在它的 fly 方法中我们直接调用了 flyable-&gt;fly () 方法。换而言之，BirdTimeProxy 其实代理了传入的 Flyable 对象，这就是典型的静态代理实现。
从表面上看，静态代理已经完美解决了我们的问题。可是，试想一下，如果我们需要计算 SDK 中 100 个方法的运行时间，同样的代码至少需要重复 100 次，并且创建至少 100 个代理类。往小了说，如果 Bird 类有多个方法，我们需要知道其他方法的运行时间，同样的代码也至少需要重复多次。因此，静态代理至少有以下两个局限性问题：
如果同时代理多个类，依然会导致类无限制扩展 如果类中有多个方法，同样的逻辑需要反复实现 那么，我们是否可以使用同一个代理类来代理任意对象呢？我们以获取方法运行时间为例，是否可以使用同一个类（例如：TimeProxy）来计算任意对象的任一方法的执行时间呢？甚至再大胆一点，代理的逻辑也可以自己指定。比如，获取方法的执行时间，打印日志，这类逻辑都可以自己指定。这就是本文重点探讨的问题，也是最难理解的部分：动态代理。
动态代理 继续回到上面这个问题：是否可以使用同一个类（例如：TimeProxy）来计算任意对象的任一方法的执行时间呢。
这个部分需要一定的抽象思维，我想，你脑海中的第一个解决方案应该是使用反射。反射是用于获取已创建实例的方法或者属性，并对其进行调用或者赋值。很明显，在这里，反射解决不了问题。但是，再大胆一点，如果我们可以动态生成 TimeProxy 这个类，并且动态编译。然后，再通过反射创建对象并加载到内存中，不就实现了对任意对象进行代理了吗？为了防止你依然一头雾水，我们用一张图来描述接下来要做什么：
动态生成 Java 源文件并且排版是一个非常繁琐的工作，为了简化操作，我们使用 JavaPoet 这个第三方库帮我们生成 TimeProxy 的源码。希望 JavaPoet 不要成为你的负担，不理解 JavaPoet 没有关系，你只要把它当成一个 Java 源码生成工具使用即可。
**PS：**你记住，任何工具库的使用都不会太难，它是为了简化某些操作而出现的，目标是简化而不是繁琐。因此，只要你适应它的规则就轻车熟路了。
第一步：生成 TimeProxy 源码 public class Proxy { public static Object newProxyInstance() throws IOException { TypeSpec.Builder typeSpecBuilder = TypeSpec.classBuilder(&#34;TimeProxy&#34;).addSuperinterface(Flyable.class); FieldSpec fieldSpec = FieldSpec.builder(Flyable.class, &#34;flyable&#34;, Modifier.PRIVATE).build(); typeSpecBuilder.addField(fieldSpec); MethodSpec constructorMethodSpec = MethodSpec.constructorBuilder() .addModifiers(Modifier.PUBLIC) .addParameter(Flyable.class, &#34;flyable&#34;) .addStatement(&#34;this.flyable = flyable&#34;) .build(); typeSpecBuilder.addMethod(constructorMethodSpec); Method[] methods = Flyable.class.getDeclaredMethods(); for (Method method : methods) { MethodSpec methodSpec = MethodSpec.methodBuilder(method.getName()) .addModifiers(Modifier.PUBLIC) .addAnnotation(Override.class) .returns(method.getReturnType()) .addStatement(&#34;long start = $T.currentTimeMillis()&#34;, System.class) .addCode(&#34;\n&#34;) .addStatement(&#34;this.flyable.&#34; + method.getName() + &#34;()&#34;) .addCode(&#34;\n&#34;) .addStatement(&#34;long end = $T.currentTimeMillis()&#34;, System.class) .addStatement(&#34;$T.out.println(\&#34;Fly Time =\&#34; + (end - start))&#34;, System.class) .build(); typeSpecBuilder.addMethod(methodSpec); } JavaFile javaFile = JavaFile.builder(&#34;com.youngfeng.proxy&#34;, typeSpecBuilder.build()).build(); // 为了看的更清楚，我将源码文件生成到桌面 javaFile.writeTo(new File(&#34;/Users/ouyangfeng/Desktop/&#34;)); return null; } } 在 main 方法中调用 Proxy.newProxyInstance ()，你将看到桌面已经生成了 TimeProxy.java 文件，生成的内容如下：
package com.youngfeng.proxy; import java.lang.Override; import java.lang.System; class TimeProxy implements Flyable { private Flyable flyable; public TimeProxy(Flyable flyable) { this.flyable = flyable; } @Override public void fly() { long start = System.currentTimeMillis(); this.flyable.fly(); long end = System.currentTimeMillis(); System.out.println(&#34;Fly Time =&#34; + (end - start)); } } 第二步：编译 TimeProxy 源码 编译 TimeProxy 源码我们直接使用 JDK 提供的编译工具即可，为了使你看起来更清晰，我使用一个新的辅助类来完成编译操作：
public class JavaCompiler { public static void compile(File javaFile) throws IOException { javax.tools.JavaCompiler javaCompiler = ToolProvider.getSystemJavaCompiler(); StandardJavaFileManager fileManager = javaCompiler.getStandardFileManager(null, null, null); Iterable iterable = fileManager.getJavaFileObjects(javaFile); javax.tools.JavaCompiler.CompilationTask task = javaCompiler.getTask(null, fileManager, null, null, null, iterable); task.call(); fileManager.close(); } } 在 Proxy-&gt;newProxyInstance () 方法中调用该方法，编译顺利完成：
// 为了看的更清楚，我将源码文件生成到桌面 String sourcePath = &#34;/Users/ouyangfeng/Desktop/&#34;; javaFile.writeTo(new File(sourcePath)); // 编译 JavaCompiler.compile(new File(sourcePath + &#34;/com/youngfeng/proxy/TimeProxy.java&#34;)); 第三步：加载到内存中并创建对象 URL[] urls = new URL[] {new URL(&#34;file:/&#34; + sourcePath)}; URLClassLoader classLoader = new URLClassLoader(urls); Class clazz = classLoader.loadClass(&#34;com.youngfeng.proxy.TimeProxy&#34;); Constructor constructor = clazz.getConstructor(Flyable.class); Flyable flyable = (Flyable) constructor.newInstance(new Bird()); flyable.fly(); 通过以上三个步骤，我们至少解决了下面两个问题：
不再需要手动创建 TimeProxy 可以代理任意实现了 Flyable 接口的类对象，并获取接口方法的执行时间 可是，说好的任意对象呢？
第四步：增加 InvocationHandler 接口 查看 Proxy-&gt;newProxyInstance () 的源码，代理类继承的接口我们是写死的，为了增加灵活性，我们将接口类型作为参数传入：
接口的灵活性问题解决了，TimeProxy 的局限性依然存在，它只能用于获取方法的执行时间，而如果要在方法执行前后打印日志则需要重新创建一个代理类，显然这是不妥的！
为了增加控制的灵活性，我们考虑针将代理的处理逻辑也抽离出来（这里的处理就是打印方法的执行时间）。新增 InvocationHandler 接口，用于处理自定义逻辑：
public interface InvocationHandler { void invoke(Object proxy, Method method, Object[] args); } 想象一下，如果客户程序员需要对代理类进行自定义的处理，只要实现该接口，并在 invoke 方法中进行相应的处理即可。这里我们在接口中设置了三个参数（其实也是为了和 JDK 源码保持一致）：
**proxy：**这个参数指定动态生成的代理类，这里是 TimeProxy **method：**这个参数表示传入接口中的所有 Method 对象 **args：**这个参数对应当前 method 方法中的参数 引入了 InvocationHandler 接口之后，我们的调用顺序应该变成了这样：
MyInvocationHandler handler = new MyInvocationHandler(); Flyable proxy = Proxy.newProxyInstance(Flyable.class, handler); proxy.fly(); 方法执行流：proxy.fly() =&gt; handler.invoke()
为此，我们需要在 Proxy.newProxyInstance () 方法中做如下改动：
在 newProxyInstance 方法中传入 InvocationHandler 在生成的代理类中增加成员变量 handler 在生成的代理类方法中，调用 invoke 方法 public static Object newProxyInstance(Class inf, InvocationHandler handler) throws Exception { TypeSpec.Builder typeSpecBuilder = TypeSpec.classBuilder(&#34;TimeProxy&#34;) .addModifiers(Modifier.PUBLIC) .addSuperinterface(inf); FieldSpec fieldSpec = FieldSpec.builder(InvocationHandler.class, &#34;handler&#34;, Modifier.PRIVATE).build(); typeSpecBuilder.addField(fieldSpec); MethodSpec constructorMethodSpec = MethodSpec.constructorBuilder() .addModifiers(Modifier.PUBLIC) .addParameter(InvocationHandler.class, &#34;handler&#34;) .addStatement(&#34;this.handler = handler&#34;) .build(); typeSpecBuilder.addMethod(constructorMethodSpec); Method[] methods = inf.getDeclaredMethods(); for (Method method : methods) { MethodSpec methodSpec = MethodSpec.methodBuilder(method.getName()) .addModifiers(Modifier.PUBLIC) .addAnnotation(Override.class) .returns(method.getReturnType()) .addCode(&#34;try {\n&#34;) .addStatement(&#34;\t$T method = &#34; + inf.getName() + &#34;.class.getMethod(\&#34;&#34; + method.getName() + &#34;\&#34;)&#34;, Method.class) // 为了简单起见，这里参数直接写死为空 .addStatement(&#34;\tthis.handler.invoke(this, method, null)&#34;) .addCode(&#34;} catch(Exception e) {\n&#34;) .addCode(&#34;\te.printStackTrace();\n&#34;) .addCode(&#34;}\n&#34;) .build(); typeSpecBuilder.addMethod(methodSpec); } JavaFile javaFile = JavaFile.builder(&#34;com.youngfeng.proxy&#34;, typeSpecBuilder.build()).build(); // 为了看的更清楚，我将源码文件生成到桌面 String sourcePath = &#34;/Users/ouyangfeng/Desktop/&#34;; javaFile.writeTo(new File(sourcePath)); // 编译 JavaCompiler.compile(new File(sourcePath + &#34;/com/youngfeng/proxy/TimeProxy.java&#34;)); // 使用反射load到内存 URL[] urls = new URL[] {new URL(&#34;file:&#34; + sourcePath)}; URLClassLoader classLoader = new URLClassLoader(urls); Class clazz = classLoader.loadClass(&#34;com.youngfeng.proxy.TimeProxy&#34;); Constructor constructor = clazz.getConstructor(InvocationHandler.class); Object obj = constructor.newInstance(handler); return obj; } 上面的代码你可能看起来比较吃力，我们直接调用该方法，查看最后生成的源码。在 main 方法中测试 newProxyInstance 查看生成的 TimeProxy 源码：
测试代码
Proxy.newProxyInstance(Flyable.class, new MyInvocationHandler(new Bird())); 生成的 TimeProxy.java 源码
package com.youngfeng.proxy; import java.lang.Override; import java.lang.reflect.Method; public class TimeProxy implements Flyable { private InvocationHandler handler; public TimeProxy(InvocationHandler handler) { this.handler = handler; } @Override public void fly() { try { Method method = com.youngfeng.proxy.Flyable.class.getMethod(&#34;fly&#34;); this.handler.invoke(this, method, null); } catch(Exception e) { e.printStackTrace(); } } } MyInvocationHandler.java
public class MyInvocationHandler implements InvocationHandler { private Bird bird; public MyInvocationHandler(Bird bird) { this.bird = bird; } @Override public void invoke(Object proxy, Method method, Object[] args) { long start = System.currentTimeMillis(); try { method.invoke(bird, new Object[] {}); } catch (IllegalAccessException e) { e.printStackTrace(); } catch (InvocationTargetException e) { e.printStackTrace(); } long end = System.currentTimeMillis(); System.out.println(&#34;Fly time = &#34; + (end - start)); } } 至此，整个方法栈的调用栈变成了这样：
看到这里，估计很多同学已经晕了，在静态代理部分，我们在代理类中传入了被代理对象。可是，使用 newProxyInstance 生成动态代理对象的时候，我们居然不再需要传入被代理对象了。我们传入了的实际对象是 InvocationHandler 实现类的实例，这看起来有点像生成了 InvocationHandler 的代理对象，在动态生成的代理类的任意方法中都会间接调用 InvocationHandler-&gt;invoke (proxy, method, args) 方法。
其实的确是这样。TimeProxy 真正代理的对象就是 InvocationHandler，不过这里设计的巧妙之处在于，InvocationHandler 是一个接口，真正的实现由用户指定。另外，在每一个方法执行的时候，invoke 方法都会被调用 ，这个时候如果你需要对某个方法进行自定义逻辑处理，可以根据 method 的特征信息进行判断分别处理。
如何使用 上面这段解释是告诉你在执行 Proxy-&gt;newProxyInstance 方法的时候真正发生的事情，而在实际使用过程中你完全可以忘掉上面的解释。按照设计者的初衷，我们做如下简单归纳：
Proxy-&gt;newProxyInstance (infs, handler) 用于生成代理对象 InvocationHandler这个接口主要用于自定义代理逻辑处理 为了完成对被代理对象的方法拦截，我们需要在 InvocationHandler 对象中传入被代理对象实例。 查看上面的代码，你可以看到我将 Bird 实例已经传入到了 MyInvocationHandler 中，原因就是第三点。
这样设计有什么好处呢？有人说，我们大费周章，饶了一大圈，最终变成了这个样子，到底图什么呢？
想象一下，到此为止，如果我们还需要对其它任意对象进行代理，是否还需要改动 newProxyInstance 方法的源码，答案是：完全不需要！
只要你在 newProxyInstance 方法中指定代理需要实现的接口，指定用于自定义处理的 InvocationHandler 对象，整个代理的逻辑处理都在你自定义的 InvocationHandler 实现类中进行处理。至此，而我们终于可以从不断地写代理类用于实现自定义逻辑的重复工作中解放出来了，从此需要做什么，交给 InvocationHandler。
事实上，我们之前给自己定下的目标 “使用同一个类来计算任意对象的任一方法的执行时间” 已经实现了。严格来说，是我们超额完成了任务，TimeProxy 不仅可以计算方法执行的时间，也可以打印方法执行日志，这完全取决于你的 InvocationHandler 接口实现。因此，这里取名为 TimeProxy 其实已经不合适了。我们可以修改为和 JDK 命名一致，即 $Proxy0，感兴趣的同学请自行实践。
JDK 实现揭秘 通过上面的这些步骤，我们完成了一个简易的仿 JDK 实现的动态代理逻辑。接下来，我们一起来看一看 JDK 实现的动态代理和我们到底有什么不同。
Proxy.java
InvocationHandler
可以看到，官方版本 Proxy 类提供的方法多一些，而我们主要使用的接口 newProxyInstance 参数也和我们设计的不太一样。这里给大家简单解释一下，每个参数的意义：
**Classloader：**类加载器，你可以使用自定义的类加载器，我们的实现版本为了简化，直接在代码中写死了 Classloader。 **Class<?>[]：**第二个参数也和我们的实现版本不一致，这个其实很容易理解，我们应该允许我们自己实现的代理类同时实现多个接口。前面设计只传入一个接口，只是为了简化实现，让你专注核心逻辑实现而已。 最后一个参数就不用说了，和我们实现的版本完全是一样的。
仔细观察官方版本的 InvocationHandler，它和我们自己的实现的版本也有一个细微的差别：官方版本 invoke 方法有返回值，而我们的版本中是没有返回值的。那么，返回值到底有什么作用呢？直接来看官方文档：
核心思想：这里的返回值类型必须和传入接口的返回值类型一致，或者与其封装对象的类型一致。
遗憾的是，这里并没有说明返回值的用途，其实这里稍微发挥一下想象力就知道了。在我们的版本实现中，Flyable 接口的所有方法都是没有返回值的，问题是，如果有返回值呢？是的，你没有猜错，这里的 invoke 方法对应的就是传入接口中方法的返回值。
答疑解惑 invoke 方法的第一个参数 proxy 到底有什么作用？ 这个问题其实也好理解，如果你的接口中有方法需要返回自身，如果在 invoke 中没有传入这个参数，将导致实例无法正常返回。在这种场景中，proxy 的用途就表现出来了。简单来说，这其实就是最近非常火的链式编程的一种应用实现。
动态代理到底有什么用？ 学习任何一门技术，一定要问一问自己，这到底有什么用。其实，在这篇文章的讲解过程中，我们已经说出了它的主要用途。你发现没，使用动态代理我们居然可以在不改变源码的情况下，直接在方法中插入自定义逻辑。这有点不太符合我们的一条线走到底的编程逻辑，这种编程模型有一个专业名称叫 AOP。所谓的 AOP，就像刀一样，抓住时机，趁机插入。
基于这样一种动态特性，我们可以用它做很多事情，例如：
事务提交或回退（Web 开发中很常见） 权限管理 自定义缓存逻辑处理 SDK Bug 修复 &hellip; 总结 到此为止，关于动态代理的所有讲解已经结束了，原谅我使用了一个诱导性的标题 “骗” 你进来阅读这篇文章。如果你不是一个久经沙场的 “老司机”，10 分钟完全看懂动态代理设计模式还是有一定难度的。但即使没有看懂也没关系，如果你在第一次阅读完这篇文章后依然一头雾水，就不妨再仔细阅读一次。在阅读的过程中，一定要跟着文章思路去敲代码。反反复复，一定会看懂的。我在刚刚学习动态代理设计模式的时候就反复看了不下 5 遍，并且亲自敲代码实践了多次。
为了让你少走弯路，我认为看懂这篇文章，你至少需要学习以下知识点：
至少已经理解了面向对象语言的多态特性 了解简单的反射用法 会简单使用 JavaPoet 生成 Java 源码 ]]></content></entry><entry><title>三十而立之年</title><url>/2019/12/31/thirty-years-old/</url><categories><category>杂谈</category></categories><tags><tag>杂谈</tag></tags><content type="html">又一年即将结束，时间过得真的很快，快到不敢静下来细细回忆。小时候觉得二十多岁的人年纪好大，再后来觉得三十岁应该离自己很远吧，没想到自己的三十而立之年来的这么“快”，总感觉自己还很是个小孩，总感觉自己还没长大，实际已经到了上有老下有小的年纪&amp;hellip;
19 年最开心的事儿就是当了爸爸，真心的感谢媳妇艰辛付出，感激上天给我们最好的礼物。最不幸的事是父亲得了一场大病，家人辛苦奔波不说父亲也是受尽了病痛的折磨，医院真是最不愿意去的地方。有时候天真的想，如果能让家人健健康康无病无痛，付出多少都会有人愿意吧！
在北京已经整整 8 年了，虽不说劳苦奔波，但每天也很疲惫，一直想着回西安去好好安顿自己的家，尤其是面对不管奋斗多久也在这个城市安不了一个家的现实的时候。但有孩子之后经济压力徒增，回去之后恐怕入不敷出，一年一年的过的很快，那就再在北京待一年再看情况吧！为了能够不焦虑的回西安 20 年得让自己有点儿改观了。
19 年最后一天也快下班了，20 年继续加油吧！</content></entry><entry><title>Rust 中字符串总结</title><url>/2019/08/04/string-in-rust/</url><categories><category>Rust</category></categories><tags><tag>Rust</tag></tags><content type="html"><![CDATA[Rust 中字符串和 Java 和 Go 中的表示有很大的区别，刚开始接触的时候有点儿懵，今天花点时间总结备忘一下。
Rust 字符串有两种形式：str 和 String，str 是内置的原始数据类型，通常是以借用的形式（&amp;str 字符串 slice）使用，而 String 是标准库提供的一种结构体，内部存储一个 u8 类型的 Vec：
pub struct String { vec: Vec&lt;u8&gt;, } str let s = &#34;hello world!&#34;; // let s: &amp;&#39;static str = &#34;hello world!&#34;; s 变量是一个 &amp;str 类型，不可变的字符串，也可称为字面量，文本被直接储存在程序的二进制文件中，拥有固态生命周期（'static），是 let s: &amp;'static str = &quot;hello world!&quot;; 的简写定义方式。
String let mut s1 = String::new(); // let mut s1 = String::from(&#34;hello world!&#34;); s1.push_str(&#34;hello world!&#34;); s1.push_str(&#34; welcome.&#34;); println!(&#34;{}&#34;, s1); // Output: // hello world! welcome. String 是可变的、有所有权的、堆上分配的 UTF-8 的字节缓冲区。
相互转换 &amp;str -&gt; String let s = &#34;hello world!&#34;; // variable s: &amp;str let s1 = String::from(s); // variable s1: String let s1 = s.to_string(); let s1 = s.to_owned(); String -&gt; &amp;str let s = String::from(&#34;hello world!&#34;); // variable s: String let s1 = s.as_str(); // variable s1: &amp;str let s1 = &amp;s[..]; // 相当于：&amp;s[0..s.len()]; &amp;String 可以当做是 &amp;str，例如：
fn main() { let s = String::from(&#34;hello world!&#34;); foo(&amp;s); } fn foo(s: &amp;str) { println!(&#34;{}&#34;, s); } + 号运算 字符串 + 号运算是 String 的一个内联函数，定义如下：
impl Add&lt;&amp;str&gt; for String { type Output = String; #[inline] fn add(mut self, other: &amp;str) -&gt; String { self.push_str(other); self } } 所以两个字面量字符串（&amp;str）不能使用 +，例如：&quot;hello &quot; + &quot;world&quot;; 会报错误： '+' cannot be used to concatenate two '&amp;str' strings。
根据 + 的定义，一个可变的 String 字符串进行 + 后会失去所有权，例如：
let mut s = String::from(&#34;hello world!&#34;); let s1 = s + &#34; welcome.&#34;; // println!(&#34;{}, {}&#34;, s, s1); // ^ value borrowed here after move 以上代码会出现以下警告：
// | // | let mut s = String::from(&#34;hello world!&#34;); // | ----^ // | | // | help: remove this `mut` // | // = note: #[warn(unused_mut)] on by default 查阅说是有 #[warn(unused_mut)] 注解后 variable does not need to be mutable。去掉以上代码中 String 定义时候用以标识可变的 mut 关键字就可以了。不太明白为啥可以这样，待日后详查。
]]></content></entry><entry><title>按比例控制流量的一种实现</title><url>/2019/07/19/control-traffic-by-rate/</url><categories><category>笔记</category></categories><tags><tag>流控</tag></tags><content type="html"><![CDATA[网关做灰度的时候，要控制流量的比例，比如 3:7 的分发流量到两个不同版本的服务上去。刚开始的想法是每次流量过来生成 100 以内的随机数，随机数落在那个区间就转到那个版本的服务上去，但是发现这样无法较精准的保证 3:7 的比例，因为有可能某段时间内生成的随机数大范围的落在某个区间内，比如请求了 100 次，每次生成的随机数都是大于 30 的，这样 70% 比例的服务就承受了 100% 的流量。
接下来想到了第二种解决方案，能够保证 10（基数） 倍的流量比例正好是 3:7，思路如下：
1、生成 0 - 99 的数组（集合） 2、打乱数组（集合）的顺序，为了防止出现某比例的流量集中出现 3、全局的计数器，要考虑原子性 4、从数组（集合）中取出计数器和 100 取余后位置的值 5、判断取到的值落在那个区间
以下是 Java 的简单实现：
package io.github.ehlxr.rate; import java.util.Collections; import java.util.List; import java.util.concurrent.atomic.AtomicInteger; /** * 按比例控制流量 * * @author ehlxr * @since 2019-07-19. */ public class RateBarrier { private AtomicInteger op = new AtomicInteger(0); private List&lt;Integer&gt; source; private int base; private int rate; public boolean allow() { return source.get(op.incrementAndGet() % base) &lt; rate; } private RateBarrier() { } public RateBarrier(int base, int rate) { this.base = base; this.rate = rate; source = new ArrayList&lt;&gt;(base); for (int i = 0; i &lt; base; i++) { source.add(i); } // 打乱集合顺序 Collections.shuffle(source); } } 以下是 3:7 流量控制的测试：
package io.github.ehlxr.rate; /** * @author ehlxr * @since 2019-07-19. */ public class Main { public static void main(String[] args) { RateBarrier rateBarrier = new RateBarrier(10, 3); final Thread[] threads = new Thread[20]; for (int i = 0; i &lt; threads.length; i++) { threads[i] = new Thread(() -&gt; { if (rateBarrier.allow()) { System.out.println(&#34;this is on 3&#34;); } else { System.out.println(&#34;this is on 7&#34;); } }); threads[i].start(); } for (Thread t : threads) { try { t.join(); } catch (InterruptedException e) { e.printStackTrace(); } } } } // Output: /* this is on 7 this is on 7 this is on 7 this is on 7 this is on 3 this is on 7 this is on 3 this is on 7 this is on 7 this is on 3 this is on 7 this is on 7 this is on 7 this is on 7 this is on 3 this is on 7 this is on 3 this is on 7 this is on 7 this is on 3 */ 以下是 Golang 版本 2:3:5 比例分流的简单实现：
package main import ( &#34;fmt&#34; &#34;math/rand&#34; &#34;sync&#34; &#34;sync/atomic&#34; ) type RateBarrier struct { source []int op uint64 base int } func NewRateBarrier(base int) *RateBarrier { source := make([]int, base, base) for i := 0; i &lt; base; i++ { source[i] = i } // 随机排序 rand.Shuffle(base, func(i, j int) { source[i], source[j] = source[j], source[i] }) return &amp;RateBarrier{ source: source, base: base, } } func (b *RateBarrier) Rate() int { return b.source[int(atomic.AddUint64(&amp;b.op, 1))%b.base] } func main() { var wg sync.WaitGroup wg.Add(20) // 2:3:5 b := NewRateBarrier(10) for i := 0; i &lt; 20; i++ { go func() { rate := b.Rate() switch { case rate &lt; 2: fmt.Println(&#34;this is on 20%&#34;) case rate &gt;= 2 &amp;&amp; rate &lt; 5: fmt.Println(&#34;this is on 30%&#34;) case rate &gt;= 5: fmt.Println(&#34;this is on 50%&#34;) } wg.Done() }() } wg.Wait() } // Output: /* this is on 30% this is on 50% this is on 30% this is on 20% this is on 50% this is on 50% this is on 50% this is on 20% this is on 30% this is on 20% this is on 50% this is on 30% this is on 30% this is on 50% this is on 50% this is on 50% this is on 20% this is on 50% this is on 50% this is on 30% */ ]]></content></entry><entry><title>关于跑步</title><url>/2019/04/21/run/</url><categories><category>杂谈</category></categories><tags><tag>杂谈</tag></tags><content type="html">天气暖和了，19 年的跑步计划已经在心中盘算好久了，本打算昨天生日的时候去开始的，结果天公不作美，一直在下小雨。
关于跑步，一开始为了减肥，现在慢慢喜欢上了这项运动。其实跑步很折磨人，总得有个目标才能坚持下来，或是为了能够在朋友圈炫耀，或是享受跑完步之后的大汗淋漓。
我会下载一个跑步软件，戴上耳机，每完成一公里就会提醒我，我也会心中默默给自己设定个目标，十公里或一个小时，每次跑步过程中我都在和自己较劲，想要放弃的时候心中总会默念，再坚持一下，已经完成了三分之二了，已经完成五分之四了&amp;hellip;&amp;hellip;设了一个目标，总会能够达成，即使过程很艰难，但更喜欢达标后的小小满足感。</content></entry><entry><title>有人把生活过成了诗，有人却在彷徨挣扎…</title><url>/2019/04/18/essay/</url><categories><category>杂谈</category></categories><tags><tag>杂谈</tag></tags><content type="html">有人把生活过成了诗，有人却在彷徨挣扎…
总有一些人能让自己触动心弦，仔细想，也许他们并没有比自己“能耐”多少，或许只是自己已习惯奔波，没有了理想，活在世俗里无法自拔…
我想试着重新生活，找回自己喜欢并且想要成为的那个我，也许吧，这一次我能持之以恒。不再愿意立下各种 flag，不是因为害怕被啪啪打脸，反正都是给自己的。
但是，为什么每次都是想着要改变自己了，真有这么讨厌自己吗？每次都想要改变什么了？给自己制定一套目标，努力克制自己去实现？追寻自己的内心去生活不应该是自己向往的吗，我找不到答案。</content></entry><entry><title>可靠消息最终一致性分布式事务实现方案</title><url>/2019/01/25/eventually-consistency/</url><categories><category>分布式</category></categories><tags><tag>分布式事务</tag><tag>事务</tag></tags><content type="html">提到分布式应用，就不得不考虑分布式事务。在分布式事务中，常见的有 CAP，BASE 理论，解决方案也有很多种，比如：2PC、TCC 、最终一致性等。
2PC（两阶段提交）比较适合单块应用，跨多个库的分布式事务。因为严重依赖于数据库层面来搞定复杂的事务，效率很低，绝对不适合高并发的场景，而且，对于微服务而言，不推荐一个服务出现跨多个数据库操作， 如果需要操作其它数据库数据，推荐通过调用别的服务接口来实现。
TCC 属于强一致性事务的方案，适用资金流转业务相关业务，比如：支付、交易等场景。根据 CAP 理论，这种实现需要牺牲可用性。
如果是一般的分布式事务场景，比如：订单插入之后要调用库存服务更新库存，库存数据没有资金那么的敏感，可以用可靠消息最终一致性方案。
下面是一种可靠消息最终一致性事务方案的实现流程：
正常流程：
A 系统发送预发送消息给消息服务系统。
消息服务系统存储预发送的消息到消息数据库。
消息服务系统返回存储预发送消息的结果到 A 系统。
如果第 3 步返回的结果是成功的， A 系统则执行业务操作，否则不执行。
A 系统业务操作成功后，通知消息服务系统 。
消息服务系统发送消息到 MQ ，并且更新预发送消息状态为已发送（但不是已消费）。
MQ 发送消息到 B 系统。
B 系统执行业务操作，保证幂等性，防止同一个消息重复执行。
B 系统向 MQ ack 此条消息，并向消息服务系统进行确认成功消费消息，让消息服务系统将消息状态置为已消费。
消息恢复系统定时去消息服务系统查一下消息数据，查看有没有状态为非已消费（预发送和已发送）状态的超时（比如 2 分钟以上还未消费的）消息。
如果第 10 步发现有非已消费状态的超时消息，调用 A 系统提供的查询接口，查询次条消息对应的业务数据是否为处理成功。
如果业务数据是处理成功的状态，那么就再次调用确认并发送消息，即进入第 6 步。如果业务数据是处理失败的，那么就调用消息服务系统进行删除该条消息数据。
再来看看有错（比如说网络断了或者服务器挂了）的时候，这个系统是怎么保证一致性的：
第 1 步失败，相当于什么都没做。 第 2 步失败，第 3 步会返回失败结果，A 系统不执行业务操作。 第 3 步失败，A 系统不执行业务操作，消息恢复系统在第 12 步判断业务处理失败。 第 4 步失败，A 系统回滚业务，同样消息恢复系统在第 12 步判断业务处理失败。 第 5、6、7、8、9 步失败，消息恢复系统在第 12 步判断业务处理成功，重试第 6 步直到成功为止。如果在第 9 步失败了，B 系统会重复消费某条消息，所以 B 系统要设计成幂等操作，对于同一操作发起的一次请求或者多次请求的结果是一致的，不会因为多次调用而产生了副作用。 所以，只要消息数据持久化了，我们就可以假设后面一定会被消费，就算后面挂了一堆东西，但是我们把挂掉的服务再全部启动，这条消息还是会被消费，不会丢失，可以保证最终一致性。
这种实现方案弱化了对消息中间件（MQ）的依赖，选用 RabbitMQ 或者 ActiveMQ 就可以实现。如果使用支持消息事物的 RocketMQ 也可以简化消息恢复系统和消息服务系统。</content></entry><entry><title>浅谈分布式事务</title><url>/2019/01/25/distributed-system-transaction/</url><categories><category>分布式</category></categories><tags><tag>分布式事务</tag><tag>事务</tag></tags><content type="html">通俗的理解，事务是一组原子操作单元。我们希望一些列的操作能够全部正确执行，如果这一组操作中的任意一个步骤发生错误，那么就需要回滚之前已经完成的操作。也就是同一个事务中的所有操作，要么全都正确执行，要么全都不要执行。
传统的单机应用系统一般使用一个关系型数据库，利用数据库事务来保证数据的一致性，下面先了解一下本地数据库事务的一些特性。
一、本地数据库事务 事务从数据库角度说，就是一组 SQL 指令，要么全部执行成功，若因为某个原因其中一条指令执行有错误，则撤销先前执行过的所有指令。
关系型数据库（例如：MySQL、SQL Server、Oracle 等）事务都有以下几个特性：原子性（Atomicity）、一致性（Consistency）、隔离性或独立性（Isolation）和持久性（Durabilily），简称就是 ACID。
原子性：表示事务执行过程中的任何失败都将导致事务所做的任何修改失效。 一致性：表示当事务执行失败时，所有被该事务影响的数据都应该恢复到事务执行前的状态。 隔离性：表示在事务执行过程中对数据的修改，在事务提交之前对其他事务不可见。 持久性：表示已提交的数据在事务执行失败时，数据的状态都应该正确。 数据库事务操作也比较简单：开始一个事务，改变（插入，删除，更新）很多行，然后提交事务（如果有异常时回滚事务）。
Connection con = null; try { ​ // 工具类得到 connection 对象 ​ con = JdbcUtils.getConnection(); ​ // 关闭自动提交，开启事务 ​ con.setAutoCommit(false); ​ // 增、删、改 等操作 ​ ... ​ // 成功操作后提交事务 ​ con.commit(); ​ con.close(); } catch (Exception e) { ​ try { ​ // 如果有异常时回滚事务 ​ con.rollback(); ​ con.close(); ​ } catch (SQLException e1) { ​ e1.printStackTrace(); ​ } } 更进一步，借助开发平台中的数据访问技术和框架（如：Spring），我们需要做的事情更少，只需要关注数据本身的改变。
随着组织规模不断扩大，业务量不断增长，单机应用和数据库已经不足以支持庞大的业务量和数据量，这个时候需要对应用和数据库进行拆分，就出现了一个应用需要同时访问两个或两个以上的数据库情况。开始我们用分布式事务来保证一致性。
二、分布式事务理论 2.1 CAP 定理 CAP 定理是由加州大学伯克利分校 Eric Brewer 教授提出来的，他指出 WEB 服务无法同时满足以下 3 个属性：
一致性（Consistency）： 客户端知道一系列的操作都会同时发生（生效） 可用性（Availability）：每个操作都必须以可预期的响应结束 分区容错性（Partition tolerance）：即使出现单个组件无法可用, 操作依然可以完成 CAP 理论告诉我们，在分布式系统中，C、A、P 三个条件中我们最多只能选择两个。那么问题来了，究竟选择哪两个条件较为合适呢？
对于一个业务系统来说，可用性和分区容错性是必须要满足的两个条件，并且这两者是相辅相成的。业务系统之所以使用分布式系统，主要原因有两个：
提升整体性能：当业务量猛增，单个服务器已经无法满足我们的业务需求的时候，就需要使用分布式系统，使用多个节点提供相同的功能，从而整体上提升系统的性能，这就是使用分布式系统的第一个原因。 实现分区容错性：单一节点或多个节点处于相同的网络环境下，那么会存在一定的风险，万一该机房断电、该地区发生自然灾害，那么业务系统就全面瘫痪了。为了防止这一问题，采用分布式系统，将多个子系统分布在不同的地域、不同的机房中，从而保证系统高可用性。 这说明分区容错性是分布式系统的根本，如果分区容错性不能满足，那使用分布式系统将失去意义。
此外，可用性对业务系统也尤为重要。在大谈用户体验的今天，如果业务系统时常出现 “系统异常”、响应时间过长等情况，这使得用户对系统的好感度大打折扣，在互联网行业竞争激烈的今天，相同领域的竞争者不甚枚举，系统的间歇性不可用会立马导致用户流向竞争对手。因此，我们只能通过牺牲一致性来换取系统的可用性（A）和分区容错性（P）。这也就是下面要介绍的 BASE 理论。
2.2 BASE 理论 CAP 理论告诉我们一个悲惨但不得不接受的事实——我们只能在 C、A、P 中选择两个条件。而对于业务系统而言，我们往往选择牺牲一致性来换取系统的可用性和分区容错性。不过这里要指出的是，所谓的 “牺牲一致性” 并不是完全放弃数据一致性，而是牺牲强一致性换取弱一致性。下面来介绍下 BASE 理论。
**Basically Available（基本可用）**整个系统在某些不可抗力的情况下，仍然能够保证 “可用性”，即一定时间内仍然能够返回一个明确的结果。 **Soft state（软状态）**同一数据的不同副本的状态，可以不需要实时一致。 **Eventually Consistent（最终一致性）**同一数据的不同副本的状态，可以不需要实时一致，但一定要保证经过一定时间后仍然是一致的。 BASE 理论是对 CAP 中的一致性和可用性进行一个权衡的结果，理论的核心思想就是：我们无法做到强一致，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual Consistency）。
有了以上理论之后，我们来看一下分布式事务的解决方案。
三、分布式事务 3.1 两阶段提交（2PC） 两阶段提交协议（Two-phase Commit，2PC）经常被用来实现分布式事务。一般分为协调器和若干事务执行者两种角色，这里的事务执行者就是具体的数据库，抽象点可以说是可以控制数据库的程序。 协调器可以和事务执行器在一台机器上。
在分布式系统中，每个节点虽然可以知晓自己的操作时成功或者失败，却无法知道其他节点的操作的成功或失败。当一个事务跨越多个节点时，为了保持事务的 ACID 特性，需要引入一个作为协调者的组件来统一掌控所有节点 (称作参与者)。
图示从支付宝向余额宝转账是怎样保证一致性的
用户发起转账申请，首先到事务协调器 事务协调器通知（prepare）支付宝扣款，同时通知余额宝收款 支付宝、余额宝分别执行扣款、收款业务操作，本地事务不提交。并且把执行结果反馈给事务协调器，成功反馈 yes，失败反馈 no 事务协调器收到支付宝和余额宝的反馈如果都是 yes，则通知支付宝和余额宝系统提交事务（commit），否则通知回顾事务（abort） 事务协调器、支付宝和余额宝在整个过程收到通知都要记录日志（log），类似日常生活中的凭证。如果某个节点宕机，可以保证从日志中恢复后续操作。 两阶段提交这种解决方案属于牺牲了一部分可用性来换取的一致性。
优点： 尽量保证了数据的强一致，适合对数据强一致要求很高的关键领域。（其实也不能 100% 保证强一致） 缺点： 涉及多次节点间的网络通信时，牺牲了可用性，对性能影响较大，不适合高并发高性能场景，如果分布式系统跨接口调用。
3.2 补偿事务（TCC） TCC 其实就是采用的补偿机制，其核心思想是：针对每个操作，都要注册一个与其对应的确认和补偿（撤销）操作。它分为三个阶段：
Try 阶段：主要是对业务系统做检测及资源预留 Confirm 阶段：主要是对业务系统做确认提交，Try 阶段执行成功并开始执行 Confirm 阶段时，默认 Confirm 阶段是不会出错的。即：只要 Try 成功，Confirm 一定成功。 Cancel 阶段：主要是在业务执行错误，需要回滚的状态下执行的业务取消，预留资源释放。 举个例子，假如 Bob 要向 Smith 转账，思路大概是：
我们有一个本地方法，里面依次调用
首先在 Try 阶段，要先调用远程接口把 Smith 和 Bob 的钱给冻结起来。 在 Confirm 阶段，执行远程调用的转账的操作，转账成功进行解冻。 如果第 2 步执行成功，那么转账成功，如果第二步执行失败，则调用远程冻结接口对应的解冻方法（Cancel）。 优点： 跟 2PC 比起来，实现以及流程相对简单了一些，但数据的一致性比 2PC 也要差一些
缺点： 缺点还是比较明显的，在 2、3 步中都有可能失败。TCC 属于应用层的一种补偿方式，所以需要程序员在实现的时候多写很多补偿的代码，在一些场景中，一些业务流程可能用 TCC 不太好定义及处理。
3.3 分布式事务框架 常用开源的分布式事务框架：
阿里巴巴开源的分布式事务解决方案 Seata 一款事务协调性框架 TX-LCN 强一致分布式事务框架 Raincat 提供柔性事务的支持，包含 TCC, TAC(自动生成回滚 SQL) 方案 Hmily TCC 型事务 java 实现 tcc-transaction 基于事务管理器（TransactionManager）实现的 TCC 全局事务 ByteTCC</content></entry><entry><title>About</title><url>/about/</url><categories/><tags/><content type="html">屌丝码农。。。
About Blog 本站是使用 Hugo 博客框架搭建，主题使用的是 hugo-theme-next ，镜像托管在 Vercel 之上。
Copyright 本站是为了个人交流学习而搭建，本站内容若非注明【转】均来自本人原创，转载必会注明原文引用地址，若您认为侵犯你的个人知识产权，请联系 我邮箱 ，我会第一时间和您进行沟通处理。若你喜欢本站内容，欢迎转载，但请注明出处，且勿用于商业用途，谢谢！！！
Cracking the code</content></entry><entry><title>恍恍惚惚又一年</title><url>/2018/12/31/summary-in-2018/</url><categories><category>年末总结</category></categories><tags><tag>年末总结</tag></tags><content type="html">再强大的内心，估计也抵抗不住岁月的流逝吧&amp;hellip;那天和媳妇聊天，说起十年前，觉得十年好遥远，现在觉得十年前就想是在昨天。
今天是 2018 年的最后一天，像去年的今天一样，仿佛就像是在昨天，想着总结一下过去的一年，发现啥都写不出来。
今年为了西安房子的装修，从 6 月份后几乎每隔一月就在西安和北京奔波一回。装修房子的过程是痛苦的，什么都不懂，多花了不少冤枉钱，效果也不太满意，如果还会有第二套房子装修，我想应该会好很多吧！
工作上马马虎虎，个人能力感觉也没什么长进。又一波互联网大寒冬来袭，倒了一大批创业公司，各大公司也纷纷传出裁员的新闻，瑟瑟发抖，越来越焦虑了！
不管怎么样，2019 还是值得期待的一年。再见 2018，你好 2019！</content></entry><entry><title>JVM-垃圾回收（二）</title><url>/2018/08/23/jvm-gc2/</url><categories><category>Java开发技术</category></categories><tags><tag>JVM</tag><tag>Java</tag></tags><content type="html"> 接着上次 JVM 中 GC 机制的总结，这次主要复习一下垃圾收集的常用算法和 Minor GC、Full GC 相关的一些知识点。
一、垃圾收集算法 1.1 标记 - 清除（Mark-Sweep） 算法分成 “标记”、“清除” 两个阶段：首先标记出所有需要回收的对象（两次标记），在标记完成后统一回收所有被标记的对象。如下图所示：
标记－清除算法的不足主要有以下两点：
空间问题，会产生大量不连续的内存碎片，导致无法给大对象分配内存。 效率问题，因为内存碎片的存在，操作会变得更加费时，因为查找下一个可用空闲块已不再是一个简单操作。 1.2 标记 - 整理（Mark-Compact） 此算法的标记过程与标记－清除算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉边界以外的内存。具体示意图如下所示：
1.3 复制（Copying） 将内存划分为大小相等的两块，每次只使用其中一块，当这一块内存用完了就将还存活的对象复制到另一块上面，然后再把使用过的内存空间进行一次清理。主要不足是只使用了内存的一半。
复制算法过程：
1.4 分代收集 JVM 采用分代收集（Generational Collection）算法，此算法相较于前几种没有什么新的特征，主要思想为：根据对象存活周期的不同将内存划分为几块，一般是把 Java 堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适合的收集算法：
新生代使用复制算法 在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。 老年代使用标记 - 清理 或者 标记 - 整理 算法 在老年代中，因为对象存活率高、没有额外空间对它进行分配担保，就必须使用 “标记 - 清除” 或 “标记 - 整理” 算法来进行回收。 二、Minor GC 和 Full GC 2.1 Minor GC 发生在新生代上，因为新生代对象存活时间很短，因此 Minor GC 会频繁执行，执行的速度一般也会比较快。
Minor GC 会使用复制收集算法进行垃圾回收，但是并不是将内存划分为大小相等的两块，而是分为一块较大的 Eden 空间和两块较小的 Survivor 空间，每次使用 Eden 空间和其中一块 Survivor。在回收时，将 Eden 和 Survivor 中还存活着的对象一次性复制到另一块 Survivor 空间上，最后清理 Eden 和使用过的那一块 Survivor。HotSpot 虚拟机的 Eden 和 Survivor 的大小比例默认为 8:1，保证了内存的利用率达到 90%。如果每次回收有多于 10% 的对象存活，那么一块 Survivor 空间就不够用了，此时需要依赖于老年代进行分配担保，也就是借用老年代的空间存储放不下的对象。
2.2 Full GC 发生在老年代上，老年代对象其存活时间长，因此 Full GC 很少执行，执行速度会比 Minor GC 慢很多。
2.3 Full GC 的触发条件 对于 Minor GC，其触发条件非常简单，当 Eden 区空间满时，就将触发一次 Minor GC。而 Full GC 则相对复杂，有以下条件：
2.3.1 调用 System.gc() 此方法的调用是建议 JVM 进行 Full GC，虽然只是建议而非一定，但很多情况下它会触发 Full GC，从而增加 Full GC 的频率，也即增加了间歇性停顿的次数。因此强烈建议能不使用此方法就不要使用，让虚拟机自己去管理它的内存，可通过 -XX:+ DisableExplicitGC 虚拟机参数来禁止 RMI 调用 System.gc()。
2.3.2 老年代空间不足 老年代空间不足的常见场景为大对象直接进入老年代、长期存活的对象进入老年代等。
为了避免以上原因引起的 Full GC，应当尽量不要创建过大的对象以及数组。除此之外，可以通过 -Xmn 虚拟机参数调大新生代的大小，让对象尽量在新生代被回收掉，不进入老年代。还可以通过 -XX:MaxTenuringThreshold 虚拟机参数调大对象进入老年代的年龄，让对象在新生代多存活一段时间。
2.3.3 空间分配担保失败 使用复制算法的 Minor GC 需要老年代的内存空间作担保，如果担保失败会执行一次 Full GC。
2.3.4 JDK 1.7 及以前的永久代空间不足 在 JDK 1.7 及以前，HotSpot 虚拟机中的方法区是用永久代实现的，永久代中存放的为一些 Class 的信息、常量、静态变量等数据，当系统中要加载的类、反射的类和调用的方法较多时，永久代可能会被占满，在未配置为采用 CMS 垃圾收集器的情况下也会执行 Full GC。如果经过 Full GC 仍然回收不了，那么虚拟机会抛出 java.lang.OutOfMemoryError。
为避免以上原因引起的 Full GC，可采用的方法为增大永久代空间或转为使用 CMS 垃圾收集器。
在 JDK 1.8 中用元空间替换了永久代作为方法区的实现，元空间是本地内存，因此减少了一种 Full GC 触发的可能性。
2.3.5 Concurrent Mode Failure 使用 CMS 垃圾收集器执行的过程中，同时有对象要放入老年代，而此时老年代空间不足（有时候 “空间不足” 是指 CMS GC 当前的浮动垃圾过多导致暂时性的空间不足），便会报 Concurrent Mode Failure 错误，并触发 Full GC。</content></entry><entry><title>JVM-垃圾回收（一）</title><url>/2018/08/23/jvm-gc1/</url><categories><category>Java开发技术</category></categories><tags><tag>JVM</tag><tag>Java</tag></tags><content type="html"><![CDATA[在 JVM 运行时数据区域中，程序计数器、虚拟机栈和本地方法栈这三个区域属于线程私有的，只存在于线程的生命周期内，线程结束之后也会消失，因此不需要对这三个区域进行垃圾回收。垃圾回收主要是针对 Java 堆和方法区进行。
一、判断对象状态 JVM 在回收一个对象时，首先要判断这个对象的状态，如果判断对象为无效的（没有被任何对象或变量引用），则需要被 JVM 垃圾回收器回收。
1.1 引用计数算法 给对象添加一个引用计数器，当对象增加一个引用时计数器加 1，引用失效时计数器减 1。引用计数不为 0 的对象仍然存活。
public class ReferenceCountingGC { public Object instance = null; public static void testGC() { ReferenceCountingGC objA = new ReferenceCountingGC(); ReferenceCountingGC objB = new ReferenceCountingGC(); objA.instance = objB; objB.instance = objA; objA = null; objB = null; // 假设在这行发生 GC，objA 和 objB 是否能被回收？ System.gc(); } } testGC() 方法执行后，objA 和 objB 不会被垃圾回收器回收，因为两个对象出现了循环引用，引用计数器永远不为 0，导致无法对它们进行回收。
虽然引用计数算法简单、高效，但是因为存在循环引用的问题，所以 JVM 并没有使用引用计数算法标记对象状态。
1.2 可达性分析算法 可达性分析（Tracing GC）通过 GC Roots 作为起始节点向下进行搜索，GC Roots 搜索的经过的路径称为引用链（Reference Chain），能够到达到的对象都是存活的（也就是引用链上的对象），不可达的对象被标记为无效的。
如图 Object5、Object6 和 Object7 虽然相互存在引用关系，但是 GC Roots 不可达，形成不了引用链，所以会被标记为无效的对象。
JVM 使用该算法来判断对象是否可被回收，在 Java 中 GC Roots 一般包含以下内容：
虚拟机栈中引用的对象 本地方法栈中引用的对象 方法区中类静态属性引用的对象 方法区中的常量引用的对象 只有引用类型的变量才有可能被认为是 GC Roots，值类型的变量永远不被认为是 GC Roots。而且 GC Roots 并不包括堆中对象所引用的对象，这样就不会出现循环引用。
可作为 GC Roots 的节点主要在全局性的引用与执行上下文中，GC Roots 必须是当前存活的引用类型对象。GC 管理的区域是 Java 堆，而虚拟机栈、方法区和本地方法栈不被 GC 所管理，因此选用这些区域内引用的对象作为 GC Roots，是不会被 GC 所回收的。其中虚拟机栈和本地方法栈都是线程私有的内存区域，只要线程没有终止，就能确保它们中引用的对象的存活。而方法区中类静态属性引用的对象是显然存活的。常量引用的对象在当前可能存活，因此，也可能是 GC Roots 的一部分。
二、垃圾回收过程 即使在可达性分析算法中不可达的对象，也不是一定会死亡的，它们暂时都处于 “缓刑” 阶段，要真正宣告一个对象 “死亡”，至少要经历两次标记过程。
2.1 第一次标记 如果对象在进行可达性分析后发现没有与 GC Roots 相连接的引用链，那它将会被第一次标记并且进行一次筛选，筛选的条件是该对象是否覆盖了 finalize() 方法：
若已覆盖该方法，并该对象的 finalize() 方法还没有被执行过，那么就会将 finalize() 扔到 F-Queue 队列中。 若未覆盖该方法，或者该对象的 finalize() 方法已经被执行过，则直接回收释放对象内存。 2.2 第二次标记 JVM 会自动建立一个低优先级的 Finalizer 线程去执行执行 F-Queue 队列中的 finalize() 方法。为了防止 F-Queue 队列中的其它对象长时间处于等待状态，而导致整个内存回收系统崩溃，一个对象在 finalize() 方法中执行缓慢，或者发生了死循环（更极端的情况），JVM 就直接停止其执行，将该对象清除回收。所以 JVM 不会让 F-Queue 队列等待所有的 finalize() 方法都执行结束。
2.3 对象重生或死亡 如果某个对象的 finalize() 方法时被执行时，与引用链上的任何一个对象建立了关联（例如：把自己（this 关键字）赋值给某个类变量或者对象的成员变量），那么该对象就在第二次标记时被移出 “即将回收” 的集合；如果没有，那么就会被垃圾收集器清除回收。
任何一个对象的 finalize() 方法都只会被 JVM 调用一次，所以自救也只能进行一次，如果回收的对象之前调用了 finalize() 方法，后面回收时就不会调用 finalize() 方法了。
使用 finalize() 方法来 “拯救” 对象是不值得提倡的，因为它不是 C/C++ 中的析构函数，而是 Java 刚诞生时为了使 C/C++ 程序员更容易接受它所做的一个妥协。它的运行代价高昂，不确定性大，无法保证各个对象的调用顺序。finalize() 能做的工作，使用 try-finally 或者其它方法都更适合、及时。
三、方法区的回收 因为在 JDK 8 之前方法区主要存放永久代对象，而永久代对象的回收率比新生代差很多，因此在方法区上进行回收性价比不高。
主要是对常量池的回收和对类的卸载。
类的卸载条件很多，需要满足以下三个条件，并且满足了也不一定会被卸载：
该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 java.lang.Class 对象没有在任何地方被引用，也就无法在任何地方通过反射访问该类方法。 可以通过 -Xnoclassgc 参数来控制是否对类进行卸载。
在大量使用反射、动态代理、CGLib 等 ByteCode 框架、动态生成 JSP 以及 OSGi 这类频繁自定义 ClassLoader 的场景都需要虚拟机具备类卸载功能，以保证不会出现内存溢出。
四、引用类型 无论是通过引用计算算法判断对象的引用数量，还是通过可达性分析算法判断对象是否可达，判定对象是否可被回收都与引用有关。
Java 具有四种强度不同的引用类型。
4.1 强引用 被强引用关联的对象不会被垃圾收集器回收。
使用 new 一个新对象的方式来创建强引用。
Object obj = new Object(); 4.2 软引用 被软引用关联的对象，只有在内存不够的情况下才会被回收。
使用 SoftReference 类来创建软引用。
Object obj = new Object(); SoftReference&lt;Object&gt; sf = new SoftReference&lt;Object&gt;(obj); obj = null; // 使对象只被软引用关联 4.3 弱引用 被弱引用关联的对象一定会被垃圾收集器回收，也就是说它只能存活到下一次垃圾收集发生之前。
使用 WeakReference 类来实现弱引用。
Object obj = new Object(); WeakReference&lt;Object&gt; wf = new WeakReference&lt;Object&gt;(obj); obj = null; WeakHashMap 的 Entry 继承自 WeakReference，主要用来实现缓存。
private static class Entry&lt;K,V&gt; extends WeakReference&lt;Object&gt; implements Map.Entry&lt;K,V&gt; Tomcat 中的 ConcurrentCache 就使用了 WeakHashMap 来实现缓存功能。ConcurrentCache 采取的是分代缓存，经常使用的对象放入 eden 中，而不常用的对象放入 longterm。eden 使用 ConcurrentHashMap 实现，longterm 使用 WeakHashMap，保证了不常使用的对象容易被回收。
public final class ConcurrentCache&lt;K, V&gt; { private final int size; private final Map&lt;K, V&gt; eden; private final Map&lt;K, V&gt; longterm; public ConcurrentCache(int size) { this.size = size; this.eden = new ConcurrentHashMap&lt;&gt;(size); this.longterm = new WeakHashMap&lt;&gt;(size); } public V get(K k) { V v = this.eden.get(k); if (v == null) { v = this.longterm.get(k); if (v != null) this.eden.put(k, v); } return v; } public void put(K k, V v) { if (this.eden.size() &gt;= size) { this.longterm.putAll(this.eden); this.eden.clear(); } this.eden.put(k, v); } } 4.4 虚引用 又称为幽灵引用或者幻影引用。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用取得一个对象实例。
为一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知。
使用 PhantomReference 来实现虚引用。
Object obj = new Object(); PhantomReference&lt;Object&gt; pf = new PhantomReference&lt;Object&gt;(obj); obj = null; ]]></content></entry><entry><title>Java 位运算笔记</title><url>/2018/08/02/java-positional-operator/</url><categories><category>Java开发技术</category></categories><tags><tag>Java</tag></tags><content type="html"><![CDATA[一些零碎的知识点总是似懂非懂，用法老是模棱两可，每次都要去网络上查询，长时间不用又忘记了。比如 Java 中的位运算。今天抽空归纳总结一下，加强一下记忆。
一、原码、反码和补码 1.1 原码 一个数在计算机中的二进制表示形式，叫做这个数的机器数。机器数是带符号的，在计算机用一个数的最高位存放符号，正数为 0, 负数为 1。所以，为区别起见，将带符号位的机器数对应的真正数值称为机器数的真值。
原码就是符号位加上真值的绝对值，即用第一位表示符号，其余位表示值。比如 8 位二进制:
[+1] 原 = 0000 0001 [-1] 原 = 1000 0001 第一位是符号位，因为第一位是符号位，所以 8 位二进制数的取值范围就是：[1111 1111 , 0111 1111]，即：[-127 , 127]
1.2 反码 反码的表示方法是：正数的反码是其本身，负数的反码是在其原码的基础上，符号位不变，其余各个位取反。
[+1] = [00000001] 原 = [00000001] 反 [-1] = [10000001] 原 = [11111110] 反 1.3 补码 补码的表示方法是：正数的补码就是其本身，负数的补码是在其原码的基础上，符号位不变，其余各位取反，最后 + 1。(即在反码的基础上 + 1)
[+1] = [00000001] 原 = [00000001] 反 = [00000001] 补 [-1] = [10000001] 原 = [11111110] 反 = [11111111] 补 二、左移运算（&laquo;） value &lt;&lt; num
num 指定要移位值；value 移动的位数。
将左操作数（value）转为二进制数后向左边移动 num 位，并且在低位补 0，高位丢弃。
例如：5 &lt;&lt; 2
0000 0000 0000 0000 0000 0000 0000 0101 5 的补码（同原码） 0000 0000 0000 0000 0000 0000 0001 0100 左移 2 位后，低位补 0。换算成 10 进制为 20 如果移动的位数超过了该类型的最大位数，那么编译器会对移动的位数取模。如：对 int 类型（最大位数 32）的数值移动 33 位，实际上只移动了 33 % 32 = 1 位。
注：n 位二进制，最高位为符号位，因此表示的数值范围：$ -2^{(n-1)} $ —— $ 2^{(n-1)}-1 $，所以模为：$ 2^{(n-1)} $。
在数字没有溢出的前提下，对于正数和负数，左移一位都相当于乘以 2 的 1 次方，左移 n 位就相当于乘以 2 的 n 次方。如：5 &lt;&lt; 2 相当于 $ 5 * 2^2 = 20 $。
如果移进高阶位（int 31 或 long 63 位），那么该值将变为负值。如：1 &lt;&lt; 31 = -2147483648
三、右移运算（&raquo;） value &gt;&gt; num
num 指定要移位值；value 移动的位数。
将左操作数（value）转为二进制数后向右边移动 num 位，符号位不变，高位补上符号位（若左操作数是正数，则高位补 0，若左操作数是负数，则高位补 1），低位丢弃。
右移时，被移走的最高位（最左边的位）由原来最高位的数字补充，这叫做符号位扩展（保留符号位）（sign extension），在进行右移操作时用来保持负数的符号。
例如：7 &gt;&gt; 2
0000 0000 0000 0000 0000 0000 0000 0111 7 的补码（同原码） 0000 0000 0000 0000 0000 0000 0000 0001 右移 2 位后，高位补 0。换算成 10 进制为 1 例如：-7 &gt;&gt; 2
1000 0000 0000 0000 0000 0000 0000 0111 -7 的原码 1111 1111 1111 1111 1111 1111 1111 1000 -7 的反码 1111 1111 1111 1111 1111 1111 1111 1001 -7 的补码 1111 1111 1111 1111 1111 1111 1111 1110 右移 2 位后，高位补 1 1000 0000 0000 0000 0000 0000 0000 0010 补码转原码。换算成 10 进制为 -2 正数右移 n 位相当于除以 2 的 n 次方并且舍弃了余数。如：7 &gt;&gt; 2 相当于： $ 7 / 2^2 = 1 $。
负数右移 n 位相当于除以 2 的 n 次方，如果有余数 -1。如：-7 &gt;&gt; 2 相当于： $ 7 * 2^2 -1= -2 $。
四、无符号右移（&raquo;&gt;） value &gt;&gt;&gt; num
num 指定要移位值；value 移动的位数。
将左操作数（value）转为二进制数后向右边移动 num 位，0 补最高位（忽略了符号位扩展）。
无符号右移运算只是对 32 位和 64 位的值有意义。
例如：-7 &gt;&gt;&gt; 2
1000 0000 0000 0000 0000 0000 0000 0111 -7 的原码 1111 1111 1111 1111 1111 1111 1111 1001 -7 的补码 0011 1111 1111 1111 1111 1111 1111 1110 右移 2 位后，高位补 0。换算成 10 进制为 1073741822 五、位逻辑运算符 5.1 与运算（&amp;） 与运算：两个运算数比较位都是 1，则结果为 1，否则为 0。例如：5 &amp; 3 = 1
0000 0000 0000 0000 0000 0000 0000 0101 5 转换为二进制 0000 0000 0000 0000 0000 0000 0000 0011 3 转换为二进制 0000 0000 0000 0000 0000 0000 0000 0001 换算成 10 进制为 1 5.2 或运算（|） 或运算：两个运算数比较位有一个为 1，则结果为 1，否则为 0。例如：5 | 3 = 7
0000 0000 0000 0000 0000 0000 0000 0101 5 转换为二进制 0000 0000 0000 0000 0000 0000 0000 0011 3 转换为二进制 0000 0000 0000 0000 0000 0000 0000 0111 换算成 10 进制为 7 5.3 异或运算（^） 异或运算：两个运算数比较位不同时，其结果是 1，否则为 0。例如：5 ^ 3 = 6
0000 0000 0000 0000 0000 0000 0000 0101 5 转换为二进制 0000 0000 0000 0000 0000 0000 0000 0011 3 转换为二进制 0000 0000 0000 0000 0000 0000 0000 0110 换算成 10 进制为 6 5.4 非运算（~） 非运算：也叫做补，一元运算符，对其运算数的每一位取反。例如：~5 = -6
0000 0000 0000 0000 0000 0000 0000 0101 5 转换为二进制 1111 1111 1111 1111 1111 1111 1111 1010 取非后的原码 1000 0000 0000 0000 0000 0000 0000 0110 转换补码，换算成 10 进制为 -6 六、其它 Java 中整数类型（byte、short、int 和 long）在内存中是以有符号的二进制补码表示。所以位运算时，首先要转换为原码。
补码转原码：补码转原码和原码转补码的方法是一样的，取反 + 1（补码的补码是原码）。
当位运算数是 byte 和 short 类型时，将自动把这些类型扩大为 int 型（32 位）。
计算出 n 位二进制数所能表示的最大十进制数位移算法：-1L ^ (-1L &lt;&lt; n) 或 ~(-1L &lt;&lt; n)。
byte 和 int 相互转换
int i = 234; byte b = (byte) i; // 结果：b = -22 // 转换过程： // 0000 0000 0000 0000 0000 0000 1110 1010 # int 234 的补码（与原码相等） // 1110 1010 # byte 低位截取 // 1001 0110 # 求得补码，转为 10 进制为 -22 int x = b ; // 结果为：x = -22；8 位 byte 的转 32 的 int，值不变。 int y = b &amp; 0xff; // 结果为：x = 234； 可以通过将其和 0xff 进行位与（&amp;）得到它的无符值 // 转换过程： // 1001 0110 # byte -22 的原码 // 1000 0000 0000 0000 0000 0000 0001 0110 # int -22 的原码 // 1111 1111 1111 1111 1111 1111 1110 1010 # int -22 补码 // 0000 0000 0000 0000 0000 0000 1111 1111 # 0xff 的二进制数 // 0000 0000 0000 0000 0000 0000 1110 1010 # 和 0xff 进与操作的结果，转换为 10 进制为 234 ]]></content></entry><entry><title>SQL 事务隔离</title><url>/2018/05/31/sql-transaction-isolation/</url><categories><category>SQL</category></categories><tags><tag>SQL</tag><tag>MySQL</tag><tag>数据库事务</tag><tag>事务</tag></tags><content type="html"><![CDATA[SQL 标准定义了 4 类隔离级别，包括了一些具体规则，用来限定事务内外的哪些改变是可见的，哪些是不可见的。低级别的隔离级一般支持更高的并发处理，并拥有更低的系统开销。
一、SQL 事务隔离级别说明 1.1 Read Uncommitted（读取未提交内容） 在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少。读取未提交的数据，也被称之为脏读（Dirty Read）。
1.2 Read Committed（读取提交内容） 这是大多数数据库系统的默认隔离级别（但不是 MySQL 默认的）。它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。这种隔离级别 也支持所谓的不可重复读（Nonrepeatable Read），因为同一事务的其他实例在该实例处理其间可能会有新的 commit，所以同一 select 可能返回不同结果。
1.3 Repeatable Read（可重读） 这是 MySQL 的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。不过理论上，这会导致另一个棘手的问题：幻读 （Phantom Read）。简单的说，幻读指当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的 “幻影” 行。InnoDB 和 Falcon 存储引擎通过多版本并发控制（MVCC，Multiversion Concurrency Control）机制解决了该问题。
1.4 Serializable（可串行化） 这是最高的隔离级别，它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁。在这个级别，可能导致大量的超时现象和锁竞争。
二、事务隔离带来的问题 这四种隔离级别采取不同的锁类型来实现，若读取的是同一个数据的话，就容易发生问题。例如：
2.1 脏读（Drity Read） 一个事务读取到另一事务未提交的更新数据。当一个事务正在访问数据，并且对数据进行了修改，而这种修改还没有提交到数据库中（这个数据在有可能会回滚），这时，另外一个事务也访问这个数据，然后使用了这个数据。
2.2 不可重复读 (Non-repeatable read） 在一个事务内，前后两次读到的数据是不一样。在 T1 事务两次读取同一数据之间，T2 事务对该数据进行了修改，就会发生 T1 事务中的两次数据读取不一样的结果。相反， 可重复读：在同一事务中多次读取数据时，能够保证所读数据一样，也就是后续读取不能读到另一事务已提交的更新数据。
2.3 幻读 (Phantom Read） 指当事务不是独立执行时发生的一种现象，例如：T1 事务对表中的 &ldquo;全部数据行&rdquo; 进行了修改，同时 T2 事务向表中插入了一行 &ldquo;新数据&rdquo;，操作 T1 事务的用户发现表中还存在没有修改的数据行，就好象发生了幻觉一 样。一般解决幻读的方法是增加范围锁 RangeS，锁定检锁范围为只读，这样就避免了幻读。
2.4 不可重复读和幻读的异同 两者都表现为两次读取的结果不一致 不可重复读是由于另一个事务对数据的更改所造成的，第二次读到了不一样的记录 幻读是由于另一个事务插入或删除引起的，第二次查询的结果发生了变化 对于不可重复读，只需要锁住满足条件的记录 对于幻读，要锁住满足条件及其相近的记录 三、MySQL 隔离级别 隔离级别 脏读（Dirty Read） 不可重复读（NonRepeatable Read） 幻读（Phantom Read） 未提交读（Read uncommitted） 可能 可能 可能 已提交读（Read committed） 不可能 可能 可能 可重复读（Repeatable read） 不可能 不可能 可能 可串行化（SERIALIZABLE） 不可能 不可能 不可能 四、MySQL 事务隔离级别设置 4.1 InnoDB 默认是可重复读的（REPEATABLE READ） 修改全局默认的事务级别，在 my.inf 文件的 [mysqld] 节里类似如下设置该选项（不推荐）
transaction-isolation = {READ-UNCOMMITTED | READ-COMMITTED | REPEATABLE-READ | SERIALIZABLE} 4.2 改变单个会话或者所有新进连接的隔离级别（推荐使用） SET [SESSION | GLOBAL] TRANSACTION ISOLATION LEVEL {READ UNCOMMITTED | READ COMMITTED | REPEATABLE READ | SERIALIZABLE} 4.3 查询全局和会话事务隔离级别方法 #查询全局的事务隔离级别 SELECT @@global.tx_isolation; #查询当前会话的事务级别 SELECT @@session.tx_isolation; ]]></content></entry><entry><title>JVM-运行时数据区域</title><url>/2018/04/08/jvm-runtime-data-area/</url><categories><category>Java开发技术</category></categories><tags><tag>JVM</tag><tag>Java</tag></tags><content type="html"><![CDATA[Java 的内存区域划分绝不仅仅只是堆内存（heap）和栈内存（Stack），实际上 JVM 在执行 Java 程序的过程中会把它所管理的内存划分为以下几个数据区域：程序计数器、Java 虚拟机栈、本地方法栈、堆、方法区、运行时常量和直接内存。如下图所示：
一、程序计数器（PC Register） 程序计数器（PC Register）是最小的一块内存区域，它的作用是记录正在执行的虚拟机字节码指令的地址。在虚拟机的模型里，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、异常处理、线程恢复等基础功能都需要依赖计数器完成。
每一个 Java 线程都有一个程序计数器，用以记录比如在线程切换回来后恢复到正确的执行位置。 如该线程正在执行一个 Java 方法，则计数器记录的是正在执行的虚拟机字节码地址，如执行 Native 方法，则计数器值为空。 此内存区域是唯一一个在 JVM 中没有规定任何 OutOfMemoryError 情况的区域。 二、Java 虚拟机栈（JVM Stacks） 每个 Java 方法在执行的同时会创建一个 &ldquo;栈帧&rdquo; 用于存储局部变量表（包括参数）、操作数栈（执行引擎计算时需要）、常量池引用、方法出口等信息。每一个方法从调用直至执行完成的过程，就对应着一个栈帧在 Java 虚拟机栈中入栈和出栈的过程。JVM 栈是线程私有的，并且生命周期与线程相同。并且当线程运行完毕后，相应内存也就被自动回收
局部变量表 存放了编译器可知的各种基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（引用指针，并非对象本身），其中 64 位长度的 long 和 double 类型的数据会占用 2 个局部变量的空间，其余数据类型只占 1 个。
局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在栈帧中分配多大的局部变量是完全确定的，在运行期间栈帧不会改变局部变量表的大小空间。
可以通过虚拟机参数 -Xss（例如：java -Xss=512M HackTheJava）来指定一个程序的 Java 虚拟机栈内存大小。
当线程请求的栈深度大于虚拟机所允许的深度，会抛出 StackOverflowError 异常（如：将一个函数反复递归自己，最终会出现这种异常）；如果 JVM 栈可以动态扩展（大部分 JVM 是可以的），当扩展时无法申请到足够内存，则会抛出 OutOfMemoryError 异常。
三、本地方法栈（Native Method Stacks） 本地方法不是用 Java 实现，对待这些方法需要特别处理。与 Java 虚拟机栈类似，它们之间的区别只不过是本地方法栈为本地方法服务。和 JVM 栈一样，这个区域也会抛出 StackOverflowError 和 OutOfMemoryError 异常。
四、堆（Heap） 堆（Heap）也叫做 Java 堆，GC 堆，是 Java 虚拟机所管理的内存中最大的一块内存区域，也是被各个线程共享的内存区域，在 JVM 启动时创建。该内存区域存放了对象实例 (所有 new 的对象)及数组，JIT 编译器貌似不是这样的。根据 Java 虚拟机规范的规定，Java 堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可。
其大小通过 - Xms（最小值）和 -Xmx（最大值）参数设置（例如：java -Xms=1M -Xmx=2M HackTheJava），-Xms 为 JVM 启动时申请的最小内存，默认为操作系统物理内存的 1/64 但小于 1G，-Xmx 为 JVM 可申请的最大内存，默认为物理内存的 1/4 但小于 1G，默认当空余堆内存小于 40% 时，JVM 会增大 Heap 到 -Xmx 指定的大小，可通过 - XX:MinHeapFreeRation 来指定这个比列；当空余堆内存大于 70% 时，JVM 会减小 heap 的大小到 -Xms 指定的大小，可通过 XX:MaxHeapFreeRation 来指定这个比列，对于运行系统，为避免在运行时频繁调整 Heap 的大小，通常 -Xms 与 -Xmx 的值设成一样。
如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出 OutOfMemoryError 异常。
Java 堆是垃圾收集管理的主要战场，现代的垃圾收集器基本都是采用分代收集算法，该算法的思想是针对不同的对象采取不同的垃圾回收算法，因此虚拟机把 Java 堆分成以下三块：新生代（Young Generation）、老年代（Old Generation）、永久代（Permanent Generation）。
4.1 新生代 程序新创建的对象都是从新生代分配内存，新生代存放着大量的生命很短的对象，因此新生代在三个区域中垃圾回收的频率最高。为了更高效地进行垃圾回收，把新生代继续划分成以下三个空间：Eden、From Survivor、To Survivor。
可通过 - Xmn 参数来指定新生代的大小，也可以通过 - XX:SurvivorRation 来调整 Eden Space 及 Survivor Space 的大小。
4.2 老年代 老年代用于存放经过多次新生代 GC 任然存活的对象，例如缓存对象。新建的对象也有可能直接进入老年代，主要有两种情况：
大对象，可通过启动参数设置 - XX:PretenureSizeThreshold=1024（单位为字节，默认为 0）来代表超过多大时就不在新生代分配，而是直接在老年代分配。 大的数组对象，且数组中无引用外部对象。 老年代所占的内存大小为 - Xmx 对应的值减去 - Xmn 对应的值。
4.3 永久代 永久代是 Hotspot 虚拟机特有的概念，是方法区的一种实现，别的 JVM 都没有这个东西。在 Java 8 中，永久代被彻底移除，取而代之的是另一块与堆不相连的本地内存——元空间。 永久代或者 &ldquo;Perm Gen&rdquo; 包含了 JVM 需要的应用元数据，这些元数据描述了在应用里使用的类和方法。注意，永久代不是 Java 堆内存的一部分。永久代存放 JVM 运行时使用的类。永久代同样包含了 Java SE 库的类和方法。永久代的对象在 Full GC 时进行垃圾收集。
五、方法区（Method Area） 方法区（Method Area）也称 &ldquo;永久代&rdquo;、&ldquo;非堆&rdquo;，它用于存储虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据，如：当程序中通过 getName、isInterface 等方法来获取信息时，这些数据来源于方法区。方法区是各个线程共享的内存区域，比如每个线程都可以访问同一个类的静态变量。默认最小值为 16MB，最大值为 64MB，可以通过 - XX:PermSize 和 -XX:MaxPermSize 参数限制方法区的大小。
和 Java 堆一样不需要连续的内存，并且可以动态扩展，动态扩展失败一样会抛出 OutOfMemoryError 异常。
由于使用反射机制的原因，虚拟机很难推测哪个类信息不再使用，因此这块区域的回收很难，对这块区域进行垃圾回收的主要目标是对常量池的回收和对类的卸载。方法区并不等同于永久代，只是因为 HotSpot VM 使用永久代来实现方法区，对于其他的 Java 虚拟机，比如 J9 和 JRockit 等，并不存在永久代概念。
六、运行时常量池（Runtime Constant Pool） 运行时常量池（Runtime Constant Pool）是方法区的一部分，值得注意的是 JDK1.7 已经把常量池转移到堆里面了。Class 文件中的常量池（编译器生成的各种字面量和符号引用）会在类加载后被放入这个区域。运行时常量池可以理解为是类或接口的常量池的运行时表现形式。除了在编译期生成的常量，还允许动态生成，例如 String 类的 intern()。这部分常量也会被放入运行时常量池。
当创建类或接口时，如果构造运行时常量池所需的内存超过了方法区所能提供的最大值，Java 虚拟机会抛出 OutOfMemoryError 异常。
七、直接内存（Direct Memory） 在 JDK 1.4 中新加入了 NIO 类，它可以使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆里的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为避免了在 Java 堆和 Native 堆中来回复制数据。
]]></content></entry><entry><title>Mac 实用小工具</title><url>/2018/03/30/mac-utility-tools/</url><categories><category>开发工具</category></categories><tags><tag>Mac</tag></tags><content type="html"> 记录备忘一下 Mac 系统下一些实用的小工具
收费 Bartender : 菜单栏管理工具，可以将菜单放进 Bartender 的二级菜单 SizeUp : 实用分屏小工具 Proxifier : 配合 ss/ssr 可以实现真正的全局代理软件 iStat Menus : 能够在系统菜单栏实时监控 CPU、内存、硬盘、网络、温度、电池以及系统时间等 CleanMyMac : 系统清理工具，可以清除 mac 系统多余的语言包、系统缓存、应用程序等 KeyCue : 快捷键辅助工具，忘记一些快捷键的时候按住 Command 就会出现快捷键大全的菜单 Araxis Merge : 可视化文件合并以及数据同步工具 BetterZip : 功能非常强大的压缩解压缩软件 Parallels Desktop : 最佳 Mac 虚拟机解决方案 TinyCal : 小历 - 小而美的日历 (OS X) 免费 Vanilla : Bartender 的免费替代 APP 官网 Hidden Bar : 隐藏 macOS 菜单栏不常用的应用图标，替代 Bartender 3 github AccessMenuBarApps : 影藏菜单栏的左侧部分内容。配合 Hidden Bar 使用（设置为同一个快捷键） 官网 Rectangle : SizeUp 的免费替代 APP github Alfred : Mac 效率神器，Spotlight 完美替代产品（需要付费购买部分功能） 官网 iTerm2 : 是一款完全免费的，专为 Mac OS 用户打造的命令行应用 官网 Typora : 极致简洁的 markdown 编辑器 官网 Kap : 免费开源的屏幕录像软件 github Karabiner Elements : 修改键位映射软件 github Gas Mask : 免费开源的 hosts 管理软件 github KeePassX : 免费开源的密码管理软件 github Keka : 丑到极致的压缩、解压软件（不换图标不忍直视） github The Unarchiver : 承诺会永久免费全能的解压缩软件 官网 Android 文件传输 : 安卓手机数据线传输助手 官网 AppCleaner : 应用程序卸载助手 官网 Tencent Lemon : 腾讯出品的 Mac 清理工具 官网 IINA : 很好用的本地视频播放器 github OpenInTerminal : 在终端（或编辑器）中打开当前目录 github FileZilla : 跨平台 FTP 客户端工具 官网 Mos : 平滑鼠标滚动效果或单独设置滚动方向的小工具 github Another.Redis.Desktop.Manager : github Draw.io : 强大简洁的在线的绘图网站，也提供了多平台的离线桌面版可供下载 github eZip : 简洁易用的免费压缩软件 官网 iRightMouse : 超级右键，强大的右键菜单工具 官网 Bob : Bob 是一款 Mac 端翻译软件 github ImageOptim : 是一款小巧免费且开源的 Mac 图片无损压缩优化工具 官网 Sourcetree : SourceTree 是 Windows 和 Mac OS X 下免费的 Git 客户端 官网 BitWarden : 免费的开源密码管理服务 github ，自我托管服务搭建 github</content></entry><entry><title>漂 泊</title><url>/2018/03/10/drifter/</url><categories><category>杂谈</category></categories><tags><tag>杂谈</tag></tags><content type="html">北漂，是一种选择，但越来越多的会是无奈和彷徨， 踌躇满志而来，万念俱灰而归，也许就是大多数北漂的归属，结局也许有些夸张。 最终，我还是无法爱上这座城市，最起码我努力爱过&amp;hellip;
月底又要搬家了，刚对一个地方稍微熟悉，就又要换一个地了，来北京这么多年，一年换一个地，行李越来越多，心情越来越不是滋味&amp;hellip;漂泊的人啊，怎样才能有一颗不流浪的心！
也许真是老了，伤感越来越多，前段时间经历了一次彻头彻尾失败的面试，对未来的工作越来越没自信，危机感油然而生，快要被淘汰了哦！在绝境中获得反省，希望能够被铭记&amp;hellip;
夜幕降临，希望每一位 Beijing Drifter 都能够安然入睡，梦回故里，第二天醒来依然对这个城市充满爱。</content></entry><entry><title>Golang 反射使用总结</title><url>/2018/01/26/golang-reflect/</url><categories><category>Golang</category></categories><tags><tag>Golang</tag><tag>Go</tag></tags><content type="html"><![CDATA[Go 语言中反射的操作主要定义在标准库 reflect 中，在标准库中定义了两种类型来表现运行时的对象信息，分别是： reflect.Value （反射对象的类型）和 reflect.Type （反射对象的值），Go 语言中所有反射操作都是基于这两个类型进行的。
{% img https://cdn.jsdelivr.net/gh/0vo/oss/images/golang-reflect.jpg 350 %}
为了方便演示操作（ 完整代码示例 ），首先定义以下结构体以及字段、方法：
type User struct { Name string `json:&#34;name&#34;` Age int `json:&#34;age&#34; default:&#34;18&#34;` addr string `json:&#34;addr&#34;` } func (u User) Do(in string) (string, int) { fmt.Printf(&#34;%s Name is %s, Age is %d \n&#34;, in, u.Name, u.Age) return u.Name, u.Age } 一、反射对象 Value 和 Type 既然 Go 语言中所有反射操作都是基于 Value 和 Type 进行的，那么想要进行反射操作，首先就要获取到反射对象的这两个类型对象才可以。
reflect 包提供了两个函数：reflect.ValueOf() 和 reflect.TypeOf()，通过这两个函数就可以方便的获取到任意类型（用 interface{} 表示任意类型）的 Value 对象和 Type 对象。例如：
u := User{&#34;tom&#34;, 27, &#34;beijing&#34;} v := reflect.ValueOf(u) fmt.Println(v) t := reflect.TypeOf(u) fmt.Println(t) 知道 Value 对象后，也可以通过 Value.Type() 方法获取到 Type 对象。例如：
t1 := v.Type() fmt.Println(t == t1) 可以看到输出结果为 true。
通过 Type 类型对象也可以获取到 Value 类型对象，不过是零值的指针。例如：
v1 := reflect.New(t) fmt.Println(v1) 结果为：&amp;{ 0}
二、反射对象的 Kind Kind 表示反射对象的类型 Type 所代表的具体类型，零值表示无效的类型，具体有以下类型值：
type Kind uint const ( Invalid Kind = iota Bool Int Int8 Int16 Int32 Int64 Uint Uint8 Uint16 Uint32 Uint64 Uintptr Float32 Float64 Complex64 Complex128 Array Chan Func Interface Map Ptr Slice String Struct UnsafePointer ) 可以通过 Value.Kind() 或者 Type.Kind() 函数获得。例如：
// 获取 Kind 类型 k := t.Kind() fmt.Println(k) k1 := v.Kind() fmt.Println(k1) fmt.Println(k == k1) fmt.Println() 可以看到两种方式获取的结果是一样的，都是 struct。
三、反射对象的字段 反射能够操作的字段和方法必须是可导出（首字母大写）的。
反射对象的字段值修改要通过调用 Value 类型的方法 Elem() 后返回的 Value 对象值来操作。
Elem() 方法定义：func (v Value) Elem() Value，返回 v 包含的值或指针 v 指向的值，v 的 Kind 如果不是 Interface 或 Ptr，则会 panic。
reflect.Indirect() 函数的如果参数是指针的 Value，则相当于调用了 Elem() 方法返回的值，否则返回 Value 自身值。
// 修改反射对象的值 i := 20 fmt.Println(&#34;before i =&#34;, i) e := reflect.Indirect(reflect.ValueOf(&amp;i)) // e := reflect.ValueOf(&amp;i).Elem() if e.CanSet() { e.SetInt(22) } fmt.Println(&#34;after i =&#34;, i) // 反射字段操作 // elem := reflect.Indirect(reflect.ValueOf(&amp;u)) elem := reflect.ValueOf(&amp;u).Elem() for i := 0; i &lt; t.NumField(); i++ { // 反射获取字段的元信息，例如：名称、Tag 等 ft := t.Field(i) fmt.Println(&#34;field name:&#34;, ft.Name) tag := ft.Tag fmt.Println(&#34;Tag:&#34;, tag) fmt.Println(&#34;Tag json:&#34;, tag.Get(&#34;json&#34;)) // 反射修改字段的值 fv := elem.Field(i) if fv.CanSet() { if fv.Kind() == reflect.Int { fmt.Println(&#34;change age to 30&#34;) fv.SetInt(30) } if fv.Kind() == reflect.String { fmt.Println(&#34;change name to jerry&#34;) fv.SetString(&#34;jerry&#34;) } } fmt.Println() } fmt.Println(&#34;after user:&#34;, u) 四、反射对象的方法 可以通过 Value 的 Method() 方法或 Type 的 Method() 方法，两种形式获取对象方法信息进行反射调用，略有不同，示例如下：
// 反射方法操作 for i := 0; i &lt; v.NumMethod(); i++ { method := t.Method(i) // 获取方法信息对象，方法 1 mt := method.Type // 获取方法信息 Type 对象，方法 1 // m := v.Method(i) // 获取方法信息对象，方法 2 // mt := m.Type() // 获取方法信息 Type 对象，方法 2 fmt.Println(&#34;method name:&#34;, method.Name) in := []reflect.Value{} // 获取方法入参类型 for j := 0; j &lt; mt.NumIn(); j++ { fmt.Println(&#34;method in type:&#34;, mt.In(j)) if mt.In(j).Kind() == reflect.String { in = append(in, reflect.ValueOf(&#34;welcome&#34;)) } // 方法 1 获取的方法信息对象会把方法的接受者也当着入参之一 if mt.In(j).Name() == t.Name() { in = append(in, v) } } // 获取方法返回类型 for j := 0; j &lt; mt.NumOut(); j++ { fmt.Println(&#34;method out type:&#34;, mt.Out(j)) } // 反射方法调用 // out := m.Call(in) // 方法 1 获取的 Method 对象反射调用方式 out := method.Func.Call(in) // 方法 1 获取的 Method 对象反射调用方式 for _, o := range out { fmt.Println(&#34;out:&#34;, o) } } 五、反射对象 Value 还原 通过 reflect.ValueOf() 可以把任意类型对象转换为 Value 类型对象，也可以通过 Value 类型的方法 Interface() 把 Value 类型对象还原为原始数据类型对象。
// Value 转原始类型 if u1, ok := v.Interface().(User); ok { fmt.Println(&#34;after:&#34;, u1.Name, u1.Age) } ]]></content></entry><entry><title>Kubernetes 学习笔记之 MiniKube 安装</title><url>/2018/01/12/kubernetes-minikube-installation/</url><categories><category>kubernetes</category></categories><tags><tag>minikube</tag><tag>kubernetes</tag><tag>k8s</tag></tags><content type="html"><![CDATA[Kubernetes 集群的搭建是有一定难度的，尤其是对于初学者来说，好多概念和原理不懂，即使有现成的教程也会出现很多不可预知的问题，很容易打击学习的积极性，就此弃坑。好在 Kubernetes 社区提供了可以在本地开发和体验的极简集群安装 MiniKube，对于入门学习来说很方便。
{% img https://cdn.jsdelivr.net/gh/0vo/oss/images/687148dbly1fo7n00rkl6j20b40b4goj.jpg 300 %}
MiniKube 官方安装介绍已经非常详细了，可以参考 installation 。但是在国内由于网络访问原因（懂的），即使有梯子也很折腾，所以记录一下阿里修改后的 MiniKube 安装。使用阿里修改后的 MiniKube 就可以从阿里云的镜像地址来获取所需 Docker 镜像和配置，其它的并没有差异，下文着重介绍。
一、kubectl 安装 MiniKube 的安装需要先安装 kubectl 及相关驱动，这没什么好说的，参考 官方介绍 。
另 kubectl 也可通过源代码编译安装，编译源码需要有 Git、Golang 环境的支撑。
➜ git clone https://github.com/kubernetes/kubernetes.git ➜ cd kubernetes ➜ make ➜ sudo cp _output/bin/kubectl /usr/local/bin/ ➜ sudo chmod +x /usr/local/bin/kubectl 二、MiniKube 安装 MiniKube 是使用 Go 语言开发的，所以安装其实很方便，一是通过下载基于不同平台早已编译好的二级制文件安装，二是可以编译源文件安装。
2.1 二级制文件安装 Mac OSX 平台 ➜ curl -Lo minikube http://kubernetes.oss-cn-hangzhou.aliyuncs.com/minikube/releases/v0.24.1/minikube-darwin-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/ Linux 平台 ➜ curl -Lo minikube http://kubernetes.oss-cn-hangzhou.aliyuncs.com/minikube/releases/v0.24.1/minikube-linux-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/ Windows 平台 下载 minikube-windows-amd64.exe 文件，并重命名为 minikube.exe，然后加入到环境变量路径下即可。
2.2 源码编译安装 编译源码需要有 Git、Golang 环境的支撑。
➜ git clone https://github.com/AliyunContainerService/minikube ➜ cd minikube ➜ git checkout aliyun-v0.24.1 ➜ make ➜ sudo cp out/minikube /usr/local/bin/ ➜ sudo chmod +x /usr/local/bin/minikube 示例版本是 v0.24.1，可更改为 其它版本 。
三、简单使用 3.1 启动 默认启动使用的是 VirtualBox 驱动，使用 --vm-driver 参数可以指定其它驱动。
# 设置 docker 加速镜像 ➜ minikube start --registry-mirror=https://registry.docker-cn.com Starting local Kubernetes v1.8.0 cluster... Starting VM... Getting VM IP address... Moving files into cluster... Setting up certs... Connecting to cluster... Setting up kubeconfig... Starting cluster components... Kubectl is now configured to use the cluster. Loading cached images from config file. 3.2 检测状态 ➜ minikube status minikube: Running cluster: Running kubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.100 3.3 启动 kubernetes dashboard ➜ minikube dashboard Opening kubernetes dashboard in default browser... 输入以上命令，浏览器中应该就会显示以下界面：
3.4 测试 运行一个 nginx 的 pod ➜ kubectl run hello --image=nginx --port=80 deployment &#34;hello&#34; created 导出运行的 nginx 服务 ➜ kubectl expose deployment hello --type=NodePort service &#34;hello&#34; exposed 查看一下运行情况，需要等一会才会显示状态为 Running ➜ kubectl get pod NAME READY STATUS RESTARTS AGE hello-6dbbbb95d-4cqwz 1/1 Running 0 2m curl 访问测试 ➜ curl $(minikube service hello --url) &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Welcome to nginx!&lt;/title&gt; &lt;style&gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Welcome to nginx!&lt;/h1&gt; &lt;p&gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.&lt;/p&gt; &lt;p&gt;For online documentation and support please refer to &lt;a href=&#34;http://nginx.org/&#34;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt; Commercial support is available at &lt;a href=&#34;http://nginx.com/&#34;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; 3.5 其它 # 查看集群的所有资源 ➜ kubectl get all ➜ kubectl get all -o wide # 进入节点服务器 ➜ minikube ssh # 执行节点服务器命令，例如查看节点 docker info ➜ minikube ssh -- docker info # 删除集群 ➜ minikube delete # 关闭集群 ➜ minikube stop 参考文献 Minikube - Kubernetes本地实验环境 ]]></content></entry><entry><title>又是一年岁末时...</title><url>/2017/12/31/summary-in-2017/</url><categories><category>年末总结</category></categories><tags><tag>年末总结</tag></tags><content type="html"><![CDATA[2017 年最后一天了，北京的天气很好，阳光明媚，关键是没有雾霾。说起雾霾，今年北京可是&quot;治理&quot;的很有效果，截止目前雾霾天和去年的好天气一样多，买了个空气净化器还没怎么派上用场，哈哈。
步入了婚姻的殿堂，享受幸福的同是多了一份责任。进入人生下一个阶段，需要处理的关系更多，新的一年需要勉励自己不断前行&hellip;
奋斗在一线的程序猿，到了一定的阶段，就不得不审视一下自己的未来。作为碌碌无为低端程序猿的我，此刻对未来感到深深的迷茫。少了年少时的拼劲，多了肩膀上的重担，不知作为下一个低端人群的我在北京还能拼搏多久。
更为可悲的是，每当危机到来的时候才发觉自己已经快退化的让自己害怕了，偶尔看到一句话：“六年开发经验？还是学习了一年，重复工作了五年”，值得深思。
2017 已经逝去，2018 任道而重远&hellip;
]]></content></entry><entry><title>Go 常用代码块</title><url>/2017/12/06/go-commons-code-snippets/</url><categories><category>Golang</category></categories><tags><tag>Golang</tag><tag>Go</tag></tags><content type="html"><![CDATA[ 总结备忘一下常用的的 Go 代码片段
遍历目录下的文件 func getFilelist(r string) { err := filepath.Walk(r, func(p string, f os.FileInfo, err error) error { if f == nil { return nil } if p == r || f.IsDir() { return nil } fmt.Println(p) return nil }) if err != nil { fmt.Printf(&#34;filepath.Walk() returned %v\n&#34;, err) } } 定时器 duration := 2 * time.Second timer := time.NewTimer(duration) go func() { for { select { case &lt;-timer.C: fmt.Println(&#34;here&#34;) timer.Reset(duration) } } }() 定时执行任务 package main import ( &#34;fmt&#34; &#34;time&#34; ) const INTERVAL_PERIOD time.Duration = 24 * time.Hour const HOUR_TO_TICK int = 23 const MINUTE_TO_TICK int = 00 const SECOND_TO_TICK int = 03 func main() { ticker := updateTicker() for { &lt;-ticker.C fmt.Println(time.Now(), &#34;- just ticked&#34;) ticker = updateTicker() } } func updateTicker() *time.Ticker { n := time.Date(time.Now().Year(), time.Now().Month(), time.Now().Day(), HOUR_TO_TICK, MINUTE_TO_TICK, SECOND_TO_TICK, 0, time.Local) if !n.After(time.Now()) { n = n.Add(INTERVAL_PERIOD) } fmt.Println(n, &#34;- next tick&#34;) diff := n.Sub(time.Now()) return time.NewTicker(diff) } 读取文件 f, err := os.Open(&#34;/test.txt&#34;) if err != nil { panic(err) } defer f.Close() fd, err := ioutil.ReadAll(f) if err != nil { panic(err) } fmt.Println(string(fd)) map 转 json m := make(map[string]interface{}) m[&#34;show&#34;] = &#34;1&#34; m[&#34;content&#34;] = &#34;test&#34; j, err := json.Marshal(m) if err != nil { panic(err) } fmt.Println(string(j)) json 转 map var result = []byte(` { &#34;show&#34;: 1, &#34;content&#34;: &#34;test&#34; }`) var r map[string]interface{} if err := json.Unmarshal(result, &amp;r); err != nil { panic(err) } fmt.Println(r) 写文件 func writeFile(path string, b []byte) { file, err := os.OpenFile(path, os.O_WRONLY|os.O_TRUNC|os.O_CREATE, 0777) defer file.Close() if err != nil { panic(err) } file.Write(b) } 简单的获取 IP func GetPulicIP() string { conn, _ := net.Dial(&#34;udp&#34;, &#34;8.8.8.8:80&#34;) defer conn.Close() localAddr := conn.LocalAddr().String() idx := strings.LastIndex(localAddr, &#34;:&#34;) return localAddr[0:idx] } ]]></content></entry><entry><title>[转]Tmux 快捷键速查表</title><url>/2017/10/25/tmux-cheat-sheet/</url><categories><category>开发工具</category></categories><tags><tag>Tmux</tag></tags><content type="html"><![CDATA[Tmux 快捷键 &amp; 速查表 启动新会话：
tmux [new -s 会话名 -n 窗口名] 恢复会话：
tmux at [-t 会话名] 列出所有会话：
tmux ls 关闭会话：
tmux kill-session -t 会话名 关闭所有会话：
tmux ls | grep : | cut -d. -f1 | awk '{print substr($1, 0, length($1)-1)}' | xargs kill 在 Tmux 中，按下 Tmux 前缀 ctrl+b，然后： 会话 :new&lt;回车&gt; 启动新会话 s 列出所有会话 $ 重命名当前会话 窗口 (标签页) c 创建新窗口 w 列出所有窗口 n 后一个窗口 p 前一个窗口 f 查找窗口 , 重命名当前窗口 &amp; 关闭当前窗口 调整窗口排序 swap-window -s 3 -t 1 交换 3 号和 1 号窗口 swap-window -t 1 交换当前和 1 号窗口 move-window -t 1 移动当前窗口到 1 号 窗格（分割窗口） % 垂直分割 &quot; 水平分割 o 交换窗格 x 关闭窗格 ⍽ 左边这个符号代表空格键 - 切换布局 q 显示每个窗格是第几个，当数字出现的时候按数字几就选中第几个窗格 { 与上一个窗格交换位置 } 与下一个窗格交换位置 z 切换窗格最大化/最小化 同步窗格 这么做可以切换到想要的窗口，输入 Tmux 前缀和一个冒号呼出命令提示行，然后输入：
:setw synchronize-panes 你可以指定开或关，否则重复执行命令会在两者间切换。 这个选项值针对某个窗口有效，不会影响别的会话和窗口。 完事儿之后再次执行命令来关闭。 帮助 调整窗格尺寸 如果你不喜欢默认布局，可以重调窗格的尺寸。虽然这很容易实现，但一般不需要这么干。这几个命令用来调整窗格：
PREFIX : resize-pane -D 当前窗格向下扩大 1 格 PREFIX : resize-pane -U 当前窗格向上扩大 1 格 PREFIX : resize-pane -L 当前窗格向左扩大 1 格 PREFIX : resize-pane -R 当前窗格向右扩大 1 格 PREFIX : resize-pane -D 20 当前窗格向下扩大 20 格 PREFIX : resize-pane -t 2 -L 20 编号为 2 的窗格向左扩大 20 格 文本复制模式： 按下**前缀 [**进入文本复制模式。可以使用方向键在屏幕中移动光标。默认情况下，方向键是启用的。在配置文件中启用 Vim 键盘布局来切换窗口、调整窗格大小。Tmux 也支持 Vi 模式。要是想启用 Vi 模式，只需要把下面这一行添加到 .tmux.conf 中：
setw -g mode-keys vi 启用这条配置后，就可以使用 h、j、k、l 来移动光标了。
想要退出文本复制模式的话，按下回车键就可以了。一次移动一格效率低下，在 Vi 模式启用的情况下，可以辅助一些别的快捷键高效工作。
例如，可以使用 w 键逐词移动，使用 b 键逐词回退。使用 f 键加上任意字符跳转到当前行第一次出现该字符的位置，使用 F 键达到相反的效果。
vi emacs 功能 ^ M-m 反缩进 Escape C-g 清除选定内容 Enter M-w 复制选定内容 j Down 光标下移 h Left 光标左移 l Right 光标右移 L 光标移到尾行 M M-r 光标移到中间行 H M-R 光标移到首行 k Up 光标上移 d C-u 删除整行 D C-k 删除到行末 $ C-e 移到行尾 : g 前往指定行 C-d M-Down 向下滚动半屏 C-u M-Up 向上滚动半屏 C-f Page down 下一页 w M-f 下一个词 p C-y 粘贴 C-b Page up 上一页 b M-b 上一个词 q Escape 退出 C-Down or J C-Down 向下翻 C-Up or K C-Up 向下翻 n n 继续搜索 ? C-r 向前搜索 / C-s 向后搜索 0 C-a 移到行首 Space C-Space 开始选中 C-t 字符调序 杂项： d 退出 tmux（tmux 仍在后台运行） t 窗口中央显示一个数字时钟 ? 列出所有快捷键 : 命令提示符 配置选项： # 鼠标支持 - 设置为 on 来启用鼠标 * setw -g mode-mouse off * set -g mouse-select-pane off * set -g mouse-resize-pane off * set -g mouse-select-window off # 设置默认终端模式为 256color set -g default-terminal &quot;screen-256color&quot; # 启用活动警告 setw -g monitor-activity on set -g visual-activity on # 居中窗口列表 set -g status-justify centre # 最大化/恢复窗格 unbind Up bind Up new-window -d -n tmp \; swap-pane -s tmp.1 \; select-window -t tmp unbind Down bind Down last-window \; swap-pane -s tmp.1 \; kill-window -t tmp 配置文件（~/.tmux.conf）： # 基础设置 set -g default-terminal &#34;screen-256color&#34; set -g display-time 3000 set -g escape-time 0 set -g history-limit 65535 set -g base-index 1 set -g pane-base-index 1 # 前缀绑定 (Ctrl+a) set -g prefix ^a unbind ^b bind a send-prefix # 分割窗口 unbind &#39;&#34;&#39; bind - splitw -v unbind % bind | splitw -h # 选中窗口 bind-key k select-pane -U bind-key j select-pane -D bind-key h select-pane -L bind-key l select-pane -R # copy-mode 将快捷键设置为 vi 模式 setw -g mode-keys vi # 启用鼠标(Tmux v2.1) set -g mouse on # 更新配置文件 bind r source-file ~/.tmux.conf \; display &#34;已更新&#34; #&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; # Tmux Plugin Manager(Tmux v2.1) # Tmux Resurrect set -g @plugin &#39;tmux-plugins/tmux-resurrect&#39; # List of plugins set -g @plugin &#39;tmux-plugins/tpm&#39; set -g @plugin &#39;tmux-plugins/tmux-sensible&#39; # Other examples: # set -g @plugin &#39;github_username/plugin_name&#39; # set -g @plugin &#39;git@github.com/user/plugin&#39; # set -g @plugin &#39;git@bitbucket.com/user/plugin&#39; # Initialize TMUX plugin manager (keep this line at the very bottom of tmux.conf) run &#39;~/.tmux/plugins/tpm/tpm&#39; #&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 原文地址 ]]></content></entry><entry><title>[转] Go Cheat Sheet</title><url>/2017/08/24/golang-cheat-sheet/</url><categories><category>Golang</category></categories><tags><tag>Golang</tag><tag>Go</tag></tags><content type="html"><![CDATA[Go Cheat Sheet Credits Most example code taken from A Tour of Go , which is an excellent introduction to Go. If you&rsquo;re new to Go, do that tour. Seriously.
Go in a Nutshell Imperative language Statically typed Syntax tokens similar to C (but less parentheses and no semicolons) and the structure to Oberon-2 Compiles to native code (no JVM) No classes, but structs with methods Interfaces No implementation inheritance. There&rsquo;s type embedding , though. Functions are first class citizens Functions can return multiple values Has closures Pointers, but not pointer arithmetic Built-in concurrency primitives: Goroutines and Channels Basic Syntax Hello World File hello.go:
package main import &#34;fmt&#34; func main() { fmt.Println(&#34;Hello Go&#34;) } $ go run hello.go
Operators Arithmetic Operator Description + addition - subtraction * multiplication / quotient % remainder &amp; bitwise and ? bitwise or ^ bitwise xor &amp;^ bit clear (and not) &lt;&lt; left shift &gt;&gt; right shift ? -&gt; |
Comparison Operator Description == equal != not equal &lt; less than &lt;= less than or equal &gt; greater than &gt;= greater than or equal Logical Operator Description &amp;&amp; logical and ?? logical or ! logical not ?? -&gt; ||
Other Operator Description &amp; address of / create pointer * dereference pointer &lt;- send / receive operator (see &lsquo;Channels&rsquo; below) Declarations Type goes after identifier!
var foo int // declaration without initialization var foo int = 42 // declaration with initialization var foo, bar int = 42, 1302 // declare and init multiple vars at once var foo = 42 // type omitted, will be inferred foo := 42 // shorthand, only in func bodies, omit var keyword, type is always implicit const constant = &#34;This is a constant&#34; Functions // a simple function func functionName() {} // function with parameters (again, types go after identifiers) func functionName(param1 string, param2 int) {} // multiple parameters of the same type func functionName(param1, param2 int) {} // return type declaration func functionName() int { return 42 } // Can return multiple values at once func returnMulti() (int, string) { return 42, &#34;foobar&#34; } var x, str = returnMulti() // Return multiple named results simply by return func returnMulti2() (n int, s string) { n = 42 s = &#34;foobar&#34; // n and s will be returned return } var x, str = returnMulti2() Functions As Values And Closures func main() { // assign a function to a name add := func(a, b int) int { return a + b } // use the name to call the function fmt.Println(add(3, 4)) } // Closures, lexically scoped: Functions can access values that were // in scope when defining the function func scope() func() int{ outer_var := 2 foo := func() int { return outer_var} return foo } func another_scope() func() int{ // won&#39;t compile because outer_var and foo not defined in this scope outer_var = 444 return foo } // Closures: don&#39;t mutate outer vars, instead redefine them! func outer() (func() int, int) { outer_var := 2 inner := func() int { outer_var += 99 // attempt to mutate outer_var from outer scope return outer_var // =&gt; 101 (but outer_var is a newly redefined // variable visible only inside inner) } return inner, outer_var // =&gt; 101, 2 (outer_var is still 2, not mutated by foo!) } Variadic Functions func main() { fmt.Println(adder(1, 2, 3)) // 6 fmt.Println(adder(9, 9))	// 18 nums := []int{10, 20, 30} fmt.Println(adder(nums...))	// 60 } // By using ... before the type name of the last parameter you can indicate that it takes zero or more of those parameters. // The function is invoked like any other function except we can pass as many arguments as we want. func adder(args ...int) int { total := 0 for _, v := range args { // Iterates over the arguments whatever the number. total += v } return total } Built-in Types bool string int int8 int16 int32 int64 uint uint8 uint16 uint32 uint64 uintptr byte // alias for uint8 rune // alias for int32 ~= a character (Unicode code point) - very Viking float32 float64 complex64 complex128 Type Conversions var i int = 42 var f float64 = float64(i) var u uint = uint(f) // alternative syntax i := 42 f := float64(i) u := uint(f) Packages Package declaration at top of every source file Executables are in package main Convention: package name == last name of import path (import path math/rand =&gt; package rand) Upper case identifier: exported (visible from other packages) Lower case identifier: private (not visible from other packages) Control structures If func main() { // Basic one if x &gt; 0 { return x } else { return -x } // You can put one statement before the condition if a := b + c; a &lt; 42 { return a } else { return a - 42 } // Type assertion inside if var val interface{} val = &#34;foo&#34; if str, ok := val.(string); ok { fmt.Println(str) } } Loops // There&#39;s only `for`, no `while`, no `until` for i := 1; i &lt; 10; i++ { } for ; i &lt; 10; { // while - loop } for i &lt; 10 { // you can omit semicolons if there is only a condition } for { // you can omit the condition ~ while (true) } Switch // switch statement switch operatingSystem { case &#34;darwin&#34;: fmt.Println(&#34;Mac OS Hipster&#34;) // cases break automatically, no fallthrough by default case &#34;linux&#34;: fmt.Println(&#34;Linux Geek&#34;) default: // Windows, BSD, ... fmt.Println(&#34;Other&#34;) } // as with for and if, you can have an assignment statement before the switch value switch os := runtime.GOOS; os { case &#34;darwin&#34;: ... } // you can also make comparisons in switch cases number := 42 switch { case number &lt; 42: fmt.Println(&#34;Smaller&#34;) case number == 42: fmt.Println(&#34;Equal&#34;) case number &gt; 42: fmt.Println(&#34;Greater&#34;) } Arrays, Slices, Ranges Arrays var a [10]int // declare an int array with length 10. Array length is part of the type! a[3] = 42 // set elements i := a[3] // read elements // declare and initialize var a = [2]int{1, 2} a := [2]int{1, 2} //shorthand a := [...]int{1, 2} // elipsis -&gt; Compiler figures out array length Slices var a []int // declare a slice - similar to an array, but length is unspecified var a = []int {1, 2, 3, 4} // declare and initialize a slice (backed by the array given implicitly) a := []int{1, 2, 3, 4} // shorthand chars := []string{0:&#34;a&#34;, 2:&#34;c&#34;, 1: &#34;b&#34;} // [&#34;a&#34;, &#34;b&#34;, &#34;c&#34;] var b = a[lo:hi]	// creates a slice (view of the array) from index lo to hi-1 var b = a[1:4]	// slice from index 1 to 3 var b = a[:3]	// missing low index implies 0 var b = a[3:]	// missing high index implies len(a) a = append(a,17,3)	// append items to slice a c := append(a,b...)	// concatenate slices a and b // create a slice with make a = make([]byte, 5, 5)	// first arg length, second capacity a = make([]byte, 5)	// capacity is optional // create a slice from an array x := [3]string{&#34;Лайка&#34;, &#34;Белка&#34;, &#34;Стрелка&#34;} s := x[:] // a slice referencing the storage of x Operations on Arrays and Slices len(a) gives you the length of an array/a slice. It&rsquo;s a built-in function, not a attribute/method on the array.
// loop over an array/a slice for i, e := range a { // i is the index, e the element } // if you only need e: for _, e := range a { // e is the element } // ...and if you only need the index for i := range a { } // In Go pre-1.4, you&#39;ll get a compiler error if you&#39;re not using i and e. // Go 1.4 introduced a variable-free form, so that you can do this for range time.Tick(time.Second) { // do it once a sec } Maps var m map[string]int m = make(map[string]int) m[&#34;key&#34;] = 42 fmt.Println(m[&#34;key&#34;]) delete(m, &#34;key&#34;) elem, ok := m[&#34;key&#34;] // test if key &#34;key&#34; is present and retrieve it, if so // map literal var m = map[string]Vertex{ &#34;Bell Labs&#34;: {40.68433, -74.39967}, &#34;Google&#34;: {37.42202, -122.08408}, } Structs There are no classes, only structs. Structs can have methods.
// A struct is a type. It&#39;s also a collection of fields // Declaration type Vertex struct { X, Y int } // Creating var v = Vertex{1, 2} var v = Vertex{X: 1, Y: 2} // Creates a struct by defining values with keys var v = []Vertex{{1,2},{5,2},{5,5}} // Initialize a slice of structs // Accessing members v.X = 4 // You can declare methods on structs. The struct you want to declare the // method on (the receiving type) comes between the the func keyword and // the method name. The struct is copied on each method call(!) func (v Vertex) Abs() float64 { return math.Sqrt(v.X*v.X + v.Y*v.Y) } // Call method v.Abs() // For mutating methods, you need to use a pointer (see below) to the Struct // as the type. With this, the struct value is not copied for the method call. func (v *Vertex) add(n float64) { v.X += n v.Y += n } Anonymous structs: Cheaper and safer than using map[string]interface{}.
point := struct { X, Y int }{1, 2} Pointers p := Vertex{1, 2} // p is a Vertex q := &amp;p // q is a pointer to a Vertex r := &amp;Vertex{1, 2} // r is also a pointer to a Vertex // The type of a pointer to a Vertex is *Vertex var s *Vertex = new(Vertex) // new creates a pointer to a new struct instance Interfaces // interface declaration type Awesomizer interface { Awesomize() string } // types do *not* declare to implement interfaces type Foo struct {} // instead, types implicitly satisfy an interface if they implement all required methods func (foo Foo) Awesomize() string { return &#34;Awesome!&#34; } Embedding There is no subclassing in Go. Instead, there is interface and struct embedding.
// ReadWriter implementations must satisfy both Reader and Writer type ReadWriter interface { Reader Writer } // Server exposes all the methods that Logger has type Server struct { Host string Port int *log.Logger } // initialize the embedded type the usual way server := &amp;Server{&#34;localhost&#34;, 80, log.New(...)} // methods implemented on the embedded struct are passed through server.Log(...) // calls server.Logger.Log(...) // the field name of the embedded type is its type name (in this case Logger) var logger *log.Logger = server.Logger Errors There is no exception handling. Functions that might produce an error just declare an additional return value of type Error. This is the Error interface:
type error interface { Error() string } A function that might return an error:
func doStuff() (int, error) { } func main() { result, error := doStuff() if (error != nil) { // handle error } else { // all is good, use result } } Concurrency Goroutines Goroutines are lightweight threads (managed by Go, not OS threads). go f(a, b) starts a new goroutine which runs f (given f is a function).
// just a function (which can be later started as a goroutine) func doStuff(s string) { } func main() { // using a named function in a goroutine go doStuff(&#34;foobar&#34;) // using an anonymous inner function in a goroutine go func (x int) { // function body goes here }(42) } Channels ch := make(chan int) // create a channel of type int ch &lt;- 42 // Send a value to the channel ch. v := &lt;-ch // Receive a value from ch // Non-buffered channels block. Read blocks when no value is available, write blocks if a value already has been written but not read. // Create a buffered channel. Writing to a buffered channels does not block if less than &lt;buffer size&gt; unread values have been written. ch := make(chan int, 100) close(ch) // closes the channel (only sender should close) // read from channel and test if it has been closed v, ok := &lt;-ch // if ok is false, channel has been closed // Read from channel until it is closed for i := range ch { fmt.Println(i) } // select blocks on multiple channel operations, if one unblocks, the corresponding case is executed func doStuff(channelOut, channelIn chan int) { select { case channelOut &lt;- 42: fmt.Println(&#34;We could write to channelOut!&#34;) case x := &lt;- channelIn: fmt.Println(&#34;We could read from channelIn&#34;) case &lt;-time.After(time.Second * 1): fmt.Println(&#34;timeout&#34;) } } Channel Axioms A send to a nil channel blocks forever
var c chan string c &lt;- &#34;Hello, World!&#34; // fatal error: all goroutines are asleep - deadlock! A receive from a nil channel blocks forever
var c chan string fmt.Println(&lt;-c) // fatal error: all goroutines are asleep - deadlock! A send to a closed channel panics
var c = make(chan string, 1) c &lt;- &#34;Hello, World!&#34; close(c) c &lt;- &#34;Hello, Panic!&#34; // panic: send on closed channel A receive from a closed channel returns the zero value immediately
var c = make(chan int, 2) c &lt;- 1 c &lt;- 2 close(c) for i := 0; i &lt; 3; i++ { fmt.Printf(&#34;%d &#34;, &lt;-c) } // 1 2 0 Printing fmt.Println(&#34;Hello, 你好, नमस्ते, Привет, ᎣᏏᏲ&#34;) // basic print, plus newline p := struct { X, Y int }{ 17, 2 } fmt.Println( &#34;My point:&#34;, p, &#34;x coord=&#34;, p.X ) // print structs, ints, etc s := fmt.Sprintln( &#34;My point:&#34;, p, &#34;x coord=&#34;, p.X ) // print to string variable fmt.Printf(&#34;%d hex:%x bin:%b fp:%f sci:%e&#34;,17,17,17,17.0,17.0) // c-ish format s2 := fmt.Sprintf( &#34;%d %f&#34;, 17, 17.0 ) // formatted print to string variable hellomsg := ` &#34;Hello&#34; in Chinese is 你好 (&#39;Ni Hao&#39;) &#34;Hello&#34; in Hindi is नमस्ते (&#39;Namaste&#39;) ` // multi-line string literal, using back-tick at beginning and end Snippets HTTP Server package main import ( &#34;fmt&#34; &#34;net/http&#34; ) // define a type for the response type Hello struct{} // let that type implement the ServeHTTP method (defined in interface http.Handler) func (h Hello) ServeHTTP(w http.ResponseWriter, r *http.Request) { fmt.Fprint(w, &#34;Hello!&#34;) } func main() { var h Hello http.ListenAndServe(&#34;localhost:4000&#34;, h) } // Here&#39;s the method signature of http.ServeHTTP: // type Handler interface { // ServeHTTP(w http.ResponseWriter, r *http.Request) // } 原文出处 ]]></content></entry><entry><title>使用 PowerMock 进行单元测试</title><url>/2017/07/25/use-introduction-of-powermock/</url><categories><category>Java开发技术</category></categories><tags><tag>Java</tag><tag>Mock</tag><tag>Mockito</tag><tag>PowerMock</tag></tags><content type="html"><![CDATA[ 单元测试（Unit Testing）又称为模块测试，是针对程序模块（软件设计的最小单位）来进行正确性检验的测试工作。如果我们写的代码依赖于某些模块对象，而单元测试过程中这些对象又很难手动创建，或者模块还没有开发完成，那么就使用一个虚拟的对象来完成单元测试，这就是所谓的 Mock。
Java 单元测试中比较流行的 Mock 测试框架有 jMock 、 EasyMock 、 Mockito ，但是这些 Mock 工具都不能 Mock static、final、private 方法等，而 PowerMock 能够做到。
使用 PowerMock，首先需要使用 @RunWith(PowerMockRunner.class) 将测试用例的 Runner 改为 PowerMockRunner。如果要 Mock static、final、private 等方法的时候，就需要加注解 @PrepareForTest。
PowerMock 有两个版本，一个是基于 EasyMock 实现的，另一个是基于 Mockito 实现的。
下面我将以 PowerMock 的 Mockito 的版本来讲述如何使用 PowerMock。
1. 普通 Mock（Mock 参数传递的对象） 测试对象
public class ClassUnderTest { public boolean callArgumentInstance(File file) { return file.exists(); } } 测试用例
public class TestClassUnderTest { @Test public void testCallArgumentInstance() { // Mock 对象，也可以使用 org.mockito.Mock 注解标记来实现 File file = PowerMockito.mock(File.class); ClassUnderTest underTest = new ClassUnderTest(); // 录制 Mock 对象行为 PowerMockito.when(file.exists()).thenReturn(true); // 验证方法行为 Assert.assertTrue(underTest.callArgumentInstance(file)); } } 普通 Mock 不需要加 @RunWith 和 @PrepareForTest 注解。
2. Mock 方法内部 new 出来的对象 测试对象
public class ClassUnderTest { public boolean callInternalInstance(String path) { File file = new File(path); return file.exists(); } } 测试用例
// 必须加注解 @PrepareForTest 和 @RunWith @RunWith(PowerMockRunner.class) public class TestClassUnderTest { @Test // 在测试方法之上需要添加注解 @PrepareForTest，注解里写的类是需要 Mock 的 new 对象代码所在的类。 @PrepareForTest(ClassUnderTest.class) public void testCallInternalInstance() throws Exception { File file = PowerMockito.mock(File.class); ClassUnderTest underTest = new ClassUnderTest(); // 当以参数为 bbb 创建 File 对象的时候，返回已经 Mock 的 File 对象。 PowerMockito.whenNew(File.class).withArguments(&#34;bbb&#34;).thenReturn(file); PowerMockito.when(file.exists()).thenReturn(true); Assert.assertTrue(underTest.callInternalInstance(&#34;bbb&#34;)); } } 3. Mock 普通对象的 final 方法 测试对象
public class ClassUnderTest { public boolean callFinalMethod(ClassDependency refer) { return refer.isAlive(); } } public class ClassDependency { public final boolean isAlive() { // do something return false; } } 测试用例
// 必须加注解 @PrepareForTest 和 @RunWith @RunWith(PowerMockRunner.class) public class TestClassUnderTest { @Test // 在测试方法之上加注解 @PrepareForTest，注解里写的类是需要 Mock 的 final 方法所在的类。 @PrepareForTest(ClassDependency.class) public void testCallFinalMethod() { ClassDependency depencency = PowerMockito.mock(ClassDependency.class); ClassUnderTest underTest = new ClassUnderTest(); PowerMockito.when(depencency.isAlive()).thenReturn(true); Assert.assertTrue(underTest.callFinalMethod(depencency)); } } 4. Mock 静态方法。 测试对象
public class ClassUnderTest { public boolean callStaticMethod() { return ClassDependency.isExist(); } } public class ClassDependency { public static boolean isExist() { // do something return false; } } 测试用例
// 必须加注解 @PrepareForTest 和 @RunWith @RunWith(PowerMockRunner.class) public class TestClassUnderTest { @Test // 在测试方法之上加注解 @PrepareForTest，注解里写的类是需要 Mock 的 static 方法所在的类。 @PrepareForTest(ClassDependency.class) public void testCallStaticMethod() { ClassUnderTest underTest = new ClassUnderTest(); // 表示需要 Mock 这个类里的静态方法 PowerMockito.mockStatic(ClassDependency.class); PowerMockito.when(ClassDependency.isExist()).thenReturn(true); Assert.assertTrue(underTest.callStaticMethod()); } } 5. Mock 私有方法 测试对象
public class ClassUnderTest { public boolean callPrivateMethod() { return isExist(); } private boolean isExist() { return false; } } 测试用例
// 必须加注解 @PrepareForTest 和 @RunWith @RunWith(PowerMockRunner.class) public class TestClassUnderTest { @Test // 在测试方法之上加注解 @PrepareForTest，注解里写的类是需要 Mock 的 private 方法所在的类。 @PrepareForTest(ClassUnderTest.class) public void testCallPrivateMethod() throws Exception { ClassUnderTest underTest = PowerMockito.mock(ClassUnderTest.class); PowerMockito.when(underTest.callPrivateMethod()).thenCallRealMethod(); PowerMockito.when(underTest, &#34;isExist&#34;).thenReturn(true); Assert.assertTrue(underTest.callPrivateMethod()); } } 6. Mock JDK 中类的静态、私有方法。 测试对象
public class ClassUnderTest { public boolean callSystemFinalMethod(String str) { return str.isEmpty(); } public String callSystemStaticMethod(String str) { return System.getProperty(str); } } 测试用例
// 必须加注解 @PrepareForTest 和 @RunWith @RunWith(PowerMockRunner.class) public class TestClassUnderTest @Test // 和 Mock 普通对象的 static、final 方法一样，只不过注解 @PrepareForTest 里写的类不一样 // 注解里写的类是需要调用系统方法所在的类。 @PrepareForTest(ClassUnderTest.class) public void testCallSystemFinalMethod() { String str = PowerMockito.mock(String.class); ClassUnderTest underTest = new ClassUnderTest(); PowerMockito.when(str.isEmpty()).thenReturn(false); Assert.assertFalse(underTest.callSystemFinalMethod(str)); } @Test @PrepareForTest(ClassUnderTest.class) public void testCallSystemStaticMethod() { ClassUnderTest underTest = new ClassUnderTest(); PowerMockito.mockStatic(System.class); PowerMockito.when(System.getProperty(&#34;aaa&#34;)).thenReturn(&#34;bbb&#34;); Assert.assertEquals(&#34;bbb&#34;, underTest.callSystemStaticMethod(&#34;aaa&#34;)); } } 7. Mock 依赖类中的方法（whenNew） 测试对象
public class ClassUnderTest { public boolean callDependency() { ClassDependency classDependency = new ClassDependency(); return classDependency.isGod(&#34;hh&#34;); } } public class ClassDependency { public boolean isGod(String oh){ System.out.println(oh); return false; } } 测试用例
// 必须加注解 @PrepareForTest 和 @RunWith @RunWith(PowerMockRunner.class) public class TestClassUnderTest { @Test // 注解里写的类是依赖类所在的类。 @PrepareForTest(ClassUnderTest.class) public void testDependency() throws Exception { ClassUnderTest underTest = new ClassUnderTest(); ClassDependency dependency = mock(ClassDependency.class); whenNew(ClassDependency.class).withAnyArguments().thenReturn(dependency); when(dependency.isGod(anyString())).thenReturn(true); Assert.assertTrue(underTest.callDependency()); } } 8. 完整示例代码 测试目标类 package cn.enncloud.ceres.powermock; import java.io.File; /** * Created by lixiangrong on 2017/7/21. */ public class ClassUnderTest { public boolean callArgumentInstance(File file) { return file.exists(); } public boolean callInternalInstance(String path) { File file = new File(path); return file.exists(); } public boolean callFinalMethod(ClassDependency refer) { return refer.isAlive(); } public boolean callSystemFinalMethod(String str) { return str.isEmpty(); } public boolean callStaticMethod() { return ClassDependency.isExist(); } public String callSystemStaticMethod(String str) { return System.getProperty(str); } public boolean callPrivateMethod() { return isExist(); } public boolean callVoidPrivateMethod(){ testVoid(); return true; } private boolean isExist() { // do something return false; } private void testVoid(){ System.out.println(&#34;do nothing&#34;); } public boolean callDependency() { ClassDependency classDependency = new ClassDependency(); return classDependency.isGod(&#34;hh&#34;); } } 依赖类 package cn.enncloud.ceres.powermock; /** * Created by lixiangrong on 2017/7/21. */ public class ClassDependency { public static boolean isExist() { // do something return false; } public final boolean isAlive() { // do something return false; } public boolean isGod(String oh){ System.out.println(oh); return false; } } 测试用例 package cn.enncloud.ceres.powermock.test; import cn.enncloud.ceres.powermock.ClassDependency; import cn.enncloud.ceres.powermock.ClassUnderTest; import org.junit.Assert; import org.junit.Test; import org.junit.runner.RunWith; import org.powermock.api.mockito.PowerMockito; import org.powermock.core.classloader.annotations.PrepareForTest; import org.powermock.modules.junit4.PowerMockRunner; import java.io.File; /** * Created by lixiangrong on 2017/7/21. */ @RunWith(PowerMockRunner.class) public class TestClassUnderTest { @Test public void testCallArgumentInstance() { File file = PowerMockito.mock(File.class); ClassUnderTest underTest = new ClassUnderTest(); PowerMockito.when(file.exists()).thenReturn(true); Assert.assertTrue(underTest.callArgumentInstance(file)); } @Test @PrepareForTest(ClassUnderTest.class) public void testCallInternalInstance() throws Exception { File file = PowerMockito.mock(File.class); ClassUnderTest underTest = new ClassUnderTest(); PowerMockito.whenNew(File.class).withArguments(&#34;bbb&#34;).thenReturn(file); PowerMockito.when(file.exists()).thenReturn(true); Assert.assertTrue(underTest.callInternalInstance(&#34;bbb&#34;)); } @Test @PrepareForTest(ClassDependency.class) public void testCallFinalMethod() { ClassDependency depencency = PowerMockito.mock(ClassDependency.class); ClassUnderTest underTest = new ClassUnderTest(); PowerMockito.when(depencency.isAlive()).thenReturn(true); Assert.assertTrue(underTest.callFinalMethod(depencency)); } @Test @PrepareForTest(ClassUnderTest.class) public void testCallSystemFinalMethod() { String str = PowerMockito.mock(String.class); ClassUnderTest underTest = new ClassUnderTest(); PowerMockito.when(str.isEmpty()).thenReturn(false); Assert.assertFalse(underTest.callSystemFinalMethod(str)); } @Test @PrepareForTest(ClassDependency.class) public void testCallStaticMethod() { ClassUnderTest underTest = new ClassUnderTest(); PowerMockito.mockStatic(ClassDependency.class); PowerMockito.when(ClassDependency.isExist()).thenReturn(true); Assert.assertTrue(underTest.callStaticMethod()); } @Test @PrepareForTest(ClassUnderTest.class) public void testCallSystemStaticMethod() { ClassUnderTest underTest = new ClassUnderTest(); PowerMockito.mockStatic(System.class); PowerMockito.when(System.getProperty(&#34;aaa&#34;)).thenReturn(&#34;bbb&#34;); Assert.assertEquals(&#34;bbb&#34;, underTest.callSystemStaticMethod(&#34;aaa&#34;)); } @Test @PrepareForTest(ClassUnderTest.class) public void testCallPrivateMethod() throws Exception { ClassUnderTest underTest = PowerMockito.mock(ClassUnderTest.class); PowerMockito.when(underTest.callPrivateMethod()).thenCallRealMethod(); PowerMockito.when(underTest, &#34;isExist&#34;).thenReturn(true); Assert.assertTrue(underTest.callPrivateMethod()); } @Test @PrepareForTest(ClassUnderTest.class) public void testCallVoidPrivateMethod() throws Exception { ClassUnderTest underTest = PowerMockito.mock(ClassUnderTest.class); PowerMockito.when(underTest.callVoidPrivateMethod()).thenCallRealMethod(); PowerMockito.doNothing().when(underTest, &#34;testVoid&#34;); Assert.assertTrue(underTest.callVoidPrivateMethod()); } @Test @PrepareForTest(ClassUnderTest.class) public void testDependency() throws Exception { ClassUnderTest underTest = new ClassUnderTest(); ClassDependency dependency = mock(ClassDependency.class); // @PrepareForTest(ClassUnderTest.class) whenNew(ClassDependency.class).withAnyArguments().thenReturn(dependency); when(dependency.isGod(anyString())).thenReturn(true); Assert.assertTrue(underTest.callDependency()); } } 9. Mock 与 Spy Mock 不是真实的对象，它只是用类型的 class 创建了一个虚拟对象，并可以设置对象行为 Spy 是一个真实的对象，但它可以设置对象行为
]]></content></entry><entry><title>[转] Java 程序员的 Golang 入门笔记</title><url>/2017/07/22/from-java-to-golang/</url><categories><category>Golang</category></categories><tags><tag>Golang</tag><tag>Java</tag><tag>Go</tag></tags><content type="html"><![CDATA[ 最近抽空学习了一下 Go 语言，好多特性感觉非常棒，由于高效的开发效率以及性能，现在好多优秀的开源项目都是基于 Go 开发，比如 Docker、etcd、consul、Kubernetes 等。Go 势必会在互联网技术的服务化，容器化的将来大展拳脚。正好网上看到一篇关于 Java 程序员入门 Golang 的文章，写的挺好的，所以特此转载过来，再加上自己的一些学习经验，供大家参考。
Golang 从 09 年发布，中间经历了多个版本的演进，已经渐渐趋于成熟，其媲美于 C 语言的性能、Python 的开发效率，又被称为 21 世纪的 C 语言，尤其适合开发后台服务。这篇文章主要是介绍 Golang 的一些主要特性，和 Java 做一个对比，以便更好的理解 Golang 这门语言。
关于 Golang 环境的搭建就不讲了，可以参考 官方文档 或者大神 astaxie 的开源书籍 build-web-application-with-golang 的相关篇章。下面我没让你就从 Go 版本的 Hello World 开始。
Hello World 每种语言都有自己的 Hello World，Go 也不例外，Go 版本的如下：
package main import ( &#34;fmt&#34; ) func main() { fmt.Println(&#34;Hello World!你好，世界！&#34;) } 我们使用 go run 运行后，会在控制台终端看到 Hello World!你好，世界！ 的输出。我们来看下这段代码：
package 是一个关键字，定义一个包，和 Java 里的 package 一样，也是模块化的关键。 main 包是一个特殊的包名，它表示当前是一个可执行程序，而不是一个库。 import 也是一个关键字，表示要引入的包，和 Java 的 import 关键字一样，引入后才可以使用它。 fmt 是一个包名，这里表示要引入 fmt 这个包，这样我们就可以使用它的函数了。 main 函数是主函数，表示程序执行的入口，Java 也有同名函数，但是多了一个String[] 类型的参数。 Println 是 fmt 包里的函数，和 Java 里的 System.out.println 作用类似，这里输出一段文字。 整段代码非常简洁，关键字、函数、包等和 Java 非常相似，不过注意，go 是不需要以 ; (分号)结尾的。
变量 go 语言变量的声明和 java 的略有不同，以声明一个 int 类型，变量名为 age 为例，go 语言变量生成如下：
var age int =10 同样的变量，在 java 中的声明是：
int age = 10; 可以看到 go 的变量声明，修饰变量的类型在变量的后面，而且是以 var 关键字开头。
var 变量名 类型 = 表达式 最后面的赋值可以在声明的时候忽略，这样变量就有一个默认的值，称之为 零值。零值 是一个统称，以类型而定，比如 int 类型的零值为 0，string 类型的零值是 ”” 空字符串。
在 go 中除了以 var 声明变量之外，还有一种简短的变量声明方式 :=,比如上面例子，可以如下简单声明：
age := 10 这种方式和上面的例子等价，但是少了 var 和变量类型，所以简短方便，用的多。使用这种方式，变量的类型由 go 根据值推导出来，比如这里默认是 int。
不过它有一个限制，那就是它只能用在函数内部；在函数外部使用则会无法编译通过，所以一般用 var 方式来定义全局变量。
var a1, a2 string = &#34;1&#34;, &#34;d&#34; Go 对于已声明但未使用的变量（局部变量）会在编译阶段报错
常量 有了变量，就少不了常量，和 var 关键字不一样，go 的常量使用 const 声明，这个和 C 里的常量一样。
const age = 10 这样就声明了一个常量 age，其值是 10，因为我们这里没有指定常量的类型，所以常量的类型是根据值推导出来的。所以等价的我们也可以指定常量类型，如下：
const age int = 10 相比来说，java 下的常量定义就要复杂一些，要有 static final 修饰符，才是常量：
private static final int AGE = 10; 这个和 go 的实现等价，但是它的定义修饰符比 go 多多了，而且常量类型不能省略。
大小写标记访问权限 我们上面的 go 例子中我特意用了小些的变量名 age，甚至常量我也没有写成 AGE，但是在 java 中，对于常量我们的习惯是全部大些。
在 go 中不能随便使用大小写的问题，是因为大小写具有特殊意义，在 go 中，大些字母开头的变量或者函数等是 public 的，可以被其他包访问；小些的则是private的，不能被其他包访问到。这样就省去了 public 和 private 声明的烦恼，使代码变的更简洁。
特别说明，这些导出规则只适用于包级别名字定义，不能使函数内部的定义。
包 包的规则和java很像，每个包都有自己独立的空间，所以可以用来做模块化，封装，组织代码等。 和java不同的是， go 的包里可以有函数，比如我们常用的fmt.Println(),但是在在java中没有这种用法，java的方法必须是属于一个类或者类的实例的。
要使用一个包，就需要先导入，使用import关键字，和java也一样，可以参见前面的hello world示例。
如果我们需要导入多个包的时候，可以像java一样，一行行导入，也可以使用快捷方式一次导入，这个是java所没有的。
import ( &#34;io&#34; &#34;log&#34; &#34;net&#34; &#34;strconv&#34; ) 类型转换 go 对于变量的类型有严格的限制，不同类型之间的变量不能进行赋值、表达式等操作，必须要要转换成同一类型才可以，比如int32和int64两种int类型的变量不能直接相加，要转换成一样才可以。
var a int32 = 13 var b int64 = 20 c := int64(a) + b 这种限制主要是防止我们误操作，导致一些莫名其妙的问题。在java中因为有自动转型的概念，所以可以不同类型的可以进行操作，比如int可以和double相加，int类型可以通过+和字符串拼接起来，这些在go中都是不可行的。
map map类型，Java里是Map接口， go 里叫做字典，因为其常用，在 go 中，被优化为一个语言上支持的结构，原生支持，就像一个关键字一样，而不是java里的要使用内置的sdk集合库，比如HashMap等。
ages := make(map[string]int) ages[&#34;linday&#34;] = 20 ages[&#34;michael&#34;] = 30 fmt.Print(ages[&#34;michael&#34;]) go 里要创建一个map对应，需要使用关键字make，然后就可以对这个map进行操作。
map的结构也非常简单，符合KV模型，定义为map[key]value, 方括号里是key的类型，方括号外紧跟着对应的value的类型，这些明显和Java的Map接口不同。如果在 go 中我们要删除map中的一个元素怎么办？使用内置的delete函数就可以,如下代码删除ages这个map中，key为michael的元素。
delete(ages,&#34;michael&#34;) 如果我们想遍历map中的K、V值怎么办？答案是使用range风格的for循环，可比Java Map的遍历简洁多了。
for name,age := range ages { fmt.Println(&#34;name:&#34;,name,&#34;,age:&#34;,age) } range一个map，会返回两个值，第一个是key，第二个是value，这个也是go多值返回的优势，下面会讲。
函数方法 在 go 中，函数和方法是不一样的，我们一般称包级别的(直接可以通过包调用的)称之为函数，比如fmt.Println()；把和一个类型关联起来的函数称之为方法，如下示例：
package lib import &#34;time&#34; type Person struct { age int name string } func (p Person) GetName() string { return p.name } func GetTime() time.Time{ return time.Now() } 其中GetTime()可以通过lib.GetTime()直接调用，称之为函数；而GetName()则属于Person这个结构体的函数，只能声明了Person类型的实例后才可以调用，称之为方法。
不管是函数还是方法，定义是一摸一样的。而在这里，最可以讲的就是多值返回，也就是可以同时返回多个值，这就大大为我们带来了方便，比如上个遍历map的例子，直接可以获取K、V，如果只能返回一个值，我们就需要调用两次方法才可以。
func GetTime() (time.Time,error){ return time.Now(),nil } 多值返回也很简单，返回的值使用逗号隔开即可。如果要接受多值的返回，也需要以逗号分隔的变量，有几个返回值，就需要几个变量，比如这里：
now,err:=GetTime() 如果有个返回值，我们用不到，不想浪费一个变量接收怎么办？这时候可以使用空标志符_,这是java没有的。
now,_:=GetTime() 指针 go 的指针和C中的声明定义是一样的，其作用类似于Java引用变量效果。
var age int = 10 var p *int = &amp;age *p = 11 fmt.Println(age) 其中指针p指向变量age的内存地址，如果修改*p的值，那么变量age的值也同时会被修改，例子中打印出来的值为11，而不是10.
相对应java引用类型的变量，可以理解为一个HashMap类型的变量，这个变量传递给一个方法，在该方法里对HashMap修改，删除，就会影响原来的HashMap。引用变量集合类最容易理解，自己的类也可以，不过基本类型不行，基本类型不是引用类型的，他们在方法传参的时候，是拷贝的值。
结构体替代类 go 中没有类型的概念，只有结构体，这个和C是一样的。
type Person struct { age int name string } go 中的结构体是不能定义方法的，只能是变量，这点和Java不一样的,如果要访问结构体内的成员变量，通过.操作符即可。
func (p Person) GetName() string { return p.name } 这就是通过.操作符访问变量的方式，同时它也是一个为结构体定义方法的例子，和函数不一样的是，在func关键字后要执行该方法的接收者，这个方法就是属于这个接收者，例子中是Person这个结构体。
在 go 中如果想像Java一样，让一个结构体继承另外一个结构体怎么办？也有办法，不过在 go 中称之为组合或者嵌入。
type Person struct { age int name string Address } type Address struct { city string } 结构体Address被嵌入了Person中，这样Person就拥有了Address的变量和方法，就想自己的一样，这就是组合的威力。通过这种方式，我们可以把简单的对象组合成复杂的对象，并且他们之间没有强约束关系， go 倡导的是组合，而不是继承、多态。
接口 go 的接口和Java类型，不过它不需要强制实现，在 go 中，如果你这个类型（基本类型，结构体等都可以）拥有了接口的所有方法，那么就默认为这个类型实现了这个接口，是隐式的，不需要和java一样，强制使用implement强制实现。
type Stringer interface { String() string } func (p Person) String() string { return &#34;name is &#34;+p.name+&#34;,age is &#34;+strconv.Itoa(p.age) } 以上实例中可以看到，Person这个结构体拥有了fmt.Stringer接口的方法，那么就说明Person实现了fmt.Stringer接口。
接口也可以像结构体一样组合嵌套，这里不再赘述。
并发 go 并发主要靠goroutine支持，也称之为go协程或者go程，他是语言层面支持的，非常轻量级的多任务支持，也可以把他简单的理解为java语言的线程，不过是不一样的。
go run() 这就启动一个goroutine来执行run函数，代码非常简洁，如果在java中，需要先New一个Thread，然后在重写他的run方法，然后在start才可以开始。
两个goroutine可以通过channel来通信，channel是一个特殊的类型，也是 go 语言级别上的支持，他类似于一个管道，可以存储信息，也可以从中读取信息。
package main import &#34;fmt&#34; func main() { result:=make(chan int) go func() { sum:=0 for i:=0;i&lt;10;i++{ sum=sum+i } result&lt;-sum }() fmt.Print(&lt;-result) } 以上示例使用一个单独的goroutine求和，当得到结果时，存放在result这个chan里，然后供 main goroutine 读取出来。当result没有被存储值的时候，读取result是阻塞的，所以会等到结果返回，协同工作，通过chan通信。
对于并发， go 还提供了一套同步机制，都在sync包里，有锁，有一些常用的工具函数等，和java的concurrent框架差不多。
异常机制 相比java的Exception来说， go 有两种机制，不过最常用的还是error错误类型，panic只用于严重的错误。
type error interface { Error() string } go 内置的error类型非常简洁，只用实现Error方法即可，可以打印一些详细的错误信息，比如常见的函数多值返回，最后一个返回值经常是error，用于传递一些错误问题，这种方式要比java throw Exception的方法更优雅。
Defer代替finally go 中没有java的finally了，那么如果我们要关闭一些一些连接，文件流等怎么办呢，为此go为我们提供了defer关键字，这样就可以保证永远被执行到，也就不怕关闭不了连接了。
f,err:=os.Open(filename) defer f.Close() readAll(f) 统一编码风格 在编码中，我们有时为了是否空行，大括号是否独占一行等编码风格问题争论不休，到了 go 这里就终止了，因为 go 是强制的，比如花括号不能独占一行，比如定义的变量必须使用，否则就不能编译通过。
第二种就是go fmt这个工具提供的非强制性规范，虽然不是强制的，不过也建议使用，这样整个团队的代码看着就像一个人写的。很多 go 代码编辑器都提供保存时自动gofmt格式的话，所以效率也非常高。
便捷的部署 go 最终生成的是一个可执行文件，不管你的程序依赖多少库，都会被打包进行，生成一个可执行文件，所以相比java庞大的jar库来说，他的部署非常方便，执行运行这个可执行文件就好了。
对于Web开发，更方便，不用安装jdk，tomcat容器等等这些环境，直接一个可执行文件，就启动了。对于 go 这种便捷的部署方式，我觉得他更能推进docker的服务化，因为docker就是倡导一个实例一个服务，而且不用各种依赖，layer层级又没那么多，docker image也会小很多。
最后， go 目前已经在TIOBE语言排行榜上名列13名了，上升速度还是非常快的，而且随着服务化，容器化，他的优势会越来越多的显现出来，得到更广泛的应用。
如果你感兴趣，那么开始吧，提前准备，机会来的时候，就不会错过了。
原文出处 ]]></content></entry><entry><title>[转] Git查看、删除、重命名远程分支和tag</title><url>/2017/06/15/%E8%BD%AC-Git%E6%9F%A5%E7%9C%8B%E5%88%A0%E9%99%A4%E9%87%8D%E5%91%BD%E5%90%8D%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF%E5%92%8Ctag/</url><categories><category>Git</category></categories><tags><tag>Git</tag></tags><content type="html"><![CDATA[查看远程分支 加上 -a 参数可以查看远程分支，远程分支会用红色表示出来（如果你开了颜色支持的话）：
$ git branch -a master remote tungway v1.52 * zrong remotes/origin/master remotes/origin/tungway remotes/origin/v1.52 remotes/origin/zrong 删除远程分支和 tag 在 Git v1.7.0 之后，可以使用这种语法删除远程分支：
$ git push origin --delete &lt;branchName&gt; 删除 tag 这么用：
git push origin --delete tag &lt;tagname&gt; 否则，可以使用这种语法，推送一个空分支到远程分支，其实就相当于删除远程分支：
git push origin :&lt;branchName&gt; 这是删除 tag 的方法，推送一个空 tag 到远程 tag：
git tag -d &lt;tagname&gt; git push origin :refs/tags/&lt;tagname&gt; 两种语法作用完全相同。
删除不存在对应远程分支的本地分支 假设这样一种情况：
我创建了本地分支 b1 并 pull 到远程分支 origin/b1； 其他人在本地使用 fetch 或 pull 创建了本地的 b1 分支； 我删除了 origin/b1 远程分支； 其他人再次执行 fetch 或者 pull 并不会删除这个他们本地的 b1 分支，运行 git branch -a 也不能看出这个 branch 被删除了，如何处理？ 使用下面的代码查看 b1 的状态：
$ git remote show origin * remote origin Fetch URL: git@github.com:xxx/xxx.git Push URL: git@github.com:xxx/xxx.git HEAD branch: master Remote branches: master tracked refs/remotes/origin/b1 stale (use &#39;git remote prune&#39; to remove) Local branch configured for &#39;git pull&#39;: master merges with remote master Local ref configured for &#39;git push&#39;: master pushes to master (up to date) 这时候能够看到 b1 是 stale 的，使用 git remote prune origin 可以将其从本地版本库中去除。
更简单的方法是使用这个命令，它在 fetch 之后删除掉没有与远程分支对应的本地分支：
git fetch -p 重命名远程分支 在 git 中重命名远程分支，其实就是先删除远程分支，然后重命名本地分支，再重新提交一个远程分支。
例如下面的例子中，我需要把 devel 分支重命名为 develop 分支：
$ git branch -av * devel 752bb84 Merge pull request #158 from Gwill/devel master 53b27b8 Merge pull request #138 from tdlrobin/master zrong 2ae98d8 modify CCFileUtils, export getFileData remotes/origin/HEAD -&gt; origin/master remotes/origin/add_build_script d4a8c4f Merge branch &#39;master&#39; into add_build_script remotes/origin/devel 752bb84 Merge pull request #158 from Gwill/devel remotes/origin/devel_qt51 62208f1 update .gitignore remotes/origin/master 53b27b8 Merge pull request #138 from tdlrobin/master remotes/origin/zrong 2ae98d8 modify CCFileUtils, export getFileData 删除远程分支：
$ git push --delete origin devel To git@github.com:zrong/quick-cocos2d-x.git - [deleted] devel 重命名本地分支：
git branch -m devel develop 推送本地分支：
$ git push origin develop Counting objects: 92, done. Delta compression using up to 4 threads. Compressing objects: 100% (48/48), done. Writing objects: 100% (58/58), 1.38 MiB, done. Total 58 (delta 34), reused 12 (delta 5) To git@github.com:zrong/quick-cocos2d-x.git * [new branch] develop -&gt; develop 然而，在 github 上操作的时候，我在删除远程分支时碰到这个错误：
$ git push --delete origin devel remote: error: refusing to delete the current branch: refs/heads/devel To git@github.com:zrong/quick-cocos2d-x.git ! [remote rejected] devel (deletion of the current branch prohibited) error: failed to push some refs to &#39;git@github.com:zrong/quick-cocos2d-x.git&#39; 这是由于在 github 中，devel 是项目的默认分支。要解决此问题，这样操作：
进入 github 中该项目的 Settings 页面； 设置 Default Branch 为其他的分支（例如：master）； 重新执行删除远程分支命令。 把本地 tag 推送到远程 git push --tags 获取远程 tag git fetch origin tag &lt;tagname&gt; 参考文章 https://makandracards.com/makandra/621-git-delete-a-branch-local-or-remote http://stackoverflow.com/questions/2003505/how-do-i-delete-a-git-branch-both-locally-and-in-github http://www.cnblogs.com/deepnighttwo/archive/2011/06/18/2084438.html http://stackoverflow.com/questions/14040754/deleting-remote-master-branch-refused-due-to-being-current-branch http://weli.iteye.com/blog/1441582 原文出处 ]]></content></entry><entry><title>占用端口、PID 查询总结</title><url>/2017/05/24/query-port-pid/</url><categories><category>Linux</category></categories><tags><tag>Linux</tag><tag>Mac</tag></tags><content type="html"> 总结一下日常工作学习中常用的查询占用端口、PID相关命令。主要有命令 ps、lsof、netstat 等命令。
ps ps -ef 或者 ps aux
根据命令（IntelliJIDEALicenseServer）搜索 PID（17107）
$ ps -ef | grep IntelliJIDEALicenseServer | grep -v &amp;#34;grep&amp;#34; 501 17017 12203 0 10:01上午 ttys000 0:00.02 IntelliJIDEALicenseServer -p 21014 lsof 根据 PID（17107）搜索占用端口（21014）
$ lsof -p 17017 -nP | grep TCP COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME IntelliJI 17017 ehlxr 3u IPv4 0xc0df10496212b0e3 0t0 TCP *:21014 (LISTEN) # 或者使用 $ lsof -nP -iTCP -sTCP:LISTEN | grep 17017 -n 表示不显示主机名 -P 表示不显示端口俗称 根据端口（21014）搜索 PID（17107）
$ lsof -i:21014 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME IntelliJI 17017 ehlxr 3u IPv4 0xc0df10496212b0e3 0t0 TCP *:21014 (LISTEN) netstat 仅 Linux 系统下好使，Mac 下无效
端口（21017），PID（1847）
$ netstat -antlp | grep IntelliJIDEA tcp 0 0 0.0.0.0:21017 0.0.0.0:* LISTEN 1847/./IntelliJIDEA</content></entry><entry><title>Linux 操作笔记</title><url>/2017/04/14/Linux-%E6%93%8D%E4%BD%9C%E7%AC%94%E8%AE%B0/</url><categories><category>Linux</category></categories><tags><tag>Linux</tag></tags><content type="html"><![CDATA[ 好记性不如烂笔头，记录一下日常工作学习中常使用的 Linux 命令。
安装磁盘分配 /boot 用来存放与 Linux 系统启动有关的程序，比如启动引导装载程序等，建议大小为 100MB。 swap 实现虚拟内存，建议大小是物理内存的1~2倍。 / Linux系统的根目录，所有的目录都挂在这个目录下面，建议大小为5GB以上。 /home 存放普通用户的数据，是普通用户的宿主目录，建议大小为剩下的空间。 系统目录结构 /：存放系统程序，也就是 At&amp;t 开发的 Unix 程序。 /usr：存放 Unix 系统商（比如 IBM 和 HP）开发的程序。 /usr/local：存放用户自己安装的程序。 /opt：在某些系统，用于存放第三方厂商开发的程序，所以取名为 option，意为 &ldquo;选装&rdquo;。 常用基本命令 ls 列出当前目录下的文件 ls ser* 查找 ser 打头的文件 ls -l （简化为：ll）列出当前目录下文件的详细信息 ls -a 列出当前目录下的所有文件（包括隐藏文件） ls -t 依时间排序，而不是用档名 ls -S 依大小排序，ls -Sr 按大小倒序 ls --full-time 以完整时间模式 (包含年、月、日、时、分) 输出（ Mac 是 ls -T） ll -d 查看当前目录的详细信息 ll -a 详细列出当前目录下的所有文件 ll 路径 列出指定路径（可为相对或绝对路径）下的文件 ll -d 路径 查看指定路径的详细信息 打开文件夹：cd 路径 cd /etc （相对路径） 或 cd xxx（绝对路径） 返回上一级目录：cd .. 查看当前目录：pwd（path where dir） 查看当前用户：whoami 切换用户：su -用户名 或者 su 用户名（从其他用户切换到 root 用户需要密码，从 root 用户切换到任何其他用户不需要密码） 切换到 root 用户：su 切换到 root 用户后使用命令：exit 切换到普通用户 清屏：clear 或者 Ctrl + L 关机：halt 或者 shutdown -h now 修改密码：修改 root 自己的密码，直接输入 passwd，输入两遍新密码即可。若修改其他用户，如 oracle 的密码，可直接输入 passwd oracle，输入两遍性新密码即可。 查看系统编码：locale 获取权限：chmod 777 文件名 检索（例如检索 profile）：ls -l | grep profile 文件拖拽软件安装：yum install lrzsz 查看进程：ps -ef、ps auxf 查看端口：netstat -antlp | grep xx 查看资源占用情况：top（Shit + &gt; 切换排序列） 查看内存：free -m/g 查看目录大小：du -sh /opt/registry 修改配置文件后立即生效：sysctl -p 查看系统环境变量：env dirname 用于取指定路径所在的目录，如：dirname /home/ikidou，结果为：/home cd dirname $0：一般用户 Shell 脚本中，切换到执行脚本所在的目录 tee：在执行 Linux 命令时，我们可以把输出重定向到文件中，比如 ls &gt; a.txt，这时我们就不能看到输出了，如果我们既想把输出保存到文件中，又想在屏幕上看到输出内容，就可以使用 tee 命令 tree：树形查看当前目录结构 系统负载查看：uptime、w、top 命令 &ldquo;load average&rdquo;，它的意思是 &ldquo;系统的平均负荷&rdquo;，里面有三个数字表示 1分钟、5分钟、15分钟内系统的平均负荷。 列出谁在使用 3306 端口：lsof -i:3306 文件操作命令 路径（绝对路径、相对路径）
创建文件夹：mkdir [路径] + 文件夹
mkdir [路径] + 文件夹1 [路径] + 文件夹2（可以同时创建多个文件夹，空格隔开） mkdir -p [路径]/[路径] (创建多层级目录) 创建文件：touch [路径] + 文件名
touch [路径] + 文件名1 [路径] + 文件名2（可以同时创建多个文件，空格隔开） 如何区分是文件夹还是文件：
删除（remove）文件：rm [路径] + 文件名 （可以删除多个文件，每个文件用空格隔开）
删除文件夹：rm -r 文件夹1 文件夹2（递归 recursive 删除文件夹1和文件夹2下的所有内容，每删除一个会提示） rm -rf 文件夹（强制 force 删除文件夹下的所有内容，不提示删除）删除后无法还原 删除文件夹或者： rmdir 文件夹（文件夹必须为空） 删除所有内容：rm -rf * 复制（copy）文件：cp 源文件 目标文件夹（正常情况下使用绝对路径） -复制文件夹：cp -r 源文件 目标文件夹
移动（move）文件：mv 源文件 目标文件夹（正常情况下使用绝对路径）
移动文件夹：mv 源文件夹 目标文件夹 重命名：mv aa.txt aaaa.txt 文件读写命令 echo &quot;Hello World&quot; &gt;&gt; a.txt 将字符串 Hello World 追加到文件 a.txt 中。 echo &quot;Hello World&quot; &gt; a.txt 将文件 a.txt 中的内容替换为字符串 Hello World。 查看文件内容：cat a.txt more services 分页查看 services 文件中的内容，按空格或 f 切换下一页，回车下一行,q 退出。（文件内容较多时使用） head services 查看 services 文件前 10 行的内容（默认前 10 行） head -20 services （前20行的内容） tail services 查看 services 文件结尾 10 行的内容 tail -20f services（滚动显示结尾 20 行的内容） vi/vim services vi 可以修改文件 文件链接：ln 文件 链接 修改源文件或者链接文件，两者同时更新 ln -s /test/a.txt /test/as.txt 软链接 as.txt 就是针对 a.txt 的软链接（类似如 windows 的快捷方式）a.txt 删除后，as.txt无效 ln /test/a.txt /test/ah.txt 硬链接 a.txt 删除后 ah.txt 仍然有效 ]]></content></entry><entry><title>Poker 机械键盘入手记</title><url>/2017/04/09/Poker-%E6%9C%BA%E6%A2%B0%E9%94%AE%E7%9B%98%E5%85%A5%E6%89%8B%E8%AE%B0/</url><categories><category>杂谈</category></categories><tags><tag>杂谈</tag></tags><content type="html"><![CDATA[作为一个码农，天天和键盘打交道，没有一款称心如意的键盘怎么能行了？要说那一款键盘堪称神器，我想大多数程序员首推 HHKB ，怎奈囊中羞涩，只能找一款 HHKB 的替代品，于是，Poker 浮现我的眼中，经济实惠，小巧玲珑，深得我心&hellip;
关注这款键盘其实很久了，虽说相对于 HHKB 来说便宜很多，但也有点儿心疼，迟迟没能入手。最近过生日，女朋友很民主的征求我的意见，问我想要什么礼物，嘿嘿
昨天下午京东入手，399 大洋，白色茶轴，今天上午就到了，嘻嘻！
先来两张，亮亮相！ 包装盒，”The keyboard to cheer you up“，简洁、大气！
PBT 材质的键帽手感棒棒的！
带了六个键帽，换上之后的效果。有没有显得很有灵气(女朋友反而觉得还是全白色的好看)
真的很小，都不够一个手的宽度，长度比 MBP 自带键盘稍长一点。
ikbc 正品，妥妥的！
修改键位 键盘的背面有 4 个拨动开关的，他们的功能分别是：
开关 1：Caps 与左 Win 切换 开关 2：右 Ctrl 与 ` ~ 切换 开关 3：左 Win 与 Fn 切换 开关 4：键盘写保护，键位编程 这是默认的键盘布局：
将开关 1 和 3 都拨到 ON 的位置，效果：Caps 变成了 Fn，左 Win 变成了 Caps，右 Fn 还是 Fn 打开 Mac 的系统偏好设置，在键盘的修饰键修改中，选择 Poker 更改修饰键，如下： Caps Lock -&gt; Option Control -&gt; Control Option -&gt; Command Command -&gt; Option 搞定，然后用拔键器把左侧的 Alt 和 Win 键帽互换个位置，这样就更接近 Mac 的标准布局了。 这样，左手按住 Fn + w、s、a、d 方便的实现上、下、左、右的操作了。
其实还有个方便使用方向键的方法，按一下fn + 空格， w, a, s, d就是方向键了，再按一次则恢复到基本模式。
唯一的问题就是现在 Caps Lock 键消失了，只不过不常用。后期可以研究一下如何通过编程的方式实现其他组合键。
参考：
Poker II 机械键盘键位改造记 ]]></content></entry><entry><title>Java 获取系统的配置信息</title><url>/2017/03/29/Java-%E8%8E%B7%E5%8F%96%E7%B3%BB%E7%BB%9F%E7%9A%84%E9%85%8D%E7%BD%AE%E4%BF%A1%E6%81%AF/</url><categories><category>Java开发技术</category></categories><tags><tag>Java</tag></tags><content type="html">System.getProperty() 可以获取系统的配置信息，最近项目开发中要用到临时文件，所以想到了使用系统临时文件目录，最后得知可以通过 System.getProperty(&amp;quot;java.io.tmpdir&amp;quot;) 可以获取不同操作系统平台下的临时目录。比如：
在 windows 中的目录是：C:\Users\登录用户~1\AppData\Local\Temp\
在 linux 下的目录是：/tmp
在 Mac 下目录是 /var/folders/c8/2c9rf0ss2w9c8tdtfcgvg9kh0000gn/T/ （我感觉是不同电脑应该不一样）
借此机会总结一下 System.getProperty() 可以获取那些系统信息:
java.version // Java运行时环境版本 java.vendor // Java运行时环境供应商 java.vendor.url // Java供应商的 URL java.home // Java安装目录 java.vm.specification.version // Java虚拟机规范版本 java.vm.specification.vendor // Java虚拟机规范供应商 java.vm.specification.name // Java虚拟机规范名称 java.vm.version // Java虚拟机实现版本 java.vm.vendor // Java虚拟机实现供应商 java.vm.name // Java虚拟机实现名称 java.specification.version // Java运行时环境规范版本 java.specification.vendor // Java运行时环境规范供应商 java.specification.name // Java运行时环境规范名称 java.class.version // Java类格式版本号 java.class.path // Java类路径 java.library.path // 加载库时搜索的路径列表 java.io.tmpdir // 默认的临时文件路径 java.compiler // 要使用的 JIT 编译器的名称 java.ext.dirs // 一个或多个扩展目录的路径 os.name // 操作系统的名称 os.arch // 操作系统的架构 os.version // 操作系统的版本 file.separator // 文件分隔符（在 UNIX 系统中是“/”） path.separator // 路径分隔符（在 UNIX 系统中是“:”） line.separator // 行分隔符（在 UNIX 系统中是“/n”） user.name // 用户的账户名称 user.home // 用户的主目录 user.dir // 用户的当前工作目录</content></entry><entry><title>Mac 使用技巧总结</title><url>/2017/02/21/Mac-%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%E6%80%BB%E7%BB%93/</url><categories><category>开发工具</category></categories><tags><tag>Mac</tag></tags><content type="html"><![CDATA[ Mac 系统日常使用技巧归纳总结。
一、Mac 中 Finder 显示、关闭隐藏文件 1.1 终端命令模式 打开终端，输入：
# 此命令显示隐藏文件 defaults write com.apple.finder AppleShowAllFiles -bool true # 此命令关闭显示隐藏文件 defaults write com.apple.finder AppleShowAllFiles -bool false 命令运行之后需要重新加载 Finder
快捷键 Option + Command + ESC，选中 Finder，重新启动即可
1.2 Finder 快捷键 在 macOS Sierra（10.12） 版本的系统中，我们可以使用快捷键⌘⇧.(Command + Shift + .) 来快速（在 Finder 中）显示和隐藏隐藏文件了。
二、Terminal 中使用 Sublime 打开文件 2.1 如果使用默认 Shell 在 Terminal 输入以下命令：
sudo ln -s &#34;/Applications/Sublime\ Text.app/Contents/SharedSupport/bin/subl&#34; /usr/bin/subl 2.2 如果使用 zsh 在文件 ~/.zshrc 最后添加
alias subl=&#34;&#39;/Applications/Sublime Text.app/Contents/SharedSupport/bin/subl&#39;&#34; alias nano=&#34;subl&#34; export EDITOR=&#34;subl&#34; 然后在 Terminal 中使用 subl 就可以打开 Sublime 了。
2.3 使用 Sublime 打开指定的文件 open -a Sublime\ Text .m2/settings.xml 可以使用 Tab 自定补全应用名称
三、多屏切换 dock 显示 如果 dock 在屏幕底部显示，移动鼠标到要显示 dock 的屏幕上，单击使屏幕处于选中状态，在屏幕底部中间部位向下滑动鼠标即可移动 dock 到当前屏幕上显示。
四、Terminal 重启 Finder、dock 等命令 # Finder 崩溃重启 killall -kill Finder # Dock 崩溃重启 killall -kill Dock # 菜单栏崩溃重启 killall -kill SystemUIServer 五、Terminal 中用打开当前路径的文件夹 在 Terminal 中输入命令：
open . 六、剪贴文件 # 复制 Command + c # 剪切到当前目录 Command + Option + v 七、快速打开最小化程序 Command + Tab 切换程序的时候，最小化的程序不自动弹出窗口，解决办法如下：
用 Command + Tab 切换到要打开的程序 先松开 Tab 键，不要松开 Command 键 然后按住 Option 键 然后依次松开 Command 和 Option 键 如上操作就可以快速打开最小化的程序了。
八、常用快捷键 Mac 键盘快捷键官方参考 功能 快捷键 预览文件 空格 重命名 Enter 打开文件 Command + o、Command + ↓ 显示桌面 F11 切换窗口全屏状态 Ctrl + Cmmad + f 影藏/显示 Dock Command + Option + d 输入 emoji 表情 Ctrl + Cmmad + 空格 九、.DS_Store 文件处理 .DS_Store (英文全称 Desktop Services Store) 是一种由苹果公司的 Mac OS X 操作系统所创造的隐藏文件，目的在于存贮目录的自定义属性，例如文件们的图标位置或者是背景色的选择。 9.1 清除文件 清除命令：
sudo find / -name &quot;.DS_Store&quot; -depth -exec rm {} \;
9.2 禁用或启用自动生成 执行以下命令，回车执行，重启 Mac 即可生效。
禁止.DS_store生成： defaults write com.apple.desktopservices DSDontWriteNetworkStores -bool TRUE
恢复.DS_store生成： defaults delete com.apple.desktopservices DSDontWriteNetworkStores
十、NTFS 格式移动盘写入文件 一般来说，MacOS 是不支持 NTFS 格式磁盘的写入，可以借助三方工具，例如：Tuxera NTFS，不过收费还挺高的。这里介绍一种方法，不需要借助三方工具也可以实现 NTFS 格式磁盘写入。（基于系统版本 macOS Sierra 10.12.4 亲测可行）
插入移动硬盘，终端输入命令：$ diskutil list 查看移动硬盘名称。例如：
$ diskutil list .... /dev/disk2 (external, physical): #: TYPE NAME SIZE IDENTIFIER 0: FDisk_partition_scheme *500.1 GB disk2 1: Windows_NTFS EHLXR 500.1 GB disk2s4 编辑系统文件 sudo vim /etc/fstab ，写入以下内容，然后保存。
LABEL=EHLXR none ntfs rw,auto,nobrowse 其中 EHLXR 为移动硬盘名称。
推出硬盘重新插入，这时候桌面已不再显示移动硬盘图标。
在 Finder 中通过快捷键 Command + Shift + g 输入/Volumes 就可以重新看到移动硬盘，这时候打开移动硬盘就可以写入文件了。
为了方便起见，可以通过命令：sudo ln -s /Volumes ~/Desktop/Volumes 在桌面建立快捷方式，方便查看。
未完待续&hellip;
]]></content></entry><entry><title>Linux 的 nohup 命令的用法</title><url>/2017/01/18/Linux-%E7%9A%84-nohup-%E5%91%BD%E4%BB%A4%E7%9A%84%E7%94%A8%E6%B3%95/</url><categories><category>Linux</category></categories><tags><tag>Linux</tag><tag>nohup</tag></tags><content type="html"><![CDATA[ 如果你正在运行一个进程，而且你觉得在退出帐户时该进程还不会结束，那么可以使用 nohup 命令。
命令简介 $ nohup Command [ Arg … ] [ &amp; ] 该命令可以在你退出帐户/关闭终端之后继续运行相应的进程。nohup 就是不挂起的意思（no hang up）。
nohup 运行由 Command 和相关的 Arg 参数指定的命令，忽略所有挂断（SIGHUP）信号。要运行后台中的 nohup 命令，添加 &amp; （ 表示 and 的符号）到命令的尾部。
如果使用 nohup 命令提交作业，那么在缺省情况下该作业的所有输出都被重定向到一个名为 nohup.out 的文件中，除非另外指定了输出文件。
使用举例 $ nohup java -jar adapter-minisite.jar /tomcat-1 /tomcat-2 &gt; logs.txt 2&gt;&amp;1 &amp; 在上面的例子中，运行命令 java -jar adapter-minisite.jar 输入参数 /tomcat-1 和 /tomcat-2，输出被重定向到 logs.txt 文件中。
退出任务 如果运行的任务在当前 shell 终端，可以通过 jobs 命令查询相关信息，并且杀掉进程。
# 查看当前 shell 终端的后台运行任务进程信息 $ jobs [1]+ Running nohup java -jar adapter-minisite.jar /tomcat-1 /tomcat-2 &gt; logs.txt 2&gt;&amp;1 &amp; # 杀掉任务号 $ kill %1 # 或着找到 pid $ jobs -l [1]+ 11076 Running nohup java -jar adapter-minisite.jar /tomcat-1 /tomcat-2 &gt; logs.txt 2&gt;&amp;1 &amp; $ kill 11076 # 或着 $ fg %n # 置为前端运行 Ctrl + c # 退出 如果非当前 shell 终端，可以通过 ps auxf | grep 'adapter-minisite' 获取 pid 然后 kill pid。
参考文章 ]]></content></entry><entry><title>Linux 中 fg、bg、jobs 等指令</title><url>/2017/01/18/Linux-%E4%B8%AD-fgbgjobs-%E7%AD%89%E6%8C%87%E4%BB%A4/</url><categories><category>Linux</category></categories><tags><tag>Linux</tag></tags><content type="html"><![CDATA[ 记录总结一下 Linux 中 fg、bg、jobs、&amp;、ctrl + z 等相关指令对任务进程的操作。
一、基本用法 1.1 &amp; 和 jobs 指令 &amp; 用在一个命令的最后，可以把这个命令转换为后台运行的任务进程。
jobs 查看当前终端有多少在后台运行的进程。
jobs 命令执行的结果，＋ 表示是一个当前的作业，- 减号表示是一个当前作业之后的一个作业。
jobs -l 选项可显示所有任务的进程号 pid
jobs 的状态可以是 running，stopped，terminated。但是如果任务进程被终止了（kill），当前的终端环境中也就删除了任务的进程标识；也就是说 jobs 命令显示的是当前 shell 环境中后台正在运行或者被挂起的任务进程信息
1.3 fg 和 bg 指令 fg 将后台任务进程调至前台继续运行，如果后台中有多个任务进程，可以用 fg %num 将选中的任务进程调至前台。
bg 将挂起的任务进程重新启动运行，如果有多个暂停的任务进程，可以用 bg %num 将选中的任务进程启动运行。
%num 是通过 jobs 命令查到的后台正在执行的任务的序号（不是 pid）
二、进程的挂起 2.1 后台进程的挂起 在 solaris 中通过 stop 命令执行，通过 jobs 命令查看任务号（假设为 num），然后执行：stop %num
在 redhat 中，不存在 stop 命令，可通过执行命令 kill -stop PID，将进程挂起
2.2 前台进程的挂起 ctrl + z：可以将一个正在前台执行的任务放到后台运行，并且挂起
三、挂起进程重新运行 通过 bg %num 即可将挂起的任务进程的状态由 stopped 改为 running，仍在后台运行
通过 fg %num 即可将挂起的任务进程转为前台执行
四、进程的终止 4.1 后台进程的终止 方法一： 通过 jobs 命令查看任务号（假设为 num），然后执行：kill %num
方法二： 通过 ps 命令查看任务的进程号（PID，假设为 pid），然后执行：kill pid
4.2 前台进程的终止 执行 ctrl+c 即可终止前台执行任务进程
假设要后台运行 xmms，可通过命令：xmms &amp;。但万一你运行程序时忘记使用 &amp; 了，又不想重新执行，你可以先使用 ctrl+z 挂起任务进程，然后敲入bg 命令，这样任务进程就在后台继续运行了。
]]></content></entry><entry><title>[转] CentOS 升级 kernel</title><url>/2017/01/10/%E8%BD%AC-CentOS-%E5%8D%87%E7%BA%A7-kernel/</url><categories><category>Linux</category></categories><tags><tag>CentOS</tag><tag>kernel</tag><tag>Linux</tag></tags><content type="html">一、手动档 手动档就是从源码开始编译内核安装，好处是可以自己选择任意版本的内核，缺点就是耗时长，编译安装消耗系统资源
1.1、获取 kernel 源码 这世界上最伟大的 Linux 内核源码下载地址是 kernel 官网，选择一个稳定版本下载即可
1.2、解压并清理 官方要求将其解压到 /usr/src 目录，其实在哪都可以，为了规范一点索性也解压到此位置，然后为了防止编译残留先做一次清理动作
# 下载内核源码 $ wget https://cdn.kernel.org/pub/linux/kernel/v4.x/linux-4.8.6.tar.xz # 解压并移动到 /usr/src $ tar -Jxvf linux-4.8.6.tar.xz $ mv linux-4.8.6 /usr/src/kernels # 执行清理（没 gcc 的要装一下） $ cd /usr/src/kernels/linux-4.8.6 $ make mrproper &amp;amp;&amp;amp; make clean 1.3、生成编译配置表 kernel 在编译时需要一个配置文件（.config），用于描述开启哪些特性等，该文件一般可通过一下四种途径获得:
复制当前系统编译配置表，即 cp /boot/config-xxx .config；如果系统有多个内核，那么根据版本号选择最新的即可 使用 make defconfig 命令获取当前系统编译配置表，该命令会自动写入到 .config 中 使用 make localmodconfig 命令开启交互模式，然后根据提示生成编译配置表 使用 make oldconfig 命令根据旧的编译配置表生成新的编译配置表，刚方式会直接读取旧的便已配置表，并在以前没有设定过的配置时会自动开启交互模式 这里采用最后一种方式生成
1.4、编译并安装 内核配置表生成完成后便可进行编译和安装（需要安装 bc、openssl-devel 等）
$ make $ make modules $ make modules_install $ make install 最后执行重启验证即可，验证成功后可删除旧的内核
# 检索已安装的内核 $ sudo rpm -qa | grep kernel # 删除指定的内核 $ sudo rpm -e kernel-3.10.0-327.el7.x86_64 二、自动档 相对于手动档编译安装，CentOS 还可以通过使用 elrepo 源的方式直接安装最新稳定版 kernel，脚本如下
# import key $ sudo rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org # install elrepo repo $ sudo rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm # install kernel $ sudo yum --enablerepo=elrepo-kernel install kernel-ml-devel kernel-ml -y # modify grub $ sudo grub2-set-default 0 # reboot system $ sudo reboot 原文地址</content></entry><entry><title>Docker 利用数据卷容器来备份、恢复、迁移数据</title><url>/2017/01/09/Docker-%E5%88%A9%E7%94%A8%E6%95%B0%E6%8D%AE%E5%8D%B7%E5%AE%B9%E5%99%A8%E6%9D%A5%E5%A4%87%E4%BB%BD%E6%81%A2%E5%A4%8D%E8%BF%81%E7%A7%BB%E6%95%B0%E6%8D%AE/</url><categories><category>Docker</category></categories><tags><tag>Docker</tag></tags><content type="html"> 在 Docker 容器之间如果需要共享数据，可以创建一个数据卷容器来实现，并且可以方便的通过数据卷容器来备份、恢复、迁移数据。
创建数据卷容器 创建一个名为：dbdata 的数据卷容器；设置挂载点为 /vdata。
$ sudo docker run -d -v /vdata --name dbdata alpine sh 数据卷容器是一个普通的 Docker 容器，可以不需要启动。
使用数据卷容器 使用命令 --volumes-from 创建挂载数据卷容器 dbdata 的容器：db1、db2。
$ sudo docker run -it --volumes-from dbdata --name db1 alpine sh $ sudo docker run -it --volumes-from dbdata --name db1 alpine sh 在容器 db1 的挂载目录 /vdata 目录下，创建文件 1.txt 等测试数据，查看容器 db2 的挂载目录 /vdata 目录，就可以看到创建的文件数据了。
备份数据卷数据 使用一个临时容器，完成备份数据容器操作。
$ sudo docker run --rm --volumes-from dbdata -v $(pwd):/backup alpine tar cvf /backup/vdata-bak.tar /vdata 使用 tar cvf 命令，备份数据卷容器 dbdata 中的目录 /vdata 为 vdata-bak.tar，并挂载到宿主机的当前目录下。
恢复数据卷数据 创建数据卷容器：dbdata2
$ sudo docker run -d -v /vdata --name dbdata2 alpine sh 使用一个临时容器，恢复备份数据 vdata-bak.tar 到 dbdata2
$ sudo docker run --rm --volumes-from dbdata2 -v $(pwd):/backup alpine tar xvf /backup/vdata-bak.tar 使用一个临时容器，查看 dbdata2 恢复的数据：
$ sudo docker run --rm --volumes-from dbdata2 alpine /bin/ls /vdata</content></entry><entry><title>CentOS 7 安装Python3、pip3</title><url>/2017/01/07/CentOS-7-%E5%AE%89%E8%A3%85Python3pip3/</url><categories><category>Linux</category></categories><tags><tag>Linux</tag><tag>CentOS</tag><tag>Python</tag></tags><content type="html">CentOS 7 默认安装了 Python 2，当需要使用 Python 3 的时候，可以手动下载 Python 源码后编译安装。
一、安装 Python 3 1.1 安装准备 $ sudo mkdir /usr/local/python3 # 创建安装目录 # 下载 Python 源文件 $ wget --no-check-certificate https://www.python.org/ftp/python/3.6.0/Python-3.6.0.tgz # 注意：wget获取https的时候要加上：--no-check-certificate $ tar -xzvf Python-3.6.0.tgz # 解压缩包 $ cd Python-3.6.0 # 进入解压目录 1.2 编译安装 $ sudo ./configure --prefix=/usr/local/python3 # 指定创建的目录 $ sudo make $ sudo make install 1.3 配置 1.3.1 两个版本共存 创建 python3 的软链接：
$ sudo ln -s /usr/local/python3/bin/python3 /usr/bin/python3 这样就可以通过 python 命令使用 Python 2，python3 来使用 Python 3。
1.3.2 修改默认为 Python 3 将 /usr/bin 中的 python 备份
$ sudo mv python python.bak 然后创建 python3 的软链接
$ sudo ln -s /usr/local/python3/bin/python3 /usr/bin/python 这样默认的 Python 版本就替换为 Python 3 了。
因为 yum 使用 Python 2，因此替换为 Python 3 后可能无法正常工作，因此修改 yum 配置文件
$ sudo vi /usr/bin/yum 将第一行指定的 python 版本改为 python2.7（#!/usr/bin/python 改为 #!/usr/bin/python2.7）
二、安装 pip 2.1 yum 安装 # 首先安装 epel 扩展源 $ sudo yum -y install epel-release # 安装 python-pip $ sudo yum -y install python-pip # 清除 cache $ sudo yum clean all 通过这种方式貌似只能安装 pip2，想要安装 Python 3 的 pip，可以通过以下的源代码安装方式。
2.2 源码安装 # 下载源代码 $ wget --no-check-certificate https://github.com/pypa/pip/archive/9.0.1.tar.gz $ tar -zvxf 9.0.1 -C pip-9.0.1 # 解压文件 $ cd pip-9.0.1 # 使用 Python 3 安装 $ python3 setup.py install 创建链接：
$ sudo ln -s /usr/local/python3/bin/pip /usr/bin/pip3 2.3 升级 pip $ pip install --upgrade pip</content></entry><entry><title>Good bye 2016...</title><url>/2016/12/31/Good-bye-2016.../</url><categories><category>年末总结</category></categories><tags><tag>年末总结</tag></tags><content type="html"><![CDATA[时光飞逝，转眼间，2016 年已经在今天画上句号，结束了，再提起 2016 年就已经是过往了&hellip;
印象中，十年应该要算是很长的一段时间吧，但仔细一琢磨十年前也就才 2006 年&hellip;那时候是高中，每天除了上课就是写作业，学习很枯燥总感觉时间很多，憧憬着美好的未来，过着也算是无忧无虑生活。每天放学，下晚自习，总是很快的骑着自行车冲出校门口，因为晚了学生会很多，那时候骑自行车一个比一个快。周五不用上晚自习，三五成群去网吧通宵，包宿八块钱八个小时，从晚上十点到第二天早上六点，六点从网吧出来总会有种恍如隔世的感觉，大街上几乎没有人，回出租屋睡上一天，感觉就是一周最美好的时光了！看着别的同学拿着小灵通、MP3 ，很羡慕，自已也想要有一个，都不敢奢望能有一台电脑&hellip;一切仿佛也就是昨天而已，但是已经十年之前了，不禁一颤，人生能有几个十年&hellip;
总结 2016，收获，知足，感恩&hellip;眼前的要珍惜，来之不易的拥有更要珍惜。
]]></content></entry><entry><title>[转] shell 中单引号、双引号、反引号、反斜杠区别</title><url>/2016/12/30/%E8%BD%AC-shell-%E4%B8%AD%E5%8D%95%E5%BC%95%E5%8F%B7%E5%8F%8C%E5%BC%95%E5%8F%B7%E5%8F%8D%E5%BC%95%E5%8F%B7%E5%8F%8D%E6%96%9C%E6%9D%A0%E5%8C%BA%E5%88%AB/</url><categories><category>Linux</category></categories><tags><tag>Linux</tag><tag>shell</tag></tags><content type="html"><![CDATA[ shell 可以识别 4 种不同类型的引字符号：单引号字符 '，双引号字符 &quot;，反斜杠字符 \，反引号字符 ` ，学习 shell 编程的朋友可以看下。
1. 单引号( ' ) $ grep Susan phonebook Susan Goldberg 403-212-4921 Susan Topple 212-234-2343 如果我们想查找的是 Susan Goldberg，不能直接使用 grep Susan Goldberg phonebook 命令，grep 会把 Goldberg 和 phonebook 当作需要搜索的文件。
$ grep &#39;Susan Gold&#39; phonebook Susan Goldberg 403-212-4921 当 shell 碰到第一个单引号时，它忽略掉其后直到右引号的所有特殊字符。
2. 双引号( &quot; ) 双引号作用与单引号类似，区别在于它没有那么严格。单引号告诉 shell 忽略所有特殊字符，而双引号只要求忽略大多数，具体说，括在双引号中的三种特殊字符不被忽略：$、\、`。即双引号会解释字符串的特别意思，而单引号直接使用字符串。如果使用双引号将字符串赋给变量并反馈它，实际上与直接反馈变量并无差别。如果要查询包含空格的字符串，经常会用到双引号。
$ x=* $ echo $x hello.sh menus.sh misc.sh phonebook tshift.sh $ echo &#39;$x&#39; $x $ echo &#34;$x&#34; * 这个例子可以看出无引号，单引号和双引号之间的区别。在最后一种情况中，双引号告诉 shell 在引号内照样进行变量名替换，所以 shell 把 $x 替换为 ＊，因为双引号中不做文件名替换，所以就把 ＊ 作为要显示的值传递给 echo。
对于第一种情况需要进一步说明，shell 在给变量赋值时不进行文件名替换（这从第三种情况中也能看出来），各步骤发生的精确次序如下： shell 扫描命令行，把 x 的值设为星号 ＊； shell 再次扫描命令行，碰到星号 ＊，把它替换成当前目录下的文件清单；shell 启动执行 echo 命令，把文件清单作为参数传递给 echo。
这个赋值的先后次序非常重要：shell 先作变量替换，然后作文件名替换，最后把这行处理为参数。
3. 反引号(`) 命令替换是指 shell 能够将一个命令的标准输出插在一个命令行中任何位置。shell 中有两种方法作命令替换：把 shell 命令用反引号或者 $(...) 结构括起来，其中 $(...) 格式受到 POSIX 标准支持，也利于嵌套。
$ echo The date and time is `date` The date and time is 2016年 12月 28日 星期三 16:15:44 CST $ echo The date and time is $(date) The date and time is 2016年 12月 28日 星期三 16:15:44 CST $ echo Your current working directory is $(pwd) Your current working directory is /home/howard/script 4. 反斜杠 backslash-escaped( \ ) 反斜杠一般用作转义字符，或称逃脱字符。Linux 如果 echo 要让转义字符发生作用，就要使用 -e 选项，且转义字符要使用双引号。
echo -e &#34;\n&#34; 反斜杠的另一种作用，就是当反斜杠用于一行的最后一个字符时，shell 把行尾的反斜杠作为续行，这种结构在分几行输入长命令时经常使用。
原文地址 ]]></content></entry><entry><title>CentOS 安装 Nginx</title><url>/2016/12/23/CentOS-%E5%AE%89%E8%A3%85-Nginx/</url><categories><category>Linux</category></categories><tags><tag>Nginx</tag><tag>CentOS</tag><tag>Linux</tag></tags><content type="html">一、安装准备 首先由于 Nginx 的一些模块依赖一些 lib 库，所以在安装 Nginx 之前，必须先安装这些 lib 库，这些依赖库主要有 g++、gcc、openssl-devel、pcre-devel 和 zlib-devel，执行如下命令安装：
$ yum install gcc-c++ $ yum install pcre pcre-devel $ yum install zlib zlib-devel $ yum install openssl openssl--devel 二、安装 Nginx 安装之前，最好检查一下是否已经安装有 Nginx
$ find -name nginx 如果系统已经安装了 Nginx，那么就先卸载
$ yum remove nginx 首先进入 /usr/local 目录
$ cd /usr/local 从官网下载最新版的 Nginx
$ wget http://nginx.org/download/nginx-1.9.6.tar.gz $ tar -zxvf nginx-1.9.6.tar.gz $ cd nginx-1.9.6 接下来安装，使用 --prefix 参数指定 Nginx 安装的目录
$ ./configure --prefix=/usr/local/nginx # 指定 Nginx 安装的目录 /usr/local/nginx $ make prefix=/usr/local/nginx $ make install prefix=/usr/local/nginx 如果没有报错，顺利完成后，最好看一下 nginx 的安装目录
$ whereis nginx 安装完毕后，进入安装后目录（/usr/local/nginx）便可以启动或停止它了。
二、基本操作命令 2.1 启动命令 /usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf 2.2 重启命令 /usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf -s reload 2.3 停止命令 /usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf -s stop -c 制定配置文件的路径，如果不加 Nginx 会自动加载默认路径的配置文件。</content></entry><entry><title>CentOS 安装 rar、zip 解压缩</title><url>/2016/12/22/CentOS-%E5%AE%89%E8%A3%85-rarzip-%E8%A7%A3%E5%8E%8B%E7%BC%A9/</url><categories><category>Linux</category></categories><tags><tag>rar</tag><tag>zip</tag><tag>CentOS</tag><tag>Linux</tag></tags><content type="html">Windows 系统压缩的 rar 和 zip 文件，在 Linux 系统下是无法通过 tar 命令解压缩的，需要使用 rar 和 zip 命令来解压缩。下面记录一下 rar 和 zip 安装和简单的使用。
一、rar 安装使用 Linux 系统下使用 rarlinux 解压缩 rar 压缩文件，下载页面：http://www.rarsoft.com/download.htm。
1.1 下载系统对应的版本 $ wget http://www.rarsoft.com/rar/rarlinux-x64-5.4.0.tar.gz 1.2 解压、安装 $ tar -zxvf rarlinux-x64-5.4.0.tar.gz $ cd rar $ make 看见下面这些信息就是安装成功了：
mkdir -p /usr/local/bin mkdir -p /usr/local/lib cp rar unrar /usr/local/bin cp rarfiles.lst /etc cp default.sfx /usr/local/lib 1.3 常用 rar 命令 $ rar x centos.rar # 解压 centos.rar 到当前目录 $ rar centos.rar ./piaoyi.org/ # 将 piaoyi.org 目录打包为 centos.rar 1.4 常见错误原因分析 1.4.1 如果在运行命令 rar 时,出现下面这个问题 rar: /lib/i686/nosegneg/libc.so.6: version &amp;#39;GLIBC_2.7&amp;#39; not found (required by rar) 解决办法：
$ cp rar_static /usr/local/bin/rar 1.4.2 使用 rar 的时候出现错误 bash: /usr/local/bin/rar: /lib/ld-linux.so.2: bad ELF interpreter: No such file or directory 因为 64 位系统中安装了 32 位程序，解决方法：
$ yum install glibc.i686 1.4.3 重新安装 glibc.i686 以后还有如下类似错误 error while loading shared libraries: libstdc++.so.6: cannot open shared object file: No such file or directory 再继续安装包：
$ yum install libstdc++.so.6 二、zip/unzip 安装使用 2.1 检查是否有包含 zip（unzip） 的软件包 $ yum provides zip Loaded plugins: fastestmirror Loading mirror speeds from cached hostfile * base: mirrors.cqu.edu.cn * extras: mirrors.cqu.edu.cn * updates: mirrors.tuna.tsinghua.edu.cn zip-3.0-11.el7.x86_64 : A file compression and packaging utility compatible with PKZIP Repo : base zip-3.0-11.el7.x86_64 : A file compression and packaging utility compatible with PKZIP Repo : @base 2.2 安装 zip、unzip $ yum install zip $ yum install unzip 2.3 常用命令 $ zip -r myfile.zip ./* # 将当前目录下的所有文件和文件夹全部压缩成 myfile.zip 文件，-r 表示递归压缩子目录下所有文件. $ unzip -o -d /home/sunny myfile.zip # 把 myfile.zip 文件解压到 /home/sunny/ # -o ：不提示的情况下覆盖文件 # -d：将文件解压缩到指定目录下 $ zip -d myfile.zip smart.txt # 删除压缩文件中 smart.txt 文件 $ zip -m myfile.zip ./rpm_info.txt # 向压缩文件中 myfile.zip 中添加 rpm_info.txt 文件</content></entry><entry><title>Vim Tab 设置为 4 个空格</title><url>/2016/12/02/Vim-Tab-%E8%AE%BE%E7%BD%AE%E4%B8%BA-4-%E4%B8%AA%E7%A9%BA%E6%A0%BC/</url><categories><category>开发工具</category></categories><tags><tag>Vim</tag></tags><content type="html">在 vim 的配置文件中（Liunx：/etc/vimrc）中添加以下代码后，重启 vim 即可实现按 TAB 产生 4 个空格：
set ts=4 # 注：ts 是 tabstop 的缩写，设 TAB 宽 4 个空格 set expandtab 对于已保存的文件，可以使用下面的方法进行空格和 TAB 的替换：
TAB 替换为空格：
:set ts=4 :set expandtab :%retab! 空格替换为 TAB：
:set ts=4 :set noexpandtab :%retab! 加 ! 是用于处理非空白字符之后的 TAB，即所有的 TAB，若不加 !，则只处理行首的 TAB。</content></entry><entry><title>忙忙碌碌的十一月份</title><url>/2016/11/30/%E5%BF%99%E5%BF%99%E7%A2%8C%E7%A2%8C%E7%9A%84%E5%8D%81%E4%B8%80%E6%9C%88%E4%BB%BD/</url><categories><category>杂谈</category></categories><tags><tag>杂谈</tag></tags><content type="html">早就有计划在西安买房，一直觉得还早，主要是没钱，近期有人肯借钱就想看看市场。买房需要做很多准备工作，本想着慢慢先看着，一边从同学朋友那里打听消息，一边从网上看各个楼盘信息，怎奈越看越着急，就想一下子赶紧买了。看中的两个楼盘，价格其实都远远超过自己的预算和目前能承受的能力，不得不说我是一个容易被蛊惑的人，听着置业顾问的天花乱坠的描述，有好几次都差点都掉坑里了。
不得不吐槽ZF的户口、档案制度，不知道坑了多少人。上大学的时候响应学校的号召，傻乎乎的把户口从老家迁到了学校，现在也没想明白把户口迁过去是为了什么。毕业后二不拉几的嫌麻烦就随大流没有及时把户口、档案迁回去。买房按揭要户口，才意识到这个大坑不尽早填上只会越来越麻烦。为了搞清楚怎样能把户口迁回去，我战战兢兢的打通了相关部门的服务电话号（还好还好，才打了几十次就有人接听了），服务态度和传说中的“为人民服务”一样，果然没让我失望，折腾了整整一天，总算搞明白了大概该怎么弄。还好有同学在，不用自己再去跑一趟，准备好相关的资料邮寄过去就只能祈祷了&amp;hellip;
听说十二月份房价要涨，今天最后一天了，中介也“劝告”了我好几次赶紧出手，自己也心动了无数次。十一月份马上结束了，纠结的心该平静了些了，终究还是没有出手&amp;hellip;</content></entry><entry><title>[转] Linux 查看系统内核版本和发行版本</title><url>/2016/11/18/%E8%BD%AC-Linux-%E6%9F%A5%E7%9C%8B%E7%B3%BB%E7%BB%9F%E5%86%85%E6%A0%B8%E7%89%88%E6%9C%AC%E5%92%8C%E5%8F%91%E8%A1%8C%E7%89%88%E6%9C%AC/</url><categories><category>Linux</category></categories><tags><tag>Linux</tag><tag>Linux版本</tag></tags><content type="html">一、查看 Linux 内核版本命令 以下两条命令适合所有 Linux 系统。
1、cat /proc/version $ cat /proc/version Linux version 3.10.0-327.el7.x86_64 (builder@kbuilder.dev.centos.org) (gcc version 4.8.3 20140911 (Red Hat 4.8.3-9) (GCC) ) #1 SMP Thu Nov 19 22:10:57 UTC 2015 ➜ ~ cat /proc/version Linux version 4.4.0-36-generic (buildd@lcy01-01) (gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.2) ) #55-Ubuntu SMP Thu Aug 11 18:01:55 UTC 2016 2、uname -a $ uname -a Linux centos 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux ➜ ~ uname -a Linux www 4.4.0-36-generic #55-Ubuntu SMP Thu Aug 11 18:01:55 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux 二、查看 Linux 系统版本的命令 1、lsb_release -a $ lsb_release -a LSB Version: :core-4.1-amd64:core-4.1-noarch Distributor ID: CentOS Description: CentOS Linux release 7.2.1511 (Core) Release: 7.2.1511 Codename: Core ➜ ~ lsb_release -a No LSB modules are available. Distributor ID: Ubuntu Description: Ubuntu 16.04.1 LTS Release: 16.04 Codename: xenial 这个命令适用于所有的 Linux 发行版，包括 Redhat、SuSE、Debian… 等发行版。
有的系统中默认并没有安装 lsb_release，需要安装。下面介绍一下 CentOS 系统中安装方法。
首先查找 lsb_release 安装包：
$ yum provides lsb_release 已加载插件：fastestmirror Loading mirror speeds from cached hostfile * base: mirrors.btte.net * extras: mirrors.btte.net * updates: mirrors.btte.net redhat-lsb-core-4.1-27.el7.centos.1.i686 : LSB Core module support 源 ：base 匹配来源： 文件名 ：/usr/bin/lsb_release 安装：
$ sudo yum install -y redhat-lsb-core 2、cat /etc/redhat-release 这种方法只适合查看 Redhat 系的 Linux，如：CentOS。
$ cat /etc/redhat-release CentOS Linux release 7.2.1511 (Core) 3、cat /etc/issue 此命令适用于所有的 Linux 发行版。
➜ ~ cat /etc/issue Ubuntu 16.04.1 LTS \n \l</content></entry><entry><title>Test daocloud CI</title><url>/2016/11/18/Test-daocloud-CI/</url><categories><category>持续构建</category></categories><tags><tag>持续构建</tag></tags><content type="html">Test daocloud CI</content></entry><entry><title>[转] Git 版本回滚</title><url>/2016/11/07/%E8%BD%AC-Git-%E7%89%88%E6%9C%AC%E5%9B%9E%E6%BB%9A/</url><categories><category>Git</category></categories><tags><tag>Git</tag></tags><content type="html"><![CDATA[ 总有一天你会遇到下面的问题：
改完代码匆忙提交，上线发现有问题，怎么办？赶紧回滚。 改完代码测试也没有问题，但是上线发现你的修改影响了之前运行正常的代码报错，必须回滚。 这些开发中很常见的问题，所以 git 的取消提交，回退甚至返回上一版本都是特别重要的。大致分为下面 2 种情况：
一、没有 push 这种情况发生在你的本地代码仓库，可能你 add，commit 以后发现代码有点问题，准备取消提交，用到下面命令回退到某个版本：
git reset [--soft | --mixed | --hard] 上面常见三种类型
mixed 会保留源码，只是将 git commit 和 index 信息回退到了某个版本。
git reset 默认是 --mixed 模式 git reset --mixed 等价于 git reset soft 保留源码，只回退到 commit 信息到某个版本。不涉及 index 的回退，如果还需要提交，直接 commit 即可。
hard 源码也会回退到某个版本，commit 和 index 都回回退到某个版本。（注意，这种方式是改变本地代码仓库源码）
二、已经 push 2.1 方法一 使用 git reset --hard &lt;commit...&gt; 回退本地代码到某个版本。
使用 git push --force 将本次变更强行推送至服务器（因为你线上的代码没有变，线上 commit，index 都没有变，所以必须使用强制推送）。这样在服务器上的最后一次错误提交也彻底消失了。
**注意：**这样操作比较比较危险，例如：在你的 commit 之后别人又提交了新的 commit，那在你强制推送之后，那位仁兄的 commit 也跟着一起消失了，所以不建议这样操作。
2.2 方法二 使用以下命令：
git revert &lt;commit-ish&gt;... git revert 用于反转提交，用一个新提交来撤销某次提交，执行 revert 命令时要求工作树必须是干净的。
revert 之后你再 git push 既可以把线上的代码更新。（这里不会像 reset 造成冲突的问题）
revert 使用需要先找到你想回滚版本唯一的 commit 标识代码，可以用 git log 或者在 adgit 搭建的 web 环境历史提交记录里查看。
git revert c011eb3c20ba6fb38cc94fe5a8dda366a3990c61 通常，前几位即可：
git revert c011eb3 三、revert 与 reset 比较 git revert 是用一次新的 commit 来回滚之前的 commit，git reset 是直接删除指定的 commit。
看似达到的效果是一样的，其实完全不同。
3.1 上面我们说的如果你已经 push 到线上代码库， reset 删除指定 commit 以后，你 git push 可能导致一大堆冲突。但是 revert 并不会。
3.2 如果在日后现有分支和历史分支需要合并的时候，reset 恢复部分的代码依然会出现在历史分支里。但是 revert 方向提交的 commit 并不会出现在历史分支里。
3.3 reset 是在正常的 commit 历史中，删除了指定的 commit，这时 HEAD 是向后移动了，而 revert 是在正常的 commit 历史中再 commit 一次，只不过是反向提交，他的 HEAD 是一直向前的。
参考文章地址 ]]></content></entry><entry><title>Docker 学习笔记</title><url>/2016/11/03/Docker-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url><categories><category>Docker</category></categories><tags><tag>Docker</tag></tags><content type="html"><![CDATA[
一、CentOS 系统安装 参考《Docker — 从入门到实践》 之 &ldquo;CentOS 操作系统安装 Docker&rdquo; 1.1 系统要求 Docker 最低支持 CentOS 7。
Docker 需要安装在 64 位的平台，并且内核版本不低于 3.10。 CentOS 7 满足最低内核的要求，但由于内核版本比较低，部分功能（如 overlay2 存储层驱动）无法使用，并且部分功能可能不太稳定。
参考升级 CentOS 升级 kernel 1.2 使用脚本自动安装 Docker 官方为了简化安装流程，提供了一套安装脚本，CentOS 系统上可以使用这套脚本安装：
curl -sSL https://get.docker.com/ | sh 执行这个命令后，脚本就会自动的将一切准备工作做好，并且把 Docker 安装在系统中。
不过，由于伟大的墙的原因，在国内使用这个脚本可能会出现某些下载出现错误的情况。国内的一些云服务商提供了这个脚本的修改版本，使其使用国内的 Docker 软件源镜像安装，这样就避免了墙的干扰。
1.2.1 阿里云的安装脚本 curl -sSL http://acs-public-mirror.oss-cn-hangzhou.aliyuncs.com/docker-engine/internet | sh - 1.2.2 DaoCloud 的安装脚本 curl -sSL https://get.daocloud.io/docker | sh 1.3 手动安装 1.3.1 添加内核参数 默认配置下，在 CentOS 使用 Docker 可能会碰到下面的这些警告信息：
WARNING: bridge-nf-call-iptables is disabled WARNING: bridge-nf-call-ip6tables is disabled 添加内核配置参数以启用这些功能。
$ sudo tee -a /etc/sysctl.conf &lt;&lt;-EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF 然后重新加载 sysctl.conf 即可
$ sudo sysctl -p 1.3.2 添加 yum 源 虽然 CentOS 软件源 Extras 中有 Docker，名为 docker，但是不建议使用系统源中的这个版本，它的版本相对比较陈旧，而且并非 Docker 官方维护的版本。因此，我们需要使用 Docker 官方提供的 CentOS 软件源。
执行下面的命令添加 yum 软件源。
$ sudo tee /etc/yum.repos.d/docker.repo &lt;&lt;-&#39;EOF&#39; [dockerrepo] name=Docker Repository baseurl=https://yum.dockerproject.org/repo/main/centos/7/ enabled=1 gpgcheck=1 gpgkey=https://yum.dockerproject.org/gpg EOF 1.3.3 安装 Docker 更新 yum 软件源缓存，并安装 docker-engine。
$ sudo yum update $ sudo yum install docker-engine 1.3.4 启动 Docker 引擎 $ sudo systemctl enable docker $ sudo systemctl start docker 1.3.5 建立 docker 用户组 默认情况下，docker 命令会使用 Unix socket 与 Docker 引擎通讯。而只有 root 用户和 docker 组的用户才可以访问 Docker 引擎的 Unix socket。出于安全考虑，一般 Linux 系统上不会直接使用 root 用户。因此，更好地做法是将需要使用 docker 的用户加入 docker 用户组。
建立 docker 组：
$ sudo groupadd docker 将当前用户加入 docker 组：
$ sudo usermod -aG docker $USER 1.4 参考文档 参见 Docker 官方 CentOS 安装文档 。
二、基本概念 2.1 Docker images Docker image 是一个只读类型的模板。比如一个镜像可以是一个包含 apache 和你的 web 应用的 ubuntu 操作系统。我们经常使用镜像来创建容器。Docker 提供了一种快捷的方式来构建新镜像或者更新镜像，同时你也可以下载其他人已经创建好的镜像。Docker image 是 Docker 结构中的构建组件。
2.2 Docker Registries Docker registries 用来保存镜像。它分为公开仓库和私有仓库，你可以从仓库中上传或者下载镜像。公开的 Docker 仓库称之为 Docker Hub 。它提供了你可以使用的非常多的镜像。你可以自由的创建镜像或者使用这里面其他人已经创建好的镜像。Docker registries 属于 Docker 中的分发组件。
2.3 Docker containers Docker containers 同目录有几分相似。Docker containers 保存了执行应用所需的所有资源。每一个 Docker containers 都是由 image 创建的。Docker containers 可以 run, start, stop, restart, rm。需要注意的是，Docker containers 之间是隔离的。Docker containers 属于 Docker 中的执行组件。
三、常用命令 3.1 镜像操作 拉取镜像：docker pull nginx:latest 查看镜像列表：docker images 删除镜像：docker rmi &lt;image id&gt; 想要删除 untagged images（也就是那些 id 为 None 的镜像）：docker rmi $(docker images | grep &quot;^&lt;none&gt;&quot; | awk '{print $3}') 删除全部镜像：docker rmi $(docker images -q) 3.2 创建容器 创建一个容器并运行，例如：
docker run --name blog --privileged=true -v /home/Hexo/public:/usr/share/nginx/html:ro -d -p 80:80 docker.io/nginx --name：容器名称 --privileged=true：设置权限 -v：挂载本地磁盘目录，格式为本地磁盘绝对路径:容器路径 保证删除容器后，数据不被删除
:ro：只读 -d：后台模式 i：交互式操作 t：终端 -p：设置宿主计算机和容器端口映射 在Dockerfile 中有一条指令是 EXPOSE 22，如果使用 -P，宿主机会随机选择一个 没有被使用的端口 和 docker 容器的 22 端口 做 端口映射，如果 docker 主机或者容器重启后，宿主机又会随机选择一个没有被使用的端口和 docker 容器的 22 端口做端口映射，这样端口会发生 变化
如果使用 -p，比如 2222:22，这样不管是 docker 主机或者容器重启后，2222:22 端口都是这样来映射，不会发生改变
3.3 容器操作 启动：docker start blog 停止：docker stop blog；停止所有容器：docker stop $(docker ps -a -q) 删除：docker rm blog；删除所有容器：docker rm $(docker ps -a -q) 查看：docker ps -a（查看所有容器，包括已启动的和未启动的） 3.3 进入容器 sudo docker exec -it blog /bin/bash 四、Docker 私有仓库 Docker 提供了一个中央仓库，同时也允许我们使用 registry 搭建本地私有仓库。
4.1 搭建 Docker 私有仓库 首先拉取 registry 镜像
docker pull registry 创建 register 容器并运行
docker run --name registry -d -p 5000:5000 --privileged=true -v /opt/registry:/var/lib/registry/ registry 挂载容器中存放镜像的目录到本地 /opt/registry，需要注意的一点是，容器中存放镜像的目录 registry 官方镜像示例中使用的是 /tmp/registry-dev ，但实验证明在版本 2.5.0 中，目录是 /var/lib/registry/。
CentOS 系统挂载目录需要加上 --privileged=true 解决挂载的目录没有权限的问题。
打开浏览器输入 register 容器宿主计算机地址加端口（如：http://192.168.237.128:5000/v2/ ），如果出现 {}，即表明 register 容器运行成功。
4.2 测试使用私有仓库 首先给需要 push 到仓库的 images 打 TAG，前面需要带上私有仓库的地址。
$ docker tag docker.io/registry:latest 192.168.237.128:5000/registry:2.5.0 push 到私有仓库
$ docker push 192.168.237.128:5000/registry:2.5.0 通过以下命令删除镜像 192.168.237.128:5000/registry:2.5.0：
$ docker rmi 192.168.237.128:5000/registry:2.5.0 然后通过以下命令即可拉取私有仓库的镜像：
$ docker pull 192.168.237.128:5000/hexo:latest 如果 docker push 的时候出现以下错误信息：
$ docker push 192.168.237.128:5000/registry:2.5.0 The push refers to a repository [192.168.237.128:5000/registry] unable to ping registry endpoint https://192.168.237.128:5000/v0/ v2 ping attempt failed with error: Get https://192.168.237.128:5000/v2/: http: server gave HTTP response to HTTPS client v1 ping attempt failed with error: Get https://192.168.237.128:5000/v1/_ping: http: server gave HTTP response to HTTPS client 修改文件 /etc/sysconfig/docker 添加以下代码，然后重启 docker 服务即可重新 push。
ADD_REGISTRY=&#39;--add-registry 192.168.237.128:5000&#39; INSECURE_REGISTRY=&#39;--insecure-registry 192.168.237.128:5000&#39; 五、Dockerfile 5.1 什么是Dockerfile？ Dockerfile 是自动构建 docker 镜像的配置文件，Dockerfile 中的命令非常类似 linux shell 下的命令 Dockerfile，可以让用户自定义构建 docker 镜像，支持以 # 开头的注释行
一般，Dockerfile分为4部分
基础镜像（父镜像）信息 维护者信息 镜像操作命令 容器启动命令 5.2 Dockerfile 介绍 FROM centos:centos7.1.1503
基于 父镜像 构建其他 docker 镜像，父镜像：可以通过 docker pull 命令获得，也可以自己制作
MAINTAINER Carson,C.J.Zeong &lt;zcy@nicescale.com&gt;
Dockerfile 维护者
ENV TZ &quot;Asia/Shanghai&quot;
ENV（environment）设置环境变量，一个 Dockerfile 中可以写多个。以上例子是：设置 docker 容器的时区为 Shanghai
Dockerfile 中有 2 条指令可以拷贝文件
ADD aliyun-mirror.repo /etc/yum.repos.d/CentOS-Base.repo
拷贝本地文件到 docker 容器里，还可以拷贝 URL 链接地址下的文件，ADD 还具有解压软件包的功能（支持 gzip, bzip2 or xz）
COPY test /mydir
拷贝本地文件到 docker 容器
RUN yum install -y curl wget....
RUN 命令，非常类似 Linux 下的 shell 命令 (the command is run in a shell - /bin/sh -c - shell form) 在 Dockerfile 中每执行一条指令（ENV、ADD、RUN等命令），都会生成一个 docker image layer
ADD supervisord.conf /etc/supervisord.conf
添加 supervisor 的主配置文件，到 docker 容器里
EXPOSE 22
端口映射 EXPOSE &lt;host_port&gt;:&lt;container_port&gt; 推荐使用 docker run -p &lt;host_port&gt;:&lt;container_port&gt; 来固化端口
ENTRYPOINT [&quot;/usr/bin/supervisord&quot;, &quot;-n&quot;, &quot;-c&quot;, &quot;/etc/supervisord.conf&quot;]
一个 Dockerfile 中只有最后一条 ENTRYPOINT 生效，并且每次启动 docker 容器，都会执行 ENTRYPOINT
ONBUILD ADD . /app
ONBUILD 在生成当前docker镜像的时候不生效，在子镜像生效；ONBUILD在产品发布时起着非常重要的作用！举例 A镜像中有ONBUILD指令，在构建A镜像时ONBUILD指令不执行；B镜像FROM A,在构建B镜像时ONBUILD指令开始执行；
VOLUME [&quot;/var/lib/mysql&quot;]
指定 docker 容器和宿主机做映射的文件目录
5.3 生成 docker 镜像 以上文件就是用来生成第一个 docker 镜像的 Dockerfile，通过 docker build 指令来生成 docker 镜像
docker build -t csphere/centos:7.1 .
如果 Dockerfile 在当前目录下，输入点 . 就可以进行加载当前目录下的 Dockerfile 如果不在当前目录下需要运行 docker build -t csphere/centos:7.1 &lt;Dockerfile_dir&gt; 加载相对路径下的 Dockerfile
docker 镜像的命名规则 registry_url/namespace/image_name:tag 默认 tag 是 latest
在构建 Docker 镜像时，如果有自己内部的 yum 源，替换成自己内部的 yum 地址，可以加快构建速度。 如果第一次构建失败，会有部分镜像 layer 生成，第二次构建会基于第一次构建所生成的 layer（use cache），继续构建
Step 10 : EXPOSE 22 ---&gt; Running in 0ed1c5479ebc ---&gt; c57a5bac41c8 Removing intermediate container 0ed1c5479ebc Step 11 : ENTRYPOINT /usr/bin/supervisord -n -c /etc/supervisord.conf ---&gt; Running in e16c7ac2fd45 ---&gt; 185ef7b101a8 Removing intermediate container e16c7ac2fd45 Successfully built 185ef7b101a8 可以看到每执行一条 Dockerfile 的指令都会生成一个镜像的 layer c57a5bac41c8 185ef7b101a8 最后 185ef7b101a8 这个是 docker 镜像的ID，185ef7b101a8 是由 c57a5bac41c8 185ef7b101a8&hellip;layers 叠加而成，体现了 docker 镜像是分层的
未完待续...]]></content></entry><entry><title>测试使用 daocloud 持续构建 Hexo</title><url>/2016/11/02/%E6%B5%8B%E8%AF%95%E4%BD%BF%E7%94%A8-daocloud-%E6%8C%81%E7%BB%AD%E6%9E%84%E5%BB%BA-Hexo/</url><categories><category>持续构建</category></categories><tags><tag>持续构建</tag><tag>Daocloud</tag></tags><content type="html">测试使用 daocloud 持续构建 Hexo {% note danger %} Coding Hexo DaoCloud {% endnote %} {% note primary %} 看到这边博文，表示持续构建成功！！！ {% endnote %}</content></entry><entry><title>测试 Travis CI 结合 Hexo 使用</title><url>/2016/11/02/%E6%B5%8B%E8%AF%95-Travis-CI-%E7%BB%93%E5%90%88-Hexo-%E4%BD%BF%E7%94%A8/</url><categories><category>持续构建</category></categories><tags><tag>持续构建</tag><tag>Travis CI</tag></tags><content type="html">测试 Travis CI 结合 Hexo 使用</content></entry><entry><title>热爱微小的改变</title><url>/2016/10/31/%E7%83%AD%E7%88%B1%E5%BE%AE%E5%B0%8F%E7%9A%84%E6%94%B9%E5%8F%98/</url><categories><category>杂谈</category></categories><tags><tag>杂谈</tag></tags><content type="html"> 当你有着强烈的情绪时，可能是因为你要明哲保身。你不愿冒险因为你害怕被拒绝、害怕失败、害怕自己hold不住。真的，试想下如果你彻底的失败了，甚至在完全陌生的人面前流泪你会怎么做？也许你不相信你会采取合理的风险。也许你曾经打破常规去挑战自我的极限，但是结果却并不令你满意。但卖掉你所有的财产，搬去墨西哥并非我今天所谈论的改变类型。
也许你会害怕改变，虽然你也不知道其中的原因。一想到沉溺于同样的地方遇见同样的人，遵循例行的生活是一种最不愉快的体验。
的确如此，你每天在同一时间起床之后重复同样的行为。你吃着一成不变的食物去同一家电影院。你约会同样的朋友甚至在每周同一时间。很多情况下常规和结构化的模式有助于情绪管理。虽然可预测性是你行为方式的一部分，但如果这部分所占比例太高，则会挤压你生活中的乐趣。
探索新体验是增加幸福感的方式之一。新体验并不需要太大的改变。或许你可以探索下你所在城市的自己不熟悉的地方，品尝些新食物，做一道新菜，阅读一些你平时并非爱好的杂志，或者去参加一些你从未体验过的活动。你也可以走进一家专卖亚洲食品的商店，或者到公园里参加一些游戏活动。新体验能够培养兴趣或者参与甚至取代你之前熟知的意识。当在熟知的范围内发生适当的改变，会刺激你的活力和幸福感。你看孩子们始终拥有对世界的好奇心和迷恋感，因为对于他们来说什么都是新的。所以当你寻求新体验的时候你也能保持一份好奇感。
当你想做一些全新的不同的事情时候，通常会面临已有的惯性思维。你给自己各种借口待在房子里，重复自己昨天甚至前天同样的事情。你告诉自己学习新东西太麻烦了，或者你根本就不喜欢轮滑，那么还为什么要熬夜到很晚，甚至失去自己呼呼大睡的机会呢？这种消极的心理暗示总是非常具有说服力，你会认为你年纪太大没必要再去学习骑骆驼，或者如果你已经在博物馆工作那么你可能接下来的一辈子都会留在这里。
从事新活动是你可以学习的行为。给自己创造一些新体验。如果有朋友愿意和你一起，这将是非常有帮助的。刚开始你可能会觉得不舒服但是一定要坚持下去。直到在这个水平内你感到习惯然后再继续往下走。把你想做的事情列一个清单，保证一周一次并坚持四个月。有时你会厌烦这很正常，所以一定要坚持下去。培养兴趣和保持活力是非常值得做的事情。
译者：小太阳123 作者：KARYN HALL,PHD</content></entry><entry><title>看懂的人都还在加班中...</title><url>/2016/10/27/%E7%9C%8B%E6%87%82%E7%9A%84%E4%BA%BA%E9%83%BD%E8%BF%98%E5%9C%A8%E5%8A%A0%E7%8F%AD%E4%B8%AD.../</url><categories><category>杂谈</category></categories><tags><tag>程序员幽默</tag></tags><content type="html"> 以下是关于程序员的一些笑话，据说看懂的人都还在加班中。
0、老婆给当程序员的老公打电话：下班顺路买十个包子，如果看到卖西瓜的，买一个。当晚老公手捧一个包子进了家门。老婆怒道：你怎么只买一个包子？！老公甚恐，喃喃道：因为我真看到卖西瓜的了。
1、一程序员去面试，面试官问：“你毕业才两年，这三年工作经验是怎么来的？！”程序员答：“加班。”
2、某程序员对书法十分感兴趣，退休后决定在这方面有所建树。于是花重金购买了上等的文房四宝。一日，饭后突生雅兴，一番磨墨拟纸，并点上了上好的檀香，颇有王羲之风范，又具颜真卿气势，定神片刻，泼墨挥毫，郑重地写下一行字：hello world。
3、问：程序员最讨厌康熙的哪个儿子。答：胤禩。因为他是八阿哥（bug）
4、程序猿要了3个孩子，分别取名叫Ctrl、Alt 和 Delete，如果他们不听话，程序猿就只要同时敲他们一下就会好的。
5、今天在公司听到一句惨绝人寰骂人的话：“你 TM 就是一个没有对象的野指针！”
6、程xx遭遇车祸成植物人，医生说她活下来的希望只有万分之一，唤醒更为渺茫。她的同事和亲人没放弃，并根据程xx对 testing 痴迷的作风，每天都在她身边念：“你测的模块上线后回滚了。”奇迹发生了，程xx醒来第一句话：确认那模块是我测的？
7、一个程序员在海滨游泳时溺水身亡。他死前拼命的呼救，当时海滩上有许多救生员，但是没有人救他。因为他一直大喊“F1!”“F1!”，谁都不知道“F1”究竟是什么意思。
8、世界上最远的距离，是我在 if 里你在 else 里，虽然经常一起出现，但却永不结伴执行。
9、正在码代码 ing，医院回来的同事一脸的苦逼样子，问他怎么了？他回答：得了类风湿性关节炎了，我怕会遗传给下一代啊。我一脸的问号：谁说类风湿性关节炎能遗传的？丫一脸诧异：类不是继承的吗？
10、我很奇怪客栈这个词，难道后入住的必须先退房吗？
11、话说，决定一个程序员跳槽与否的关键因素是他前同事的现工资。
12、程序员最憋屈的事情就是：你辛辛苦苦熬夜写了一个风格优雅的源文件，被一个代码风格极差的同事改了且没署名，以至于别人都以为你是写的。
13、前端工程师说，我去交友网站找女朋友去了。朋友问，找到了么？工程师说，找到了他们页面的一个 bug`。
14、C 程序看不起 C++ 程序员， C++ 程序员看不起 Java 程序员， Java 程序员看不起 C# 程序员，C# 程序员看不起美工，周末了，美工带着妹子出去约会了，一群程序员还在加班！
15、据说一老外年轻的时候，立志要当一名伟大的作家。怎么才算伟大呢？他说：我写的东西全世界都要看到！看完他们必定会歇斯底里！会火冒三丈！会痛苦万分！结果，他成功了，他在微软公司负责写系统蓝屏时的报错提示信息。
16、程序员应聘必备词汇：了解＝听过名字；熟悉＝知道是啥；熟练＝用过；精通＝做过东西。
17、两程序员聊天，程序员甲抱怨：“做程序员太辛苦了，我想换行……我该怎么办？”程序员乙：“敲一下回车。”
18、程序员最讨厌的四件事：写注释、写文档、别人不写注释、别人不写文档……
19、假如生活欺骗了你，找 50 个程序员问问为什么编程；假如生活让你想死，找 50 个程序员问问 Bug 改完了没有；假如你觉得生活拮据，找 50 个程序员问问工资涨了没有；假如你觉得活着无聊，找 50 个程序员问问他们一天都干了什么！
20、男人要记住，与女人吵架的要领是，要像在安装软件或注册网站时阅读 服务条款 那样，直接忽略所有的内容，到最后面勾选 我同意，然后点击 确定。
21、朋友今天遇到的真事：客户说我们设备卡，死活找不到原因，工程师赶到现场，给客户换了个鼠标垫，故障排除……
22、产品经理：“你明白吧，这里向右划可以出菜单，然后需要一个闪烁的动画，还有，我想这个tab可以拉下来，你懂吧？ 设计师：“别废话，把你要抄的产品给我看下。”
23、百度研发的无人驾驶汽车，你会发现有些地方你是去不了的。腾讯研发的无人驾驶汽车，你会发现很多地方你要去是要黄钻会员的。当然，谷歌研发的无人驾驶汽车，你会发现查无此车。
24、话说昨天是周日，程序猿跟产品经理一起看电视。每个节目看到一半程序猿就换台，看到一半就换台，几次之后产品经理终于忍无可忍的咆哮：老子刚看出点意思你就换、刚看出点意思你就换，到底还让不让人看啦？！程序猿淡定的盯着电视道：你半路改需求的时候我可没吱过声！
25、“为什么删除手机上的图标的时候它们会抖？” “它们怕被删除呗。” “那为什么电话短信之类系统自带的删不掉的也在抖呢？” “那是它们在得瑟…”
26、有时候觉得，电脑就像一个高贵冷艳的妹纸。 400，是她冷冰冰地说：“我听不懂你在说什么”； 401，是她无情地转身：“我不认识你，别说那些奇怪的话”； 403，是她残酷的拒绝：“我听懂你的话，也认出你的脸，可我不爱你”； 404，是她紧闭心门：“我这儿没有你想要的东西”； 503，是“呵呵我去洗澡”。
27、问：为何软件正在占领全世界，而程序员得不到尊重？答曰：遍身罗绮者，不是养蚕人。
28、她来例假了肚子疼，他坐着她旁边，看了她一眼，拿出手机玩游戏，她看在眼里，心里凉了半截。两分钟后，她实在坐不下去了，正准备离开，只见他默默地递过来他的小米手机说：拿去捂着。
29、惊闻微软要裁员 1.8 万人。他们就不能让这些员工们“在后台运行”吗？
30、今天看到我同事在笔记本电脑上实现 滑动解锁 ！真的，看到我当场就震精了，异常碉堡……你知道吗？！他的电脑开机解锁密码是 ASDFGHJKL;’ ，然后唰得一下过去最后一个键落在回车上，就，就解锁了！
31、骗子网站太特么多了，你一打开，必定跳出一个很下流的游戏广告！——但这还不是最关键的，关键是如果你忍不住点击进入游戏之后，就会发现这些下流的元素全都不见了！这也太没有职业道德了吧！
32、最近发现自己陷入了一种状态，叫开机迷失。开电脑前，该做什么清清楚楚。但只要一登录，随手开个网页，刷下新闻微博甚至仅仅是音乐网站，然后再回过神来，已经到了深夜，要做的事一项没做。而最痛苦的是，在睡前闭眼的时候，整个被浪费的白天和那些被耽误的正事，总会掺杂着负罪感，无比清晰地浮现。
33、我要设计一个新的智能机应用程序叫做 惊慌，只要你一说 老婆 这两个字, 它就会关闭所有网页, 隐藏所有跟女性的聊天以及怪怪的文件夹, 并且把我老婆的照片设为墙纸。
34、我从苹果手机的背面知道了一个好地方叫做加利福尼亚，据说那里有阳光和沙滩，还有 Google 和 Tesla。据说那里的人经常讨论怎么创业，而不是怎么移民。我虽然在中国，但也去过那里，不是在梦中，而是通过 VPN。
35、边上工位的妹纸叫柳依依，她的爸爸也是位程序员，依依的大姐叫玲玲，二姐叫玲依，三姐叫依玲
整理自网络</content></entry><entry><title>Windows 10 删除、恢复资源管理器中“视频、图片”等文件夹</title><url>/2016/10/25/Windows-10-%E5%88%A0%E9%99%A4%E6%81%A2%E5%A4%8D%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E5%99%A8%E4%B8%AD%E8%A7%86%E9%A2%91%E5%9B%BE%E7%89%87%E7%AD%89%E6%96%87%E4%BB%B6%E5%A4%B9/</url><categories><category>开发工具</category></categories><tags><tag>Windows 10</tag></tags><content type="html"> Windows 10 资源管理器中的&amp;quot;视频、图片、文档、下载、音乐、桌面&amp;quot;等文件夹目前通过一般途径是删除不了的，只能通过注册表的删除删掉，本文介绍一种比较简单的删除、恢复方法。
准备 打开注册表（Win+R 输入 regedit）找到以下键： HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\ 单击 NameSpace 右键 导出，保存文件后缀名为 .reg。
删除 用文本编辑器打开该文件，只保留类似以下内容，并且在行首添加 - 号，然后保存。
Windows Registry Editor Version 5.00 [-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{088e3905-0323-4b02-9826-5d99428e115f}] [-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{1CF1260C-4DD0-4ebb-811F-33C572699FDE}] [-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{24ad3ad4-a569-4530-98e1-ab02f9417aa8}] [-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{374DE290-123F-4565-9164-39C4925E467B}] [-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{3ADD1653-EB32-4cb0-BBD7-DFA0ABB5ACCA}] [-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{3dfdf296-dbec-4fb4-81d1-6a3438bcf4de}] [-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{A0953C92-50DC-43bf-BE83-3742FED03C9C}] [-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{A8CDFF1C-4878-43be-B5FD-F8091C1C60D0}] [-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{B4BFCC3A-DB2C-424C-B029-7FE99A87C641}] [-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{d3162b92-9365-467a-956b-92703aca08af}] [-HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\MyComputer\NameSpace\{f86fa3ab-70d2-4fc7-9c99-fcbf05467f3a}] 双击该文件即可删除。
恢复 想要恢复，编辑文件，删除行首 - 号，保存，双击运行即可恢复。</content></entry><entry><title>设置 Sublime Text3 主题透明</title><url>/2016/10/24/%E8%AE%BE%E7%BD%AE-Sublime-Text3-%E4%B8%BB%E9%A2%98%E9%80%8F%E6%98%8E/</url><categories><category>开发工具</category></categories><tags><tag>Sublime</tag></tags><content type="html"><![CDATA[1. 下载安装包 下载地址： SublimeTextTrans 。
2. 安装插件 解压到 Sublime Text3 的 Packages 存放目录下，命名为：SublimeTextTrans。点击 Sublime Text3 的 Preferences -&gt; Browse Packages 打开 Packages 存放目录，一般情况是在 C:\Users\YOURNAME\AppData\Roaming\Sublime Text 3\Packages 目录下。
3. 设置 Sublime Text3 的透明度级别。 点击 Sublime Text3 的 Preferences -&gt; Package Setting -&gt; SublimeTextTrans -&gt; Setting - User 就可以设置透明度的级别了。
也可以通过 Ctrl+Shift+1、2、3、4、5、6 加载预设的 6 个透明级别。预设的透明级别在 Preferences -&gt; Package Setting -&gt; SublimeTextTrans -&gt; Setting - Default 中可以查看。
]]></content></entry><entry><title>[转]用 Markdown 写印象笔记（Evernote）</title><url>/2016/10/19/%E8%BD%AC%E7%94%A8-Markdown-%E5%86%99%E5%8D%B0%E8%B1%A1%E7%AC%94%E8%AE%B0Evernote/</url><categories><category>开发工具</category></categories><tags><tag>Markdown</tag><tag>Evernote</tag></tags><content type="html"><![CDATA[使用印象笔记很久了，什么都觉得很好，就是不支持 Markdown 书写语法，实在是太遗憾了。今天发现网上有人介绍了一款 Sublime 的一个插件 Evernote ，尝试了一下觉得还是一个不错的方案，正好我也很喜欢使用 Sublime 编辑器，如此甚好，哈哈&hellip;
{% img https://cdn.jsdelivr.net/gh/0vo/oss/images/2016101901.png 400 %}
一、安装 1.1 用 Sublime 的 PackageControl 安装 Evernote 插件 1.2 设置 Sublime 与印象笔记做关联 国内印象笔记用户打开链接： https://app.yinxiang.com/api/DeveloperToken.action ，国际 Evernote 用户打开链接： https://www.evernote.com/api/DeveloperToken.action 。然后点击页面按钮 Create a developer token 生成开发者秘钥。
打开 Sublime Preferences -&gt; Package Settings -&gt; Evernote -&gt; Settings - User 在文件中贴入如下内容：
{ &#34;noteStoreUrl&#34;: &#34;&#34;, &#34;token&#34;: &#34;&#34; } noteStoreUrl 和 token 值为之前打开的页面的上的值，然后保存。
测试是否成功：通过快捷键 ctrl+shift+p 打开 Sublime 命令窗口，输入 evernote，就会看见 Evernote 的许多命令，点击 evernote:list recent notes，如果看到罗列出最新的笔记，则说明授权成功。
二、快捷键设置 插件默认没有添加快捷键，但可以自己配置。通过快捷键 ctrl+shift+p 打开 Sublime 命令窗口，输入 key binding，选择 User 那，写入你的内容。
下面是我的设置：
[ { &#34;keys&#34;: [&#34;Ctrl+e&#34;, &#34;Ctrl+o&#34;], &#34;command&#34;: &#34;open_evernote_note&#34; }, { &#34;keys&#34;: [&#34;Ctrl+s&#34;], &#34;command&#34;: &#34;save_evernote_note&#34;, &#34;context&#34;: [{&#34;key&#34;: &#34;evernote_note&#34;}, {&#34;key&#34;: &#34;evernote_has_guid&#34;}] }, { &#34;keys&#34;: [&#34;Ctrl+s&#34;], &#34;command&#34;: &#34;send_to_evernote&#34;, &#34;context&#34;: [{&#34;key&#34;: &#34;evernote_note&#34;}, {&#34;key&#34;: &#34;evernote_has_guid&#34;, &#34;operator&#34;: &#34;equal&#34;, &#34;operand&#34;: false}] } ] **解释：**意思是，按 ctrl+e, o 后，会打开印象笔记，按 ctrl+s 会将笔记保存并且同步到印象笔记。
三、常用命令 通过快捷键 ctrl+shift+p，打开 Sublime 命令窗口就可以使用一些常用命令了。
Evernote: New empty note：创建笔记 Evernote: Open Evernote Note：打开印象笔记 Evernote: List recent notes：打开最近笔记 Evernote: Search note：搜索笔记 四、个人配置 主要更改了代码的字体和颜色，需要注意的是替换掉 noteStoreUrl 和 token 值为步骤 2 中获取内容。点击 Sublime Text 的 Preferences -&gt; Package Settings -&gt; Evernote -&gt; Settings - User 粘贴以下内容：
{ &#34;noteStoreUrl&#34;: &#34;更换为步骤 2 中获取的 noteStoreUrl&#34;, &#34;token&#34;: &#34;更换为步骤 2 中获取的 token&#34;, &#34;inline_css&#34;: { &#34;body&#34;: &#34;&#34;, &#34;pre&#34;: &#34;color: #000000; font-family: Consolas,monospace; font-size: 0.9em; white-space: pre-wrap; word-wrap: break-word; border: 1px solid #cccccc; border-radius: 3px; overflow: auto; padding: 6px 10px; margin-bottom: 10px;&#34;, &#34;code&#34;: &#34;color: black; font-family: Consolas,monospace; font-size: 1.1em;&#34;, &#34;inline-code&#34;: &#34;color: #c7254e; font-family: Consolas,monospace; padding: 0.1em 0.2em; margin: 0.1em; font-size: 85%; background-color: #f9f2f4; border-radius: 3px; border: 1px solid #d6d6d6;&#34;, &#34;h1&#34;: &#34;margin-bottom: 1em; margin-top: 1.2em;&#34;, &#34;footnotes&#34;: &#34;border-top: 1px solid #9AB39B; font-size: 80%;&#34;, &#34;hr&#34;: &#34;color:#9AB39B;background-color:#9AB39B;height:1px;border:none;&#34;, &#34;sup&#34;: &#34;color:#6D6D6D;font-size:1ex&#34;, &#34;blockquote&#34;: &#34;border-left: .5ex solid #BFBFBF; margin-left: 0px; padding-left: 1em; margin-top: 1.4285em; margin-bottom: 1.4285em;&#34;, &#34;table&#34;: &#34;border-collapse: collapse; border-spacing: 0; margin: 1em;&#34;, &#34;td&#34;: &#34;border: 1px solid #DDD; padding: 6px 13px;&#34;, &#34;th&#34;: &#34;border: 1px solid #DDD; padding: 6px 13px;&#34;, &#34;tr:odd&#34;: &#34;border: 1px solid #DDD; padding: 6px 13px;&#34;, &#34;tr:even&#34;: &#34;border: 1px solid #DDD; padding: 6px 13px; background-color: #F8F8F8;&#34; }, &#34;code_highlighting_style&#34;: &#34;github&#34;, &#34;code_friendly&#34;: true, &#34;gfm_tables&#34;: true, &#34;wiki_tables&#34;: false, &#34;emphasis_mark&#34;: &#34;_&#34;, &#34;strong_mark&#34;: &#34;**&#34;, &#34;item_mark&#34;: &#34;*&#34;, &#34;notes_order&#34;: &#34;updated&#34;, &#34;max_notes&#34;: 100, &#34;update_on_save&#34;: false, &#34;evernote_autocomplete&#34;: true, &#34;sort_notebooks&#34;: false, &#34;show_stacks&#34;: true, &#34;open_single_result&#34;: true, &#34;tab_prefix&#34;: &#34;Evernote: &#34;, &#34;warn_on_close&#34;: true } 原文地址 ]]></content></entry><entry><title>[转] Kafka入门教程</title><url>/2016/10/10/%E8%BD%AC-Kafka%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/</url><categories><category>Java开发技术</category></categories><tags><tag>Kafka</tag></tags><content type="html"><![CDATA[一、基本概念 1. 介绍 Kafka 是一个分布式的、可分区的、可复制的消息系统。它提供了普通消息系统的功能，但具有自己独特的设计。这个独特的设计是什么样的呢？
{% img https://cdn.jsdelivr.net/gh/0vo/oss/images/kafka-logo.png 400 %}
首先让我们看几个基本的消息系统术语：
Kafka 将消息以 topic 为单位进行归纳。 将向 Kafka topic 发布消息的程序成为 producers。 将预订 topics 并消费消息的程序成为 consumer。 Kafka 以集群的方式运行，可以由一个或多个服务组成，每个服务叫做一个 broker。 producers 通过网络将消息发送到 Kafka 集群，Kafka 集群向消费者提供消息，如下图所示： 客户端和服务端通过 TCP 协议通信。Kafka 提供了 Java 客户端，并且对多种语言都提供了支持。
2. Topics 和 Logs 先来看一下 Kafka 提供的一个抽象概念：topic。一个 topic 是对一组消息的归纳。对每个 topic，Kafka 对它的日志进行了分区，如下图所示：
{% img https://cdn.jsdelivr.net/gh/0vo/oss/images/20161009002.png 400 %}
每个分区都由一系列有序的、不可变的消息组成，这些消息被连续的追加到分区中。分区中的每个消息都有一个连续的序列号叫做 offset，用来在分区中唯一的标识这个消息。
在一个可配置的时间段内，Kafka 集群保留所有发布的消息，不管这些消息有没有被消费。比如，如果消息的保存策略被设置为 2 天，那么在一个消息被发布的两天时间内，它都是可以被消费的。之后它将被丢弃以释放空间。Kafka 的性能是和数据量无关的常量级的，所以保留太多的数据并不是问题。
实际上每个 consumer 唯一需要维护的数据是消息在日志中的位置，也就是 offset。这个 offset 有 consumer 来维护：一般情况下随着 consumer 不断的读取消息，这 offset 的值不断增加，但其实 consumer 可以以任意的顺序读取消息，比如它可以将 offset 设置成为一个旧的值来重读之前的消息。
以上特点的结合，使 Kafka consumers 非常的轻量级：它们可以在不对集群和其他 consumer 造成影响的情况下读取消息。你可以使用命令行来 tail 消息而不会对其他正在消费消息的 consumer 造成影响。
将日志分区可以达到以下目的：首先这使得每个日志的数量不会太大，可以在单个服务上保存。另外每个分区可以单独发布和消费，为并发操作 topic 提供了一种可能。
3. 分布式 每个分区在 Kafka 集群的若干服务中都有副本，这样这些持有副本的服务可以共同处理数据和请求，副本数量是可以配置的。副本使 Kafka 具备了容错能力。
每个分区都由一个服务器作为 leader，零或若干服务器作为 followers，leader 负责处理消息的读和写，followers 则去复制 leader。如果 leader down 了，followers 中的一台则会自动成为 leader。集群中的每个服务都会同时扮演两个角色：作为它所持有的一部分分区的 leader，同时作为其他分区的 followers，这样集群就会据有较好的负载均衡。
4. Producers Producer 将消息发布到它指定的 topic 中,并负责决定发布到哪个分区。通常简单的由负载均衡机制随机选择分区，但也可以通过特定的分区函数选择分区。使用的更多的是第二种。
5. Consumers 发布消息通常有两种模式：队列模式（queuing）和发布-订阅模式（publish-subscribe）。
队列模式中 consumers 可以同时从服务端读取消息，每个消息只被其中一个 consumer 读到。 发布-订阅模式中消息被广播到所有的 consumer 中。 Consumers 可以加入一个 consumer 组，共同竞争一个 topic，topic 中的消息将被分发到组中的一个成员中。同一组中的 consumer 可以在不同的程序中，也可以在不同的机器上。如果所有的 consumer 都在一个组中，这就成为了传统的队列模式，在各 consumer 中实现负载均衡。如果所有的 consumer 都不在不同的组中，这就成为了发布-订阅模式，所有的消息都被分发到所有的 consumer 中。更常见的是，每个 topic 都有若干数量的 consumer 组，每个组都是一个逻辑上的 “订阅者”，为了容错和更好的稳定性，每个组由若干 consumer 组成。这其实就是一个发布-订阅模式，只不过订阅者是个组而不是单个consumer。
相比传统的消息系统，Kafka 可以很好的保证有序性。 传统的队列在服务器上保存有序的消息，如果多个 consumers 同时从这个服务器消费消息，服务器就会以消息存储的顺序向 consumer 分发消息。虽然服务器按顺序发布消息，但是消息是被异步的分发到各 consumer 上，所以当消息到达时可能已经失去了原来的顺序，这意味着并发消费将导致顺序错乱。为了避免故障，这样的消息系统通常使用 专用consumer 的概念，其实就是只允许一个消费者消费消息，当然这就意味着失去了并发性。
在这方面 Kafka 做的更好，通过分区的概念，Kafka 可以在多个 consumer 组并发的情况下提供较好的有序性和负载均衡。将每个分区分只分发给一个 consumer 组，这样一个分区就只被这个组的一个 consumer 消费，就可以顺序的消费这个分区的消息。因为有多个分区，依然可以在多个 consumer 组之间进行负载均衡。
注意： consumer 组的数量不能多于分区的数量，也就是有多少分区就允许多少并发消费。
Kafka 只能保证一个分区之内消息的有序性，在不同的分区之间是不可以的，这已经可以满足大部分应用的需求。如果需要 topic 中所有消息的有序性，那就只能让这个 topic 只有一个分区，当然也就只有一个 consumer 组消费它。
二、环境搭建 Step 1：下载 Kafka 点击 下载最新的版本并解压。
$ tar -zxvf kafka_2.11-0.10.0.1.tgz -C ~/apps $ cd kafka_2.11-0.10.0.1 Step 2：启动服务 Kafka 用到了 Zookeeper，所有首先启动 Zookeeper，下面简单的启用一个单实例的 Zookkeeper 服务。可以在命令的结尾加个 &amp; 符号，这样就可以启动后离开控制台。
$ ./bin/zkServer.sh start ZooKeeper JMX enabled by default Using config: /home/ehlxr/apps/zookeeper-3.4.9/bin/../conf/zoo.cfg Starting zookeeper ... STARTED 现在启动Kafka:
$ bin/kafka-server-start.sh config/server.properties [2013-04-22 15:01:47,028] INFO Verifying properties (kafka.utils.VerifiableProperties) [2013-04-22 15:01:47,051] INFO Property socket.send.buffer.bytes is overridden to 1048576 (kafka.utils.VerifiableProperties) Step 3：创建 topic 创建一个叫做 “test” 的 topic，它只有一个分区，一个副本。
$ bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test 可以通过 list 命令查看创建的 topic：
$ bin/kafka-topics.sh --list --zookeeper localhost:2181 test 除了手动创建 topic，还可以配置 broker 让它自动创建 topic。
Step 4：发送消息 Kafka 使用一个简单的命令行 producer，从文件中或者从标准输入中读取消息并发送到服务端。默认的每条命令将发送一条消息。
运行 producer 并在控制台中输一些消息，这些消息将被发送到服务端：
$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test This is a messageThis is another message ctrl+c 可以退出发送。
Step 5：启动 consumer Kafka 也有一个命令行 consumer 可以读取消息并输出到标准输出：
$ bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning This is a message This is another message 你在一个终端中运行 consumer 命令行，另一个终端中运行 producer 命令行，就可以在一个终端输入消息，另一个终端读取消息。
这两个命令都有自己的可选参数，可以在运行的时候不加任何参数可以看到帮助信息。
Step 6：搭建一个多个 broker 的集群 刚才只是启动了单个 broker，现在启动有 3 个 broker 组成的集群，这些 broker 节点也都是在本机上的。
首先为每个节点编写配置文件：
$ cp config/server.properties config/server-1.properties $ cp config/server.properties config/server-2.properties 在拷贝出的新文件中添加以下参数：
config/server-1.properties: broker.id=1 port=9093 log.dir=/tmp/kafka-logs-1 config/server-2.properties: broker.id=2 port=9094 log.dir=/tmp/kafka-logs-2 broker.id 在集群中唯一的标注一个节点，因为在同一个机器上，所以必须制定不同的端口和日志文件，避免数据被覆盖。
刚才已经启动可 Zookeeper 和一个节点，现在启动另外两个节点：
$ bin/kafka-server-start.sh config/server-1.properties &amp; $ bin/kafka-server-start.sh config/server-2.properties &amp; 创建一个拥有 3 个副本的 topic：
$ bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topic 现在我们搭建了一个集群，怎么知道每个节点的信息呢？运行 describe topics 命令就可以了：
$ bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic 下面解释一下这些输出。第一行是对所有分区的一个描述，然后每个分区都会对应一行，因为我们只有一个分区所以下面就只加了一行。
leader：负责处理消息的读和写，leader是从所有节点中随机选择的。 replicas：列出了所有的副本节点，不管节点是否在服务中。 isr：是正在服务中的节点。 在我们的例子中，节点 1 是作为 leader 运行。
向 topic 发送消息：
$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-replicated-topic my test message 1my test message 2 消费这些消息：
$ bin/kafka-console-consumer.sh --zookeeper localhost:2181 --from-beginning --topic my-replicated-topic my test message 1 my test message 2 测试一下容错能力，Broker 1 作为 leader 运行，现在我们 kill 掉它：
$ ps | grep server-1.properties7564 ttys002 0:15.91 /System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/bin/java... $ kill -9 7564 另外一个节点被选做了 leader，node 1 不再出现在 in-sync 副本列表中：
$ bin/kafka-topics.sh --describe --zookeeper localhost:218192 --topic my-replicated-topic Topic:my-replicated-topic PartitionCount:1 ReplicationFactor:3 Configs: Topic: my-replicated-topic Partition: 0 Leader: 2 Replicas: 1,2,0 Isr: 2,0 虽然最初负责续写消息的 leader down 掉了，但之前的消息还是可以消费的：
$ bin/kafka-console-consumer.sh --zookeeper localhost:2181 --from-beginning --topic my-replicated-topic ... my test message 1 my test message 2 看来 Kafka 的容错机制还是不错的。
三、搭建Kafka开发环境 我们搭建了 kafka 的服务器，并可以使用 Kafka 的命令行工具创建 topic，发送和接收消息。下面我们来搭建 kafka 的开发环境。
1. 添加依赖 搭建开发环境需要引入 kafka 的 jar 包，一种方式是将 Kafka 安装包中 lib 下的 jar 包加入到项目的 classpath 中，这种比较简单了。不过我们使用另一种更加流行的方式：使用 maven 管理 jar 包依赖。
创建好 maven 项目后，在 pom.xml 中添加以下依赖：
&lt;dependency&gt; &lt;groupId&gt; org.apache.kafka&lt;/groupId &gt; &lt;artifactId&gt; kafka_2.10&lt;/artifactId &gt; &lt;version&gt; 0.8.0&lt;/ version&gt; &lt;/dependency&gt; 添加依赖后你会发现有两个 jar 包的依赖找不到。没关系我都帮你想好了，点击这里下载这两个 jar 包，解压后你有两种选择，第一种是使用 mvn 的 install 命令将 jar 包安装到本地仓库，另一种是直接将解压后的文件夹拷贝到 mvn 本地仓库的 com 文件夹下，比如我的本地仓库是 d:\mvn，完成后我的目录结构是这样的：
2. 配置程序 首先是一个充当配置文件作用的接口,配置了 Kafka 的各种连接参数：
package com.sohu.kafkademon; public interface KafkaProperties { final static String zkConnect = &#34;10.22.10.139:2181&#34;; final static String groupId = &#34;group1&#34;; final static String topic = &#34;topic1&#34;; final static String kafkaServerURL = &#34;10.22.10.139&#34;; final static int kafkaServerPort = 9092; final static int kafkaProducerBufferSize = 64 * 1024; final static int connectionTimeOut = 20000; final static int reconnectInterval = 10000; final static String topic2 = &#34;topic2&#34;; final static String topic3 = &#34;topic3&#34;; final static String clientId = &#34;SimpleConsumerDemoClient&#34;; } 3. Producer package com.sohu.kafkademon; import java.util.Properties; import kafka.producer.KeyedMessage; import kafka.producer.ProducerConfig; public class KafkaProducer extends Thread { private final kafka.javaapi.producer.Producer&lt;Integer, String&gt; producer; private final String topic; private final Properties props = new Properties(); public KafkaProducer(String topic) { props.put(&#34;serializer.class&#34;, &#34;kafka.serializer.StringEncoder&#34;); props.put(&#34;metadata.broker.list&#34;, &#34;10.22.10.139:9092&#34;); producer = new kafka.javaapi.producer.Producer&lt;Integer, String&gt;(new ProducerConfig(props)); this.topic = topic; } @Override public void run() { int messageNo = 1; while (true) { String messageStr = new String(&#34;Message_&#34; + messageNo); System.out.println(&#34;Send:&#34; + messageStr); producer.send(new KeyedMessage&lt;Integer, String&gt;(topic, messageStr)); messageNo++; try { sleep(3000); } catch (InterruptedException e) { // TODO Auto-generated catch block e.printStackTrace(); } } } } 4. Consumer package com.sohu.kafkademon; import java.util.HashMap; import java.util.List; import java.util.Map; import java.util.Properties; import kafka.consumer.ConsumerConfig; import kafka.consumer.ConsumerIterator; import kafka.consumer.KafkaStream; import kafka.javaapi.consumer.ConsumerConnector; public class KafkaConsumer extends Thread { private final ConsumerConnector consumer; private final String topic; public KafkaConsumer(String topic) { consumer = kafka.consumer.Consumer.createJavaConsumerConnector( createConsumerConfig()); this.topic = topic; } private static ConsumerConfig createConsumerConfig() { Properties props = new Properties(); props.put(&#34;zookeeper.connect&#34;, KafkaProperties.zkConnect); props.put(&#34;group.id&#34;, KafkaProperties.groupId); props.put(&#34;zookeeper.session.timeout.ms&#34;, &#34;40000&#34;); props.put(&#34;zookeeper.sync.time.ms&#34;, &#34;200&#34;); props.put(&#34;auto.commit.interval.ms&#34;, &#34;1000&#34;); return new ConsumerConfig(props); } @Override public void run() { Map&lt;String, Integer&gt; topicCountMap = new HashMap&lt;String, Integer&gt;(); topicCountMap.put(topic, new Integer(1)); Map&lt;String, List&lt;KafkaStream&lt;byte[], byte[]&gt;&gt;&gt; consumerMap = consumer.createMessageStreams(topicCountMap); KafkaStream&lt;byte[], byte[]&gt; stream = consumerMap.get(topic).get(0); ConsumerIterator&lt;byte[], byte[]&gt; it = stream.iterator(); while (it.hasNext()) { System.out.println(&#34;receive：&#34; + new String(it.next().message())); try { sleep(3000); } catch (InterruptedException e) { e.printStackTrace(); } } } } 5. 简单的发送接收 运行下面这个程序，就可以进行简单的发送接收消息了：
package com.sohu.kafkademon; public class KafkaConsumerProducerDemo { public static void main(String[] args) { KafkaProducer producerThread = new KafkaProducer(KafkaProperties.topic); producerThread.start(); KafkaConsumer consumerThread = new KafkaConsumer(KafkaProperties.topic); consumerThread.start(); } } 6. 高级别的consumer 下面是比较负载的发送接收的程序：
package com.sohu.kafkademon; import java.util.HashMap; import java.util.List; import java.util.Map; import java.util.Properties; import kafka.consumer.ConsumerConfig; import kafka.consumer.ConsumerIterator; import kafka.consumer.KafkaStream; import kafka.javaapi.consumer.ConsumerConnector; public class KafkaConsumer extends Thread { private final ConsumerConnector consumer; private final String topic; public KafkaConsumer(String topic) { consumer = kafka.consumer.Consumer.createJavaConsumerConnector( createConsumerConfig()); this.topic = topic; } private static ConsumerConfig createConsumerConfig() { Properties props = new Properties(); props.put(&#34;zookeeper.connect&#34;, KafkaProperties.zkConnect); props.put(&#34;group.id&#34;, KafkaProperties.groupId); props.put(&#34;zookeeper.session.timeout.ms&#34;, &#34;40000&#34;); props.put(&#34;zookeeper.sync.time.ms&#34;, &#34;200&#34;); props.put(&#34;auto.commit.interval.ms&#34;, &#34;1000&#34;); return new ConsumerConfig(props); } @Override public void run() { Map&lt;String, Integer&gt; topicCountMap = new HashMap&lt;String, Integer&gt;(); topicCountMap.put(topic, new Integer(1)); Map&lt;String, List&lt;KafkaStream&lt;byte[], byte[]&gt;&gt;&gt; consumerMap = consumer.createMessageStreams(topicCountMap); KafkaStream&lt;byte[], byte[]&gt; stream = consumerMap.get(topic).get(0); ConsumerIterator&lt;byte[], byte[]&gt; it = stream.iterator(); while (it.hasNext()) { System.out.println(&#34;receive：&#34; + new String(it.next().message())); try { sleep(3000); } catch (InterruptedException e) { e.printStackTrace(); } } } } 四、数据持久化 1. 不要畏惧文件系统! Kafka 大量依赖文件系统去存储和缓存消息。对于硬盘有个传统的观念是硬盘总是很慢，这使很多人怀疑基于文件系统的架构能否提供优异的性能。实际上硬盘的快慢完全取决于使用它的方式。设计良好的硬盘架构可以和内存一样快。
在 6 块 7200 转的 SATA RAID-5 磁盘阵列的线性写速度差不多是 600MB/s，但是随即写的速度却是 100k/s，差了差不多6000倍。现代的操作系统都对次做了大量的优化，使用了 read-ahead 和 write-behind 的技巧，读取的时候成块的预读取数据，写的时候将各种微小琐碎的逻辑写入组织合并成一次较大的物理写入。对此的深入讨论可以查看这里，它们发现线性的访问磁盘，很多时候比随机的内存访问快得多。
为了提高性能，现代操作系统往往使用内存作为磁盘的缓存，现代操作系统乐于把所有空闲内存用作磁盘缓存，虽然这可能在缓存回收和重新分配时牺牲一些性能。所有的磁盘读写操作都会经过这个缓存，这不太可能被绕开除非直接使用 I/O。所以虽然每个程序都在自己的线程里只缓存了一份数据，但在操作系统的缓存里还有一份，这等于存了两份数据。
另外再来讨论一下 JVM，以下两个事实是众所周知的：
Java对象占用空间是非常大的，差不多是要存储的数据的两倍甚至更高。 随着堆中数据量的增加，垃圾回收回变的越来越困难。 基于以上分析，如果把数据缓存在内存里，因为需要存储两份，不得不使用两倍的内存空间，Kafka 基于 JVM，又不得不将空间再次加倍,再加上要避免 GC 带来的性能影响，在一个 32G 内存的机器上，不得不使用到 28-30G 的内存空间。并且当系统重启的时候，又必须要将数据刷到内存中（10GB 内存差不多要用10分钟），就算使用冷刷新（不是一次性刷进内存，而是在使用数据的时候没有就刷到内存）也会导致最初的时候新能非常慢。但是使用文件系统，即使系统重启了，也不需要刷新数据。使用文件系统也简化了维护数据一致性的逻辑。
所以与传统的将数据缓存在内存中然后刷到硬盘的设计不同，Kafka 直接将数据写到了文件系统的日志中。
2. 常量时间的操作效率 在大多数的消息系统中，数据持久化的机制往往是为每个 cosumer 提供一个 B 树或者其他的随机读写的数据结构。B 树当然是很棒的，但是也带了一些代价：比如 B 树的复杂度是 O(log N)，O(log N) 通常被认为就是常量复杂度了，但对于硬盘操作来说并非如此。磁盘进行一次搜索需要 10ms，每个硬盘在同一时间只能进行一次搜索，这样并发处理就成了问题。虽然存储系统使用缓存进行了大量优化，但是对于树结构的性能的观察结果却表明，它的性能往往随着数据的增长而线性下降，数据增长一倍，速度就会降低一倍。
直观的讲，对于主要用于日志处理的消息系统，数据的持久化可以简单的通过将数据追加到文件中实现，读的时候从文件中读就好了。这样做的好处是读和写都是 O(1) 的，并且读操作不会阻塞写操作和其他操作。这样带来的性能优势是很明显的，因为性能和数据的大小没有关系了。
既然可以使用几乎没有容量限制（相对于内存来说）的硬盘空间建立消息系统，就可以在没有性能损失的情况下提供一些一般消息系统不具备的特性。比如，一般的消息系统都是在消息被消费后立即删除，Kafka 却可以将消息保存一段时间（比如一星期），这给 consumer 提供了很好的机动性和灵活性，这点在今后的文章中会有详述。
五、消息传输的事务定义 之前讨论了 consumer 和 producer 是怎么工作的，现在来讨论一下数据传输方面。数据传输的事务定义通常有以下三种级别：
最多一次: 消息不会被重复发送，最多被传输一次，但也有可能一次不传输。 最少一次: 消息不会被漏发送，最少被传输一次，但也有可能被重复传输。 精确的一次（Exactly once）: 不会漏传输也不会重复传输,每个消息都传输被一次而且仅仅被传输一次，这是大家所期望的。 大多数消息系统声称可以做到 “精确的一次”，但是仔细阅读它们的的文档可以看到里面存在误导，比如没有说明当 consumer 或 producer 失败时怎么样，或者当有多个 consumer 并行时怎么样，或写入硬盘的数据丢失时又会怎么样。kafka 的做法要更先进一些。当发布消息时，Kafka 有一个 “committed” 的概念，一旦消息被提交了，只要消息被写入的分区的所在的副本 broker 是活动的，数据就不会丢失。关于副本的活动的概念，下节文档会讨论。现在假设 broker 是不会 down 的。
如果 producer 发布消息时发生了网络错误，但又不确定实在提交之前发生的还是提交之后发生的，这种情况虽然不常见，但是必须考虑进去，现在 Kafka 版本还没有解决这个问题，将来的版本正在努力尝试解决。
并不是所有的情况都需要 “精确的一次” 这样高的级别，Kafka 允许 producer 灵活的指定级别。比如 producer 可以指定必须等待消息被提交的通知，或者完全的异步发送消息而不等待任何通知，或者仅仅等待 leader 声明它拿到了消息（followers 没有必要）。
现在从 consumer 的方面考虑这个问题，所有的副本都有相同的日志文件和相同的 offset，consumer 维护自己消费的消息的 offset，如果 consumer 不会崩溃当然可以在内存中保存这个值，当然谁也不能保证这点。如果 consumer 崩溃了，会有另外一个 consumer 接着消费消息，它需要从一个合适的 offset 继续处理。这种情况下可以有以下选择：
consumer 可以先读取消息，然后将 offset 写入日志文件中，然后再处理消息。这存在一种可能就是在存储 offset 后还没处理消息就 crash 了，新的 consumer 继续从这个 offset 处理，那么就会有些消息永远不会被处理，这就是上面说的 “最多一次”。 consumer 可以先读取消息，处理消息，最后记录 offset，当然如果在记录 offset 之前就 crash 了，新的 consumer 会重复的消费一些消息，这就是上面说的 “最少一次”。 “精确一次” 可以通过将提交分为两个阶段来解决：保存了 offset 后提交一次，消息处理成功之后再提交一次。但是还有个更简单的做法：将消息的 offset 和消息被处理后的结果保存在一起。比如用 Hadoop ETL 处理消息时，将处理后的结果和 offset 同时保存在 HDFS 中，这样就能保证消息和 offser 同时被处理了。 六、性能优化 Kafka 在提高效率方面做了很大努力。Kafka 的一个主要使用场景是处理网站活动日志，吞吐量是非常大的，每个页面都会产生好多次写操作。读方面，假设每个消息只被消费一次，读的量的也是很大的，Kafka 也尽量使读的操作更轻量化。
我们之前讨论了磁盘的性能问题，线性读写的情况下影响磁盘性能问题大约有两个方面：太多的琐碎的 I/O 操作和太多的字节拷贝。I/O 问题发生在客户端和服务端之间，也发生在服务端内部的持久化的操作中。
1. 消息集（message set） 为了避免这些问题，Kafka 建立了 “消息集（message set）” 的概念，将消息组织到一起，作为处理的单位。以消息集为单位处理消息，比以单个的消息为单位处理，会提升不少性能。Producer 把消息集一块发送给服务端，而不是一条条的发送；服务端把消息集一次性的追加到日志文件中，这样减少了琐碎的 I/O 操作。consumer 也可以一次性的请求一个消息集。
另外一个性能优化是在字节拷贝方面。在低负载的情况下这不是问题，但是在高负载的情况下它的影响还是很大的。为了避免这个问题，Kafka 使用了标准的二进制消息格式，这个格式可以在 producer，broker 和 producer 之间共享而无需做任何改动。
2. Zero Copy Broker 维护的消息日志仅仅是一些目录文件，消息集以固定队的格式写入到日志文件中，这个格式 producer 和 consumer 是共享的，这使得 Kafka 可以一个很重要的点进行优化：消息在网络上的传递。现代的 unix 操作系统提供了高性能的将数据从页面缓存发送到 socket 的系统函数，在 linux 中，这个函数是 sendfile。
为了更好的理解 sendfile 的好处，我们先来看下一般将数据从文件发送到 socket 的数据流向：
操作系统把数据从文件拷贝内核中的页缓存中 应用程序从页缓存从把数据拷贝自己的内存缓存中 应用程序将数据写入到内核中 socket 缓存中操作系统把数据从 socket 缓存中拷贝到网卡接口缓存，从这里发送到网络上。 这显然是低效率的，有 4 次拷贝和 2 次系统调用。Sendfile 通过直接将数据从页面缓存发送网卡接口缓存，避免了重复拷贝，大大的优化了性能。
在一个多 consumers 的场景里，数据仅仅被拷贝到页面缓存一次而不是每次消费消息的时候都重复的进行拷贝。这使得消息以近乎网络带宽的速率发送出去。这样在磁盘层面你几乎看不到任何的读操作，因为数据都是从页面缓存中直接发送到网络上去了。
这篇文章 详细介绍了 sendfile 和 zero-copy 技术在 Java 方面的应用。
3. 数据压缩 很多时候，性能的瓶颈并非 CPU 或者硬盘而是网络带宽，对于需要在数据中心之间传送大量数据的应用更是如此。当然用户可以在没有 Kafka 支持的情况下各自压缩自己的消息，但是这将导致较低的压缩率，因为相比于将消息单独压缩，将大量文件压缩在一起才能起到最好的压缩效果。
Kafka 采用了端到端的压缩：因为有 “消息集” 的概念，客户端的消息可以一起被压缩后送到服务端，并以压缩后的格式写入日志文件，以压缩的格式发送到 consumer，消息从 producer 发出到 consumer 拿到都被是压缩的，只有在 consumer 使用的时候才被解压缩，所以叫做 “端到端的压缩”。
Kafka 支持 GZIP 和 Snappy 压缩协议。更详细的内容可以查看 这里 。
七、Producer 和 Consumer 1. Kafka Producer 消息发送 producer 直接将数据发送到 broker 的 leader（主节点），不需要在多个节点进行分发。为了帮助 producer 做到这点，所有的 Kafka 节点都可以及时的告知：哪些节点是活动的，目标 topic 目标分区的 leader 在哪。这样 producer 就可以直接将消息发送到目的地了。
客户端控制消息将被分发到哪个分区。可以通过负载均衡随机的选择，或者使用分区函数。Kafka 允许用户实现分区函数，指定分区的 key，将消息 hash 到不同的分区上（当然有需要的话，也可以覆盖这个分区函数自己实现逻辑）。比如如果你指定的 key 是 user id，那么同一个用户发送的消息都被发送到同一个分区上。经过分区之后，consumer 就可以有目的的消费某个分区的消息。
2. 异步发送 批量发送可以很有效的提高发送效率。Kafka producer 的异步发送模式允许进行批量发送，先将消息缓存在内存中，然后一次请求批量发送出去。这个策略可以配置的，比如可以指定缓存的消息达到某个量的时候就发出去，或者缓存了固定的时间后就发送出去（比如 100 条消息就发送，或者每 5 秒发送一次）。这种策略将大大减少服务端的 I/O 次数。
既然缓存是在 producer 端进行的，那么当 producer 崩溃时，这些消息就会丢失。Kafka 0.8.1 的异步发送模式还不支持回调，就不能在发送出错时进行处理。Kafka 0.9 可能会增加这样的回调函数。见 Proposed Producer API 。
3. Kafka Consumer Kafa consumer 消费消息时，向broker发出 fetch 请求去消费特定分区的消息。consumer 指定消息在日志中的偏移量（offset），就可以消费从这个位置开始的消息。customer 拥有了 offset 的控制权，可以向后回滚去重新消费之前的消息，这是很有意义的。
4. 推还是拉？ Kafka 最初考虑的问题是，customer 应该从 brokes 拉取消息还是 brokers 将消息推送到 consumer，也就是 pull 还 push。在这方面，Kafka 遵循了一种大部分消息系统共同的传统的设计：producer 将消息推送到 broker，consumer 从 broker 拉取消息。
一些消息系统比如 Scribe 和 Apache Flume 采用了push 模式，将消息推送到下游的 consumer。这样做有好处也有坏处：由 broker 决定消息推送的速率，对于不同消费速率的 consumer 就不太好处理了。消息系统都致力于让 consumer 以最大的速率最快速的消费消息，但不幸的是，push 模式下，当 broker 推送的速率远大于 consumer 消费的速率时，consumer 恐怕就要崩溃了。最终 Kafka 还是选取了传统的 pull 模式。
Pull 模式的另外一个好处是 consumer 可以自主决定是否批量的从 broker 拉取数据。Push 模式必须在不知道下游 consumer 消费能力和消费策略的情况下决定是立即推送每条消息还是缓存之后批量推送。如果为了避免 consumer 崩溃而采用较低的推送速率，将可能导致一次只推送较少的消息而造成浪费。Pull 模式下，consumer 就可以根据自己的消费能力去决定这些策略。
Pull 有个缺点是，如果 broker 没有可供消费的消息，将导致 consumer 不断在循环中轮询，直到新消息到 t 达。为了避免这点，Kafka 有个参数可以让 consumer 阻塞知道新消息到达（当然也可以阻塞知道消息的数量达到某个特定的量这样就可以批量发送）。
5. 消费状态跟踪 对消费消息状态的记录也是很重要的。
大部分消息系统在 broker 端的维护消息被消费的记录：一个消息被分发到 consumer 后 broker 就马上进行标记或者等待 customer 的通知后进行标记。这样也可以在消息在消费后立马就删除以减少空间占用。
但是这样会不会有什么问题呢？如果一条消息发送出去之后就立即被标记为消费过的，一旦 consumer 处理消息时失败了（比如程序崩溃）消息就丢失了。为了解决这个问题，很多消息系统提供了另外一个个功能：当消息被发送出去之后仅仅被标记为已发送状态，当接到 consumer 已经消费成功的通知后才标记为已被消费的状态。这虽然解决了消息丢失的问题，但产生了新问题，首先如果 consumer 处理消息成功了但是向 broker 发送响应时失败了，这条消息将被消费两次。第二个问题时，broker 必须维护每条消息的状态，并且每次都要先锁住消息然后更改状态然后释放锁。这样麻烦又来了，且不说要维护大量的状态数据，比如如果消息发送出去但没有收到消费成功的通知，这条消息将一直处于被锁定的状态， Kafka 采用了不同的策略。Topic 被分成了若干分区，每个分区在同一时间只被一个 consumer 消费。这意味着每个分区被消费的消息在日志中的位置仅仅是一个简单的整数：offset。这样就很容易标记每个分区消费状态就很容易了，仅仅需要一个整数而已。这样消费状态的跟踪就很简单了。
这带来了另外一个好处：consumer 可以把 offset 调成一个较老的值，去重新消费老的消息。这对传统的消息系统来说看起来有些不可思议，但确实是非常有用的，谁规定了一条消息只能被消费一次呢？consumer发现解析数据的程序有 bug，在修改 bug 后再来解析一次消息，看起来是很合理的额呀！
6. 离线处理消息 高级的数据持久化允许 consumer 每个隔一段时间批量的将数据加载到线下系统中比如 Hadoop 或者数据仓库。这种情况下，Hadoop 可以将加载任务分拆，拆成每个 broker 或每个 topic 或每个分区一个加载任务。Hadoop 具有任务管理功能，当一个任务失败了就可以重启而不用担心数据被重新加载，只要从上次加载的位置继续加载消息就可以了。
八、主从同步 Kafka 允许 topic 的分区拥有若干副本，这个数量是可以配置的，你可以为每个 topic 配置副本的数量。Kafka 会自动在每个个副本上备份数据，所以当一个节点 down 掉时数据依然是可用的。
Kafka 的副本功能不是必须的，你可以配置只有一个副本，这样其实就相当于只有一份数据。
创建副本的单位是 topic 的分区，每个分区都有一个 leader 和零或多个 followers。所有的读写操作都由 leader 处理，一般分区的数量都比 broker 的数量多的多，各分区的 leader 均匀的分布在 brokers 中。所有的 followers 都复制 leader 的日志，日志中的消息和顺序都和 leader 中的一致。flowers 向普通的 consumer 那样从 leader 那里拉取消息并保存在自己的日志文件中。
许多分布式的消息系统自动的处理失败的请求，它们对一个节点是否存活着（alive）有着清晰的定义。Kafka 判断一个节点是否活着有两个条件：
节点必须可以维护和 ZooKeeper 的连接，Zookeeper 通过心跳机制检查每个节点的连接。 如果节点是个 follower，他必须能及时的同步 leader 的写操作，延时不能太久。 符合以上条件的节点准确的说应该是“同步中的（in sync）”，而不是模糊的说是 “活着的” 或是 “失败的”。Leader 会追踪所有 “同步中” 的节点，一旦一个 down 掉了，或是卡住了，或是延时太久，leader 就会把它移除。至于延时多久算是 “太久”，是由参数 replica.lag.max.messages 决定的，怎样算是卡住了，怎是由参数 replica.lag.time.max.ms 决定的。
只有当消息被所有的副本加入到日志中时，才算是 “committed”，只有 committed 的消息才会发送给 consumer，这样就不用担心一旦 leader down 掉了消息会丢失。Producer 也可以选择是否等待消息被提交的通知，这个是由参数 request.required.acks 决定的。
Kafka 保证只要有一个 “同步中” 的节点，“committed” 的消息就不会丢失。
1. Leader的选择 Kafka 的核心是日志文件，日志文件在集群中的同步是分布式数据系统最基础的要素。
如果leaders永远不会down的话我们就不需要followers了！一旦leader down掉了，需要在followers中选择一个新的leader.但是followers本身有可能延时太久或者crash，所以必须选择高质量的follower作为leader.必须保证，一旦一个消息被提交了，但是leader down掉了，新选出的leader必须可以提供这条消息。大部分的分布式系统采用了多数投票法则选择新的leader,对于多数投票法则，就是根据所有副本节点的状况动态的选择最适合的作为leader.Kafka并不是使用这种方法。
Kafaka动态维护了一个同步状态的副本的集合（a set of in-sync replicas），简称ISR，在这个集合中的节点都是和leader保持高度一致的，任何一条消息必须被这个集合中的每个节点读取并追加到日志中了，才回通知外部这个消息已经被提交了。因此这个集合中的任何一个节点随时都可以被选为leader.ISR在ZooKeeper中维护。ISR中有f+1个节点，就可以允许在f个节点down掉的情况下不会丢失消息并正常提供服。ISR的成员是动态的，如果一个节点被淘汰了，当它重新达到“同步中”的状态时，他可以重新加入ISR.这种leader的选择方式是非常快速的，适合kafka的应用场景。
一个邪恶的想法：如果所有节点都down掉了怎么办？Kafka对于数据不会丢失的保证，是基于至少一个节点是存活的，一旦所有节点都down了，这个就不能保证了。 实际应用中，当所有的副本都down掉时，必须及时作出反应。可以有以下两种选择:
等待ISR中的任何一个节点恢复并担任leader。 选择所有节点中（不只是ISR）第一个恢复的节点作为leader. 这是一个在可用性和连续性之间的权衡。如果等待ISR中的节点恢复，一旦ISR中的节点起不起来或者数据都是了，那集群就永远恢复不了了。如果等待ISR意外的节点恢复，这个节点的数据就会被作为线上数据，有可能和真实的数据有所出入，因为有些数据它可能还没同步到。Kafka目前选择了第二种策略，在未来的版本中将使这个策略的选择可配置，可以根据场景灵活的选择。 这种窘境不只Kafka会遇到，几乎所有的分布式数据系统都会遇到。
2. 副本管理 以上仅仅以一个topic一个分区为例子进行了讨论，但实际上一个Kafka将会管理成千上万的topic分区.Kafka尽量的使所有分区均匀的分布到集群所有的节点上而不是集中在某些节点上，另外主从关系也尽量均衡这样每个几点都会担任一定比例的分区的leader。
优化leader的选择过程也是很重要的，它决定了系统发生故障时的空窗期有多久。Kafka选择一个节点作为“controller”,当发现有节点down掉的时候它负责在游泳分区的所有节点中选择新的leader,这使得Kafka可以批量的高效的管理所有分区节点的主从关系。如果controller down掉了，活着的节点中的一个会备切换为新的controller.
九、客户端API 1. Kafka Producer APIs Kafka Procuder API 有两种，它们分别是：kafka.producer.SyncProducer 和 kafka.producer.async.AsyncProducer。它们都实现了同一个接口：
class Producer { /* 将消息发送到指定分区 */ publicvoid send(kafka.javaapi.producer.ProducerData&lt;K,V&gt; producerData); /* 批量发送一批消息 */ publicvoid send(java.util.List&lt;kafka.javaapi.producer.ProducerData&lt;K,V&gt;&gt; producerData); /* 关闭producer */ publicvoid close(); } Producer API提供了以下功能：
可以将多个消息缓存到本地队列里，然后异步的批量发送到broker，可以通过参数producer.type=async做到。缓存的大小可以通过一些参数指定：queue.time和batch.size。一个后台线程（(kafka.producer.async.ProducerSendThread）从队列中取出数据并让kafka.producer.EventHandler将消息发送到broker，也可以通过参数event.handler定制handler，在producer端处理数据的不同的阶段注册处理器，比如可以对这一过程进行日志追踪，或进行一些监控。只需实现kafka.producer.async.CallbackHandler接口，并在callback.handler中配置。 自己编写Encoder来序列化消息，只需实现下面这个接口。默认的Encoder是kafka.serializer.DefaultEncoder。 interface Encoder&lt;T&gt; { public Message toMessage(T data); } 提供了基于Zookeeper的broker自动感知能力，可以通过参数zk.connect实现。如果不使用Zookeeper，也可以使用broker.list参数指定一个静态的brokers列表，这样消息将被随机的发送到一个broker上，一旦选中的broker失败了，消息发送也就失败了。 通过分区函数kafka.producer.Partitioner类对消息分区。 interface Partitioner&lt;T&gt; { int partition(T key, int numPartitions); } 分区函数有两个参数：key和可用的分区数量，从分区列表中选择一个分区并返回id。默认的分区策略是hash(key)%numPartitions.如果key是null,就随机的选择一个。可以通过参数partitioner.class定制分区函数。 2. KafKa Consumer APIs Consumer API有两个级别。低级别的和一个指定的broker保持连接，并在接收完消息后关闭连接，这个级别是无状态的，每次读取消息都带着offset。
高级别的API隐藏了和brokers连接的细节，在不必关心服务端架构的情况下和服务端通信。还可以自己维护消费状态，并可以通过一些条件指定订阅特定的topic,比如白名单黑名单或者正则表达式。
2.1 低级别的API class SimpleConsumer { /*向一个broker发送读取请求并得到消息集 */ public ByteBufferMessageSet fetch(FetchRequest request); /*向一个broker发送读取请求并得到一个相应集 */ public MultiFetchResponse multifetch(List&lt;FetchRequest&gt; fetches); /** * 得到指定时间之前的offsets * 返回值是offsets列表，以倒序排序 * @param time: 时间，毫秒, * 如果指定为OffsetRequest$.MODULE$.LATIEST_TIME(), 得到最新的offset. * 如果指定为OffsetRequest$.MODULE$.EARLIEST_TIME(),得到最老的offset. */ publiclong[] getOffsetsBefore(String topic, int partition, long time, int maxNumOffsets); } 低级别的API是高级别API实现的基础，也是为了一些对维持消费状态有特殊需求的场景，比如Hadoop consumer这样的离线consumer。
2.2 高级别的API /* 创建连接 */ ConsumerConnector connector = Consumer.create(consumerConfig); interface ConsumerConnector { /** * 这个方法可以得到一个流的列表，每个流都是MessageAndMetadata的迭代， * 通过MessageAndMetadata可以拿到消息和其他的元数据（目前之后topic） * Input: a map of &lt;topic, #streams&gt; * Output: a map of &lt;topic, list of message streams&gt; */ public Map&lt;String,List&lt;KafkaStream&gt;&gt; createMessageStreams(Map&lt;String,Int&gt; topicCountMap); /** * 你也可以得到一个流的列表，它包含了符合TopicFiler的消息的迭代， * 一个TopicFilter是一个封装了白名单或黑名单的正则表达式。 */ public List&lt;KafkaStream&gt; createMessageStreamsByFilter( TopicFilter topicFilter, int numStreams); /* 提交目前消费到的offset */ public commitOffsets() /* 关闭连接 */ public shutdown() } 这个API围绕着由KafkaStream实现的迭代器展开，每个流代表一系列从一个或多个分区多和broker上汇聚来的消息，每个流由一个线程处理，所以客户端可以在创建的时候通过参数指定想要几个流。一个流是多个分区多个broker的合并，但是每个分区的消息只会流向一个流。
每调用一次createMessageStreams都会将consumer注册到topic上，这样consumer和brokers之间的负载均衡就会进行调整。API鼓励每次调用创建更多的topic流以减少这种调整。createMessageStreamsByFilter方法注册监听可以感知新的符合filter的tipic。
十、消息和日志 消息由一个固定长度的头部和可变长度的字节数组组成。头部包含了一个版本号和CRC32校验码。
/** * 具有N个字节的消息的格式如下 * 如果版本号是0 * 1. 1个字节的 &#34;magic&#34; 标记 * 2. 4个字节的CRC32校验码 * 3. N - 5个字节的具体信息 * * 如果版本号是1 * 1. 1个字节的 &#34;magic&#34; 标记 * 2.1个字节的参数允许标注一些附加的信息比如是否压缩了，解码类型等 * 3.4个字节的CRC32校验码 * 4. N - 6 个字节的具体信息 */ 1. 日志 一个叫做“my_topic”且有两个分区的的topic,它的日志有两个文件夹组成，my_topic_0和my_topic_1,每个文件夹里放着具体的数据文件，每个数据文件都是一系列的日志实体，每个日志实体有一个4个字节的整数N标注消息的长度，后边跟着N个字节的消息。每个消息都可以由一个64位的整数offset标注，offset标注了这条消息在发送到这个分区的消息流中的起始位置。每个日志文件的名称都是这个文件第一条日志的offset.所以第一个日志文件的名字就是00000000000.kafka.所以每相邻的两个文件名字的差就是一个数字S,S差不多就是配置文件中指定的日志文件的最大容量。
消息的格式都由一个统一的接口维护，所以消息可以在producer,broker和consumer之间无缝的传递。存储在硬盘上的消息格式如下所示：
消息长度: 4 bytes (value: 1+4+n) 版本号: 1 byte CRC校验码: 4 bytes 具体的消息: n bytes 2. 写操作 消息被不断的追加到最后一个日志的末尾，当日志的大小达到一个指定的值时就会产生一个新的文件。对于写操作有两个参数，一个规定了消息的数量达到这个值时必须将数据刷新到硬盘上，另外一个规定了刷新到硬盘的时间间隔，这对数据的持久性是个保证，在系统崩溃的时候只会丢失一定数量的消息或者一个时间段的消息。
3. 读操作 需要两个参数：一个64位的offset和一个S字节的最大读取量。S通常比单个消息的大小要大，但在一些个别消息比较大的情况下，S会小于单个消息的大小。这种情况下读操作会不断重试，每次重试都会将读取量加倍，直到读取到一个完整的消息。可以配置单个消息的最大值，这样服务器就会拒绝大小超过这个值的消息。也可以给客户端指定一个尝试读取的最大上限，避免为了读到一个完整的消息而无限次的重试。
在实际执行读取操纵时，首先需要定位数据所在的日志文件，然后根据offset计算出在这个日志中的offset(前面的的offset是整个分区的offset),然后在这个offset的位置进行读取。定位操作是由二分查找法完成的，Kafka在内存中为每个文件维护了offset的范围。
下面是发送给 consumer 的结果的格式：
MessageSetSend (fetch result) total length : 4 bytes error code : 2 bytes message 1 : x bytes ... message n : x bytes MultiMessageSetSend (multiFetch result) total length : 4 bytes error code : 2 bytes messageSetSend 1 ... messageSetSend n 4. 删除 日志管理器允许定制删除策略。目前的策略是删除修改时间在N天之前的日志（按时间删除），也可以使用另外一个策略：保留最后的N GB数据的策略(按大小删除)。为了避免在删除时阻塞读操作，采用了copy-on-write形式的实现，删除操作进行时，读取操作的二分查找功能实际是在一个静态的快照副本上进行的，这类似于Java的CopyOnWriteArrayList。
5. 可靠性保证 日志文件有一个可配置的参数M，缓存超过这个数量的消息将被强行刷新到硬盘。一个日志矫正线程将循环检查最新的日志文件中的消息确认每个消息都是合法的。合法的标准为：所有文件的大小的和最大的offset小于日志文件的大小，并且消息的CRC32校验码与存储在消息实体中的校验码一致。如果在某个offset发现不合法的消息，从这个offset到下一个合法的offset之间的内容将被移除。
有两种情况必须考虑：
当发生崩溃时有些数据块未能写入。 写入了一些空白数据块。第二种情况的原因是，对于每个文件，操作系统都有一个inode（inode是指在许多“类Unix文件系统”中的一种数据结构。每个inode保存了文件系统中的一个文件系统对象,包括文件、目录、大小、设备文件、socket、管道, 等等），但无法保证更新inode和写入数据的顺序，当inode保存的大小信息被更新了，但写入数据时发生了崩溃，就产生了空白数据块。CRC校验码可以检查这些块并移除，当然因为崩溃而未写入的数据块也就丢失了。 原文地址 ]]></content></entry><entry><title>闲言碎语，不知所云...</title><url>/2016/09/28/%E9%97%B2%E8%A8%80%E7%A2%8E%E8%AF%AD%E4%B8%8D%E7%9F%A5%E6%89%80%E4%BA%91.../</url><categories><category>杂谈</category></categories><tags><tag>杂谈</tag></tags><content type="html"> 闲言碎语，不知所云，乱七八糟，聊以自慰&amp;hellip;
搭建博客有一段时间了，记录总结的基本都是技术相关的，对于一个有强迫症加语文学的不好的人来说，总结技术文章太痛苦了，总结一篇文章得纠结好久，总觉得组织的语言不够恰当准确，每次都是改了又改，反复琢磨，怎奈肚子里的墨水实在是少的可怜，真是羡慕那些可以把自己心里所想的东西用文字能够表达很清楚的人。
说到底还是看的东西少，缺少写东西的锻炼，青春迷茫的时候有大把机会可以利用，可惜都不知道把时间用在了哪儿，等慢慢想要沉淀的时候才发现工作和生活中琐碎已经占据了所有的时间。但不管怎样，从此刻启程，做一些自己想要做的事儿，我想应该不会太晚吧！
搭建 Hexo 博客的初衷之一是觉得对于身为码农的我来说，Hexo 好玩，有新鲜感，对于非码农来说有一定的门槛（显的逼格高）。之二就是书写简单，不必局限于单一的编辑工具，少了刻意去追求排版样式烦恼。之三是觉得安全可靠，文章资源一切都可以掌控与自己，不必担心丢失，之前也买过 VPS 摆弄过 WordPress ，最后 VPS 租期到了，那段时间忙于找工作，博客的内容也没有及时保存下来，虽说博客没多少东西，但始终觉得有些遗憾&amp;hellip;
夜深人不静，半夜公司楼下打车真是不好打。这几天每天到家都凌晨以后了，身体真是大不如前了&amp;hellip;</content></entry><entry><title>Oh My Zsh 替换你的 Bash Shell</title><url>/2016/09/24/using-oh-my-zsh/</url><categories><category>Linux</category></categories><tags><tag>Linux</tag><tag>Zsh</tag><tag>Shell</tag></tags><content type="html"><![CDATA[ Oh-My-Zsh is an open source, community-driven framework for managing your ZSH configuration. It comes bundled with a ton of helpful functions, helpers, plugins, themes, and a few things that make you shout&hellip;
Oh My ZSH! 1. Oh My Zsh 简介 无意中看见了 Linux 的一款 Shell，相比于 Bash 极其美艳，决定入坑试水一下，不试不知道，一试绝对有惊喜，不仅外观美艳而且功能强大，那还有不替换 Bash 的理由吗？搞起搞起&hellip;&hellip;
Linux 提供了很多种 Shell ，想要查看系统有安装哪些 Shell 可以通过命令：chsh -l 或者 cat /etc/shells 进行查看。
cat /etc/shells /bin/sh /bin/bash /sbin/nologin /usr/bin/sh /usr/bin/bash /usr/sbin/nologin 其中 Bash 是绝大多数 Linux 系统默认的 Shell，虽然 Zsh Shell 没有被所有 Linux 预安装，但几乎每一款 Linux 都包含 Zsh Shell， 根据不同版本的 Linux 可以用 apt-get、yum 等包管理器进行安装。
但是我们今天介绍的并非是 Zsh，而是 Oh My Zsh，这是个什么鬼了？虽说 Zsh Shell 很牛逼，但配置相当麻烦，所以阻挡了好多人尝试的勇气（我想这也是为啥 Zsh 不被 Linux 设为默认 Shell 的原因吧），但是永远不要低估一个爱折腾程序猿的创造力，国外一程序猿真就创造出了一款易于使用的 Zsh 版本： Oh My Zsh Oh My Zsh 几乎兼容日常使用的所有 Bash Shell 指令，让你可以无缝接入，不需要再去花额外的时间去适应。当然还扩展了大量炫酷的指令，要不然干嘛用它对吧，而且支持更换主题和插件机制。下面就开始搞起吧！
2. 安装 安装 Oh My Zsh 首先需要安装 git 和 zsh，如果已经安装，请自行忽略。本文是基于 CentOS 介绍，其它 Linux 大致相同。
2.1 git 和 zsh 安装 # git sodu yum install git # zsh sodu yum install zsh 2.2 Oh My Zsh 安装 Oh My Zsh 提供了 curl 和 wget 两种安装方式，选择任意一种自己方便的方式即可。
# via curl sudo sh -c &#34;$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&#34; # via wget sudo sh -c &#34;$(wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)&#34; 3. 配置使用 3.1 切换 Shell 安装完成之后，使用以下命令即可切换当前 Shell：
# chsh -s /bin/zsh usermod $USER -s /usr/bin/zsh 退出当前终端连接，再次登入即可看到当前的 Shell 已经成功更改为 Oh My Zsh。
3.2 更改主题 Oh My Zsh 的配置文件路径为：~/.zshrc，配置文件中的 ZSH_THEME 就是主题配置字段，如下默认为 robbyrussell 主题配色。更多主题配色可参考 Oh My Zsh 主题 ，这里我推荐是用 ys 主题。
ZSH_THEME=&#34;robbyrussell&#34; 3.3 其它 Linux 用户配置使用 Linux 其它用户想要使用 Oh My Zsh Shell，只需要拷贝当前用户家目录下的 ~/.zshrc 配置文件到当想要使用 Oh My Zsh Shell 用户的家目录下即可。需要注意的是配置文件中 Oh My Zsh 安装路径其它用户可以访问的到，所以最好更改为绝对路径，如下：
export ZSH=/home/ehlxr/.oh-my-zsh 4. 插件配置 Oh My Zsh 支持插件机制，配置插件可以大大简化相关繁琐重复的命令。例如默认开启的 git 插件，就可以使用以下图表中的简写命令了：
那如何配置其它的插件了？只需要在 Oh My Zsh 的配置文件 ~/.zshrc 中找到 plugins=(git) 字段，配置插件的名称即可，多个插件使用空格隔开。
那支持哪些插件了？Oh My Zsh 安装目录下的 plugins 目录下就是支持的所有插件。例如：ls ~/.oh-my-zsh/plugins 就可以看到所有支持的插件名称了。
Oh My Zsh 支持插件的简写说明可以参看 oh-my-zsh Plugins wiki 。
5. 常用命令简介 l 等价与 ls -la，查看当前目录下所有文件。 打开目录不需要再敲 cd，直接输入目录名即可。 TAB 提示更加聪明。 聪明的历史记录，例如：敲下 ls 命令，按下键盘 up 按键，就会带出以 ls 开头的历史记录命令。 6. 常用插件 .zshrc 配置
export JAVA_HOME=$(/usr/libexec/java_home -v 1.8) export JAVA_6_HOME=$(/usr/libexec/java_home -v 1.8) export JAVA_7_HOME=$(/usr/libexec/java_home -v 1.8) export JAVA_8_HOME=$(/usr/libexec/java_home -v 1.8) export ZSH=/Users/ehlxr/.oh-my-zsh ZSH_THEME=&#34;ys&#34; plugins=(git wd autojump sublime sudo zsh-autosuggestions zsh-syntax-highlighting extract history-substring-search) source $ZSH/oh-my-zsh.sh alias h=&#34;hexo cl &amp;&amp; hexo g &amp;&amp; gulp &amp;&amp; hexo d&#34; alias gacp=&#39;git add --all &amp;&amp; git commit -v &amp;&amp; git push&#39; alias gacpm=&#39;git add --all &amp;&amp; git commit -m &#34;update&#34; &amp;&amp; git push&#39; alias js=&#39;j --stat&#39; source ~/.bash_profile 6.1 git 插件 安装
直接在 .zshrc 中启用。
效果
默认开启的插件，提供了大量 git 的 alias ，参看 oh-my-zsh Plugins wiki 。
6.2 wd 插件 安装
直接在 .zshrc 中启用。
效果
给常用目录添加一个标识，然后就可以快速的切换到常用的目录。参考 wd 。
6.3 autojump 插件 安装
# Mac，可以使用 brew 安装： brew install autojump # CentOS 可以使用 yum 安装: yum install -y epel-release yum install -y autojump-zsh 然后在 .zshrc 中启用。
效果
autojump 按照你的使用频率记录路径，使得目录的跳转更为方便。参考 autojump 6.4 sublime插件 安装
直接在 .zshrc 中启用。
效果
使用命令行打开 sublime。 常用命令如下：
st # 直接打开sublime st file_a # 用sublime打开文件 file st dir_a # 用sublime打开目录 dir stt # 在sublime打开当前目录，相当于 st . 6.5 sudo 插件 安装
直接在 .zshrc 中启用。
效果
按 2 次 esc 会在命令前自动输入 sudo
6.6 zsh-autosuggestions 插件 安装
git clone https://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM/plugins/zsh-autosuggestions 然后在 .zshrc 中启用。
效果
根据历史记录智能自动补全命令，输入命令时会以暗色补全，按方向键右键完成输入。
6.7 zsh-syntax-highlighting 插件 安装
git clone https://github.com/zsh-users/zsh-syntax-highlighting.git $ZSH_CUSTOM/plugins/zsh-syntax-highlighting 然后在 .zshrc 中启用。
效果
fish shell 风格的语法高亮插件。输入的命令根据主题自动高亮。比如我的主题是 ys-modified，输入正确的命令是黄色，输入错误的命令是红色。
6.8 extract 插件 安装
直接在 .zshrc 中启用。
效果
功能强大的解压插件，所有类型的文件解压一个命令 x 全搞定，再也不需要去记 tar 后面到底是哪几个参数了。
6.9 history-substring-search 插件 历史命令搜索插件，如果和 zsh-syntax-highlighting 插件共用，要配置到语法高亮插件之后。
安装
git clone https://github.com/zsh-users/zsh-history-substring-search.git $ZSH_CUSTOM/plugins/history-substring-search 然后在 .zshrc 中启用。
效果
上下键查询历史命令记录，可模糊匹配历史命令记录中任意字符串。
]]></content></entry><entry><title>5 分钟搭建 Git 服务器-Gogs</title><url>/2016/09/06/gogs-installation-introduction/</url><categories><category>Git</category></categories><tags><tag>Gogs</tag><tag>Git</tag></tags><content type="html"><![CDATA[ Gogs 基于 Go 语言的自助 Git 服务。它具有易安装、跨平台、轻量级、开源化等特性&hellip;
{% img https://cdn.jsdelivr.net/gh/0vo/oss/images/gogs-logo.png 150 %}
最近新到一家公司，发现在使用 Gogs 搭建 Git 服务，遂研究了一下，和前段时间研究的 GitLab 做了一个简单的对比，虽然 Gogs 相对与 GitLab 还比较年轻，也许没有 GitLab 强大和稳健，但 Gogs 更加简单易用，而且能够满足正常的工作使用。
Gogs 是轻量级的 Git 服务，正如官方介绍的：一个廉价的树莓派的配置足以满足 Gogs 的最低系统硬件要求。最大程度上节省您的服务器资源！关键的一点是免费开源的，所有的代码都开源在 GitHub 上。下面结合官方的介绍，总结一下在 Linux 系统下的安装方法，真的是相当的简单，5 分钟足矣！
一、安装 1.1 下载 下载对应系统版本的二进制安装包，并上传至 Linux 系统，或通过以下命令下载：
$ wget https://github.com/gogits/gogs/releases/download/v0.9.97/linux_amd64.tar.gz Gogs发布版本 1.2 解压安装包 $ tar -xzvf gogs_v0.9.97_linux_amd64.tar.gz 1.3 安装 进入到刚刚解压后的目录执行命令 ./gogs web，出现以下信息：
1.4 配置 打开浏览器输地址入：http://ip:3000，第一次会出现以下的配置界面，根据实际情况选择即可。
1.5 完成安装 之后进入以下界面，表明安装已经完成。
1.6 后台运行 后台运行可参考以下命令：
$ nohup ./gogs web &gt; nohup.out 2&gt;&amp;1 &amp; 升级 引用自官方 二级制升级文档 2.1 首先，确认当前安装的位置： # 默认位置在 git 用户下的家目录 $ sudo su - git $ cd ~ $ pwd /home/git $ ls gogs gogs-repositories 2.2 然后将当前目录移动到另一个临时的位置，但不是删除！ $ mv gogs gogs_old 2.3 下载并解压新的二进制： # 请根据系统和类型获取相应的二进制版本 $ wget https://dl.gogs.io/gogs_v$VERSION_$OS_$ARCH.tar.gz $ tar -zxvf gogs_v$VERSION_$OS_$ARCH.tar.gz $ ls gogs gogs_old gogs-repositories gogs_v$VERSION_$OS_$ARCH.tar.gz 2.4 复制 custom、data 和 log 目录到新解压的目录中： $ cp -R gogs_old/custom gogs $ cp -R gogs_old/data gogs $ cp -R gogs_old/log gogs 2.5 最后，运行并打开浏览器进行测试： $ cd gogs $ ./gogs web ]]></content></entry><entry><title>CentOS 中配置 Git 命令自动补全</title><url>/2016/09/04/CentOS-%E4%B8%AD%E9%85%8D%E7%BD%AE-Git-%E5%91%BD%E4%BB%A4%E8%87%AA%E5%8A%A8%E8%A1%A5%E5%85%A8/</url><categories><category>Git</category></categories><tags><tag>CentOS</tag><tag>Git</tag></tags><content type="html">1. Step 1 保存以下文件的内容为：git-completion.bash
git-completion.bash 2. Step 2 将上述文件 git-completion.bash copy 至个人 home 目录，可设为隐藏文件以免后续被误删。
$ cp git-completion.bash ~/.git-completion.bash Step 3 编辑环境变量文件
$ vi ~/.bashrc 在最后加入下面内容
source ~/.git-completion.bash 完成以上步骤后，重启 shell，就可以通过 tab 键自动补全 Git 命令了。</content></entry><entry><title>Naruto-Pictures</title><url>/2016/09/02/Naruto-Pictures/</url><categories><category>图片</category></categories><tags><tag>picture</tag><tag>naruto</tag></tags><content type="html">{% gp 5-3 %} {% endgp %}</content></entry><entry><title>使用Hexo基于GitHub Pages搭建个人博客（三）</title><url>/2016/08/30/%E4%BD%BF%E7%94%A8Hexo%E5%9F%BA%E4%BA%8EGitHub-Pages%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E4%B8%89/</url><categories><category>Hexo</category></categories><tags><tag>Hexo</tag><tag>GitHub</tag></tags><content type="html"><![CDATA[ 生命不息，折腾不休。从搭建 blog 以来，博文虽没有写几篇，但折腾的时间花了不少，走过了不少弯路，也踩过了不少的坑，虽然很懒，但本着好记性不如烂笔头的宗旨，打算在折腾记忆尚未磨灭之际记录一下走过的路和踩过的坑&hellip;
接着前两篇使用 Hexo 基于 GitHub Pages 搭建个人博客之上，本文记录了在使用非常漂亮简洁的 Next 主题过程中的各种折腾，其它主题的相关设置大同小异。再次衷心的感谢 Hexo 的作者和 Next 主题作者的无私奉献。
一、主题基本配置 记录一下 Next 主题的基本配置、设置「阅读全文」、关闭新建页面的评论功能、页面文章的篇数、宽度调节、设置「JavaScript 第三方库」等内容、在线字体替换（选择关闭，加快访问速度）
1.1 主题安装 Next 主题安装与所有 Hexo 主题安装一样。当克隆或者下载（ Next主题GitHub地址 ）完成主题文件后，拷贝至站点目录的 themes 目录下，一般命名为 next，打开站点配置文件， 找到 theme 字段，并将其值更改为 next 即可。
theme: next 1.2 Next 主题宽度调节 编辑 themes/next/source/css/_variables/custom.styl 文件，新增变量：
// 修改成你期望的宽度 $content-desktop = 700px // 当视窗超过 1600px 后的宽度 $content-desktop-large = 900px 以上方法不适用 Pisces Scheme
Pisces Scheme 编辑 themes/next/source/css/_schemes/Picses/_layout.styl 文件，更改以下 css 选项定义值：
.header{ width: 1150px; } .container .main-inner { width: 1150px; } .content-wrap { width: calc(100% - 260px); } 二、多说评论 2.1 安装多说 注册 多说 ，登录后在首页选择 “我要安装”。
2.2 创建站点 按要求填写如下图所示表单：
注意： 多说域名 这一栏填写的即是你的 duoshuo_shortname，在下一步主题配置中需要使用到。
2.3 主题配置多说 创建站点完成后，在 主题配置文件 中新增 duoshuo_shortname 字段，值设置成上一步创建站点中填写的 多说域名 即可。
2.4 开启多说热评文章 在 主题配置文件 中，设置 duoshuo_hotartical 配置项的值为 true，即可开启多说热评文章。
2.5 多说评论样式调整 登录 多说 后，在首页右上角点击“后台管理”，选择站点名称打开多说后台管理页面，选择“设置”下拉找到“自定义CSS”输入框，填写以下CSS样式，效果参考本站文章结尾评论样式。
/*-------------访客底部----------------*/ .ds-recent-visitors { margin-bottom: 200px; } @media (max-width: 768px) { .ds-recent-visitors { margin-bottom: 440px; } } /*-------------非圆角----------------*/ #ds-reset .ds-rounded { border-radius: 0px; } .theme-next #ds-thread #ds-reset .ds-textarea-wrapper { border-top-right-radius: 0px; border-top-left-radius: 0px; } .theme-next #ds-thread #ds-reset .ds-post-button { border-radius: 0px; } .ds-post-self xmp { word-wrap: break-word; } /*-------------访客----------------*/ #ds-reset .ds-avatar img, #ds-recent-visitors .ds-avatar img { width: 54px; height: 54px; /*设置图像的长和宽，这里要根据自己的评论框情况更改*/ border-radius: 27px; /*设置图像圆角效果,在这里我直接设置了超过width/2的像素，即为圆形了*/ -webkit-border-radius: 27px; /*圆角效果：兼容webkit浏览器*/ -moz-border-radius: 27px; box-shadow: inset 0 -1px 0 #3333sf; /*设置图像阴影效果*/ -webkit-box-shadow: inset 0 -1px 0 #3333sf; -webkit-transition: 0.4s; -webkit-transition: -webkit-transform 0.4s ease-out; transition: transform 0.4s ease-out; /*变化时间设置为0.4秒(变化动作即为下面的图像旋转360读）*/ -moz-transition: -moz-transform 0.4s ease-out; } /*-------------访客悬浮在头像----------------*/ #ds-reset .ds-avatar img:hover, #ds-recent-visitors .ds-avatar img:hover { box-shadow: 0 0 10px #fff; rgba(255, 255, 255, .6), inset 0 0 20 px rgba(255, 255, 255, 1); -webkit-box-shadow: 0 0 10px #fff; rgba(255, 255, 255, .6), inset 0 0 20 px rgba(255, 255, 255, 1); transform: rotateZ(360deg); /*图像旋转360度*/ -webkit-transform: rotateZ(360deg); -moz-transform: rotateZ(360deg); } #ds-thread #ds-reset .ds-textarea-wrapper textarea { background: url(http://ww4.sinaimg.cn/small/649a4735gw1et7gnhy5fej20zk0m8q3q.jpg) right no-repeat; } #ds-recent-visitors .ds-avatar { float: left } /*-------------隐藏版权----------------*/ #ds-thread #ds-reset .ds-powered-by { display: none; } 三、统计 我使用 LeanCloud 统计文章阅读数，使用不蒜子统计站点的 PV 和 UV 数。
3.1 文章阅读次数统计（LeanCloud) 参考 为NexT主题添加文章阅读量统计功能 3.2 不蒜子统计站点访问统计 编辑 主题配置文件 中的 busuanzi_count 的配置项，配置以下内容：
# count values only if the other configs are false enable: true # custom uv span for the whole site site_uv: true site_uv_header: &lt;i class=&#34;fa fa-user&#34;&gt;&lt;/i&gt; site_uv_footer: # custom pv span for the whole site site_pv: true site_pv_header: &lt;i class=&#34;fa fa-eye&#34;&gt;&lt;/i&gt; site_pv_footer: # custom pv span for one page only page_pv: false page_pv_header: &lt;i class=&#34;fa fa-file-o&#34;&gt;&lt;/i&gt; page_pv_footer: site/page_pv/uv_header 和 site/page_pv/uv_footer 为自定义样式配置，相关的值留空时将不显示，可以使用（带特效的）font-awesome。
四、设置 RSS 4.1 安装 hexo-generator-feed 在站点的根目录下执行以下命令：
$ npm install hexo-generator-feed --save 4.2 启用 RSS 编辑 站点配置文件，新增以下内容到任意位置：
# RSS订阅支持 plugin: - hexo-generator-feed # Feed Atom feed: type: atom path: atom.xml limit: 20 五、内容分享 使用 JiaThis 作为内容分享服务，具体步骤如下：
编辑 站点配置文件， 添加字段 jiathis，值为 true 即可。
六、搜索服务 6.1 安装 hexo-generator-search 在站点的根目录下执行以下命令：
$ npm install hexo-generator-search --save 6.2 启用搜索 编辑 站点配置文件，新增以下内容到任意位置：
search: path: search.xml field: post 6.3 安装 hexo-generator-searchdb 在站点的根目录下执行以下命令：
$ npm install hexo-generator-searchdb --save 6.4 启用搜索 编辑 站点配置文件，新增以下内容到任意位置：
search: path: search.xml field: post format: html limit: 10000 七、背景效果 介绍博客背景动态效果图和点击小红心效果的相关设置。
7.1 添加 JS 文件 把 js 文件 love.js 和 particle.js 放在\themes\next\source\js\src文件目录下。
7.2 引用添加的 JS 文件 更新\themes\next\layout\_layout.swig文件，在末尾（在前面引用会出现找不到的bug）添加以下 js 引入代码：
&lt;!-- 背景动画 --&gt; &lt;script type=&#34;text/javascript&#34; src=&#34;/js/src/particle.js&#34;&gt;&lt;/script&gt; &lt;!-- 页面点击小红心 --&gt; &lt;script type=&#34;text/javascript&#34; src=&#34;/js/src/love.js&#34;&gt;&lt;/script&gt; 八、图片模式 8.1 新建博文 新建博文，设置type: &quot;picture&quot;，使用{\% gp x-x \%} ... {\% endgp \%}标签引用要展示的图片地址，如下所示：
--- title: Naruto-Pictures categories: [图片] tags: [picture,naruto] date: 2016-09-02 14:36:04 keywords: picture,naruto type: &#34;picture&#34; top: 999 --- {% gp 5-3 %} ![](https://cdn.jsdelivr.net/gh/0vo/oss/images/687148dbly1flxx7cice6j218g0p0zpv.jpg) ![](https://cdn.jsdelivr.net/gh/0vo/oss/images/687148dbly1flxx7ch9rvj218g0p0jvi.jpg) ![](https://cdn.jsdelivr.net/gh/0vo/oss/images/687148dbly1flxx7cj8xej218g0p0gqw.jpg) ![](https://cdn.jsdelivr.net/gh/0vo/oss/images/687148dbly1flxx7cg745j218g0p0juj.jpg) ![](https://cdn.jsdelivr.net/gh/0vo/oss/images/687148dbly1flxx7cgf88j218g0p0ju3.jpg) {% endgp %} 8.2 图片展示效果 {\% gp 5-3 \%}：设置图片展示效果，参考 theme/next/scripts/tags/group-pictures.js 注释示意图。
8.3 修复图片展示 主题目前首页可以正常显示步骤 8.2 设置的图片效果，但是点击进入后显示效果丢失，所以需要修改一下文件 themes\next\source\css\_common\components\tags\group-pictures.styl 中的以下样式：
.page-post-detail .post-body .group-picture-column { // float: none; margin-top: 10px; // width: auto !important; img { margin: 0 auto; } } 九、博文压缩 目前知道的有两个插件可以压缩博文，hexo-all-minifier 插件和 gulp 插件。hexo-all-minifier 插件虽然使用比较简单，而且可以压缩图片，但是发现对文章缩进（输入法全拼模式下按 Tab）不支持，所以暂时使用第二种压缩手段。
9.1 hexo-all-minifier 配置使用 安装 hexo-all-minifier，在站点的根目录下执行以下命令：
$ npm install hexo-all-minifier --save hexo g 生产博文的时候就会自动压缩 HTML、JS、图片，详情参考 插件介绍 9.2 gulp 插件配置使用 hexo 依赖 gulp 插件安装，在站点的根目录下执行以下命令：
$ npm install gulp -g $ npm install gulp-minify-css gulp-uglify gulp-htmlmin gulp-htmlclean gulp --save 在 package.json 同级目录下，新建 gulpfile.js 并填入以下内容：
var gulp = require(&#39;gulp&#39;); var minifycss = require(&#39;gulp-minify-css&#39;); var uglify = require(&#39;gulp-uglify&#39;); var htmlmin = require(&#39;gulp-htmlmin&#39;); var htmlclean = require(&#39;gulp-htmlclean&#39;); // 压缩 public 目录 css gulp.task(&#39;minify-css&#39;, function() { return gulp.src(&#39;./public/**/*.css&#39;) .pipe(minifycss()) .pipe(gulp.dest(&#39;./public&#39;)); }); // 压缩 public 目录 html gulp.task(&#39;minify-html&#39;, function() { return gulp.src(&#39;./public/**/*.html&#39;) .pipe(htmlclean()) .pipe(htmlmin({ removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, })) .pipe(gulp.dest(&#39;./public&#39;)) }); // 压缩 public/js 目录 js gulp.task(&#39;minify-js&#39;, function() { return gulp.src(&#39;./public/**/*.js&#39;) .pipe(uglify()) .pipe(gulp.dest(&#39;./public&#39;)); }); // 执行 gulp 命令时执行的任务 gulp.task(&#39;default&#39;, [ &#39;minify-html&#39;,&#39;minify-css&#39;,&#39;minify-js&#39; ]); 生成博文是执行 hexo g &amp;&amp; gulp 就会根据 gulpfile.js 中的配置，对 public 目录中的静态资源文件进行压缩。
十、博文置顶 10.1 修改 hexo-generator-index 插件 替换文件：node_modules/hexo-generator-index/lib/generator.js 为： generator.js 10.2 设置文章置顶 在文章 Front-matter 中添加 top 值，数值越大文章越靠前，如：
--- title: Naruto 图集 categories: [图片] tags: [picture,naruto] date: 2016-09-02 14:36:04 keywords: picture,naruto type: &#34;picture&#34; top: 10 --- 十一、头像圆形旋转 介绍一下实现头像圆形，鼠标经过旋转或者一直让旋转效果，主要是修改 Hexo 目录下 \themes\next\source\css\_common\components\sidebar\sidebar-author.styl 文件。
11.1 头像圆形修改 修改 sidebar-author.styl 文件中 .site-author-image CSS 样式如下：
.site-author-image { display: block; margin: 0 auto; padding: $site-author-image-padding; max-width: $site-author-image-width; height: $site-author-image-height; border: $site-author-image-border-width solid $site-author-image-border-color; /*头像圆形*/ border-radius: 80px; -webkit-border-radius: 80px; -moz-border-radius: 80px; } 11.2 鼠标经过旋转修改 修改 sidebar-author.styl 文件，添加 CSS 样式 img:hover 如下代码：
img:hover { /* 鼠标经过头像旋转360度 */ -webkit-transform: rotateZ(360deg); -moz-transform: rotateZ(360deg); transform: rotateZ(360deg); } 修改 sidebar-author.styl 文件中 .site-author-image CSS 样式如下：
.site-author-image { display: block; margin: 0 auto; padding: $site-author-image-padding; max-width: $site-author-image-width; height: $site-author-image-height; border: $site-author-image-border-width solid $site-author-image-border-color; /* 头像圆形 */ border-radius: 80px; -webkit-border-radius: 80px; -moz-border-radius: 80px; box-shadow: inset 0 -1px 0 #333sf; /* 鼠标经过头像旋转360度 */ -webkit-transition: -webkit-transform 1.5s ease-out; -moz-transition: -moz-transform 1.5s ease-out; transition: transform 1.5s ease-out; } 11.3 头像循环旋转效果修改 修改 sidebar-author.styl 文件，添加如下代码：
/* Z 轴旋转动画 */ @-webkit-keyframes play { 0% { -webkit-transform: rotateZ(0deg); } 100% { -webkit-transform: rotateZ(360deg); } } @-moz-keyframes play { 0% { -moz-transform: rotateZ(0deg); } 100% { -moz-transform: rotateZ(360deg); } } @keyframes play { 0% { transform: rotateZ(0deg); } 100% { transform: rotateZ(360deg); } } 修改 sidebar-author.styl 文件中 .site-author-image CSS 样式如下：
.site-author-image { display: block; margin: 0 auto; padding: $site-author-image-padding; max-width: $site-author-image-width; height: $site-author-image-height; border: $site-author-image-border-width solid $site-author-image-border-color; /* 头像圆形 */ border-radius: 80px; -webkit-border-radius: 80px; -moz-border-radius: 80px; box-shadow: inset 0 -1px 0 #333sf; /* 设置循环动画：animation:动画名称 动画播放时长单位秒或微秒 动画播放的速度曲线linear为匀速 动画播放次数infinite为循环播放; */ -webkit-animation: play 3s linear infinite; -moz-animation: play 3s linear infinite; animation: play 3s linear infinite; /* 鼠标经过头像旋转360度 -webkit-transition: -webkit-transform 1.5s ease-out; -moz-transition: -moz-transform 1.5s ease-out; transition: transform 1.5s ease-out;*/ } 鼠标经过停止头像旋转：修改 sidebar-author.styl 文件，添加 CSS 样式 img:hover 如下代码：
img:hover { /* 鼠标经过停止头像旋转 */ -webkit-animation-play-state:paused; animation-play-state:paused; /* 鼠标经过头像旋转360度 -webkit-transform: rotateZ(360deg); -moz-transform: rotateZ(360deg); transform: rotateZ(360deg);*/ } 完整 sidebar-author.styl 文件 参考地址 。
]]></content></entry><entry><title>[转] Java程序员情书</title><url>/2016/08/22/%E8%BD%AC-Java%E7%A8%8B%E5%BA%8F%E5%91%98%E6%83%85%E4%B9%A6/</url><categories><category>杂谈</category></categories><tags><tag>杂谈</tag></tags><content type="html"><![CDATA[我能抽象出整个世界．．．
但是我却不能抽象出你．．．
你肯定是一个单例，因为你是那样的独一无二&hellip;
所以我的世界并不完整．．．
我可以重载甚至覆盖这个世界里的任何一种方法．．．
但是却不能覆盖对你的思念．．．
也许命中注定了 你与我存在于不同的包里&hellip;
在你的世界里，你被烙上了私有的属性&hellip;
我用尽全身力气，也找不到访问你的接口&hellip;
我不愿就此甘心，找到了藏身在神殿的巫师，教会了我穿越时空的方法&hellip;
终于，我用反射这把利剑，打开了你空间的缺口&hellip;
并发现了接近你的秘密&hellip;
当我迫不及待地调用了爱你这个方法．．．
并义无返顾的把自己作为参数传进这个方法时．．．
我才发现爱上你是一个没有终止条件的递归．．．
它不停的返回我对你的思念并压入我心里的堆栈．．．
在这无尽的黑夜中 ，终于体验到你对我爱的回调&hellip;
我的内存里已经再也装不下别人&hellip;
当我以为将与你在这个死循环中天荒地老时&hellip;
万恶的系统抛出了爱的异常&hellip;
此刻我才发现，我不过是操纵于虚拟机下的一个线程，你也是&hellip;
但我毫不后悔，因为在爱的洗礼之后&hellip;
我看见了一个新的生命，那是我们的, 继承
]]></content></entry><entry><title>CentOS 系统下 GitLab 搭建与基本配置</title><url>/2016/07/31/CentOS-%E7%B3%BB%E7%BB%9F%E4%B8%8B-GitLab-%E6%90%AD%E5%BB%BA%E4%B8%8E%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE/</url><categories><category>Git</category></categories><tags><tag>CentOS</tag><tag>GitLab</tag><tag>Git</tag></tags><content type="html"><![CDATA[ GitLab 是一个开源的版本管理系统，提供了类似于 GitHub 的源代码浏览，管理缺陷和注释等功能，你可以将代码免费托管到 GitLab.com，而且不限项目数量和成员数。最吸引人的一点是，可以在自己的服务器上搭建 GitLab CE （社区免费版）版本，方便内部团队协作开发和代码管理。 下面介绍如何在 CentOS 服务器上搭建 GitLab CE 版本，以及一些基本的配置。
1. 安装 GitLab 提供了两种安装方式：源码手动编译安装和软件包管理安装。
源码手动编译安装虽然配置灵活，但过程比较麻烦，不容易安装成功，所以我这里选择软件包管理安装的形式。
1.1 使用 GitLab 提供仓库在线安装 curl -sS https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | sudo bash yum install gitlab-ce 国外的 GitLab 仓库访问速度较慢，可以使用国内的站点：
curl -sS http://packages.gitlab.cc/install/gitlab-ce/script.rpm.sh | sudo bash yum install gitlab-ce 1.2 下载离线软件包安装 如果网络速度不理想，可以使用离线软件包 rpm 的方式进行安装，下面提供了几个站点的下载地址。
GitLab 官方：https://packages.gitlab.com/gitlab/gitlab-ce?filter=rpms 清华大学TUNA开源镜像站：https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/ 浙大开源镜像站：http://mirrors.lifetoy.org/gitlab-ce/yum/el7/ 下载好 rpm 软件安装包后上传到服务器指定的目录下，通过以下命令进行安装：
rpm -ivh　gitlab-ce-8.9.6-ce.0.el7.x86_64.rpm 记录一下 rpm 卸载软件安装包命令：
rpm -e --nodeps gitlab-ce-8.9.6-ce.0.el7.x86_64 2. 启动 GitLab 安装完成之后，打开配置文件 /etc/gitlab/gitlab.rb 将 external_url = 'http://git.example.com' 修改为自己的 IP 地址：external_url 'http://ip_address' ，然后执行下面的命令，对 GitLab 进行编译：
gitlab-ctl reconfigure 完成后，使用浏览器访问：http://ip_address 可进入 GitLab 登录页面，首次访问系统会让你重新设置管理员的密码，默认的管理员账号是 root，如果你想更改默认管理员账号，登录系统后可以修改帐号名。
3. GitLab 基本配置 GitLab 的相关参数配置都存在 /etc/gitlab/gitlab.rb 文件里。自 GitLab 7.6 开始的新安装包, 已经默认将所有的参数写入到 /etc/gitlab/gitlab.rb 配置文件中。
3.1 配置端口 GitLab 默认使用 80 端口对外提供服务，因为 80 端口被其他服务占用，所以需要更改。打开 /etc/gitlab/gitlab.rb 配置文件，修改 external_url 'http://ip_address' 为 external_url 'http://ip_address:new-port'，
重新编译配置：
gitlab-ctl reconfigure 这时候就可以通更改后的 IP + 端口号码进行访问了。
3.2 邮箱配置 以下是 163 邮箱的配置参考，打开　/etc/gitlab/gitlab.rb 配置文件，添加以下内容：
gitlab_rails[&#39;smtp_enable&#39;] = true gitlab_rails[&#39;smtp_address&#39;] = &#34;smtp.163.com&#34; gitlab_rails[&#39;smtp_port&#39;] = 25 gitlab_rails[&#39;smtp_user_name&#39;] = &#34;test@163.com&#34; gitlab_rails[&#39;smtp_password&#39;] = &#34;password&#34; gitlab_rails[&#39;smtp_authentication&#39;] = &#34;login&#34; gitlab_rails[&#39;smtp_enable_starttls_auto&#39;] = true gitlab_rails[&#39;gitlab_email_from&#39;] = &#34;test@163.com&#34; 注意： test@163.com 和 password 更新为自己邮箱地址和密码；邮箱需要开启 SMTP 协议。
重新编译配置即可生效：
gitlab-ctl reconfigure 其它邮箱的配置可参考：https://doc.gitlab.cc/omnibus/settings/smtp.html
3.3 头像配置 GitLab 默认使用的是 Gravatar 头像服务，不过现在貌似 Gravatar 国内好像访问不了，导致 GitLab 默认头像破裂，无法显示，可以替换为多说 Gravatar 服务器。打开 /etc/gitlab/gitlab.rb 配置文件，增加下面这一行：
gitlab_rails[&#39;gravatar_plain_url&#39;] = &#39;http://gravatar.duoshuo.com/avatar/%{hash}?s=%{size}&amp;d=identicon&#39; 再分别执行以下命令即可
gitlab-ctl reconfigure gitlab-rake cache:clear RAILS_ENV=production 也可以关闭 Gravatar 头像显示配置，登录 GitLab 管理员账户，进入设置界面（路径地址：http://ip:port/admin/application_settings ），取消以下选项即可。
3.4 用户注册配置 管理员设置界面（路径地址：http://ip:port/admin/application_settings ）以下选项可以控制用户注册配置，包括是否允许登录、注册和注册邮箱验证等选项。
3.5 常用命令 GitLab 服务启动、停止、状态查询、修改配置生效等命令：
gitlab-ctl start/stop/status/reconfigure # 服务启动、停止、状态查询、修改配置生效 也可以查看帮助文档获取更多命令信息：
gitlab-ctl --help ]]></content></entry><entry><title>CentOS 7 安装最新的 Git</title><url>/2016/07/30/CentOS-7-%E5%AE%89%E8%A3%85%E6%9C%80%E6%96%B0%E7%9A%84-Git/</url><categories><category>Linux</category></categories><tags><tag>Linux</tag><tag>Git</tag><tag>CentOS</tag></tags><content type="html"><![CDATA[ yum 源仓库里的 Git 版本更新不及时，最新版本的 Git 是 1.8.3.1，但是官方最新版本已经到了 2.9.2。想要安装最新版本的的 Git，只能下载源码进行安装。
1. 查看 yum 源仓库的 Git 信息： # yum info git 可以看出，截至目前，yum 源仓库中最新的 Git 版本才 1.8.3.1，而查看最 新的 Git 发布版本 ，已经 2.9.2 了。
2. 依赖库安装 # yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel # yum install gcc perl-ExtUtils-MakeMaker 3. 卸载低版本的 Git 通过命令：git –-version 查看系统带的版本，Git 版本是： 1.8.3.1，所以先要卸载低版本的 Git，命令：
# yum remove git 4. 下载新版的 Git 源码包 # wget https://github.com/git/git/archive/v2.9.2.tar.gz 也可以离线下载，然后传到 CentOS 系统中指定的目录下。
5. 解压到指定目录 # tar -xzvf v2.9.2.tar.gz -C ~/app/ 6. 安装 Git 分别执行以下命令进行编译安装，编译过程可能比较漫长，请耐心等待完成。
# cd git-2.9.2 # make prefix=/usr/local/git all # make prefix=/usr/local/git install 7. 添加到环境变量 # echo &#34;export PATH=$PATH:/usr/local/git/bin&#34; &gt;&gt; /etc/bashrc # source /etc/bashrc # 实时生效 8. 查看版本号 # git --version git version 2.9.2 至此，CentOS 就安装上了最新版本的 Git。
]]></content></entry><entry><title>CentOS 7 安装 Node.js</title><url>/2016/07/30/CentOS-7-%E5%AE%89%E8%A3%85-Node.js/</url><categories><category>Linux</category></categories><tags><tag>Linux</tag><tag>Node.js</tag><tag>CentOS</tag></tags><content type="html"> Node.js® is a JavaScript runtime built on Chrome&amp;rsquo;s V8 JavaScript engine. Node.js uses an event-driven, non-blocking I/O model that makes it lightweight and efficient. Node.js&amp;rsquo; package ecosystem, npm, is the largest ecosystem of open source libraries in the world. 1. 下载源码安装文件 1.1 在线安装 通过以下命令下载源文件：
wget https://nodejs.org/dist/v4.4.7/node-v4.4.7.tar.gz 1.2 离线安装 如果网络不好，可以通过 官方网站 下载，然后上传到 CentOS 系统中，下载如图所示：
我这儿官方网站是无法正常访问的，需要一些手段（大家懂得），所以在网盘备份一份，地址：http://pan.baidu.com/s/1bpIAUAz
2. 编译安装 2.1 解压文件到指定的目录 tar -xzvf node-v4.4.7.tar.gz -C app/ 2.2 安装依赖包 yum install gcc gcc-c++ 2.3 配置安装 ./configure ./configure 是源代码安装的第一步，主要的作用是对即将安装的软件进行配置，检查当前的环境是否满足要安装软件的依赖关系，生成 makefile文件，以便你可以用 make 和 make install 来编译和安装程序。
2.4 编译安装 编译命令：
make 编译的过程会花很长一段时间，等编译完成再执行安装命令：
make install 3. 检查安装 运行以下命令：
node --version 若输出对应的版本号，则安装成功。
4. 设置 npm 淘宝镜像 npm config set registry https://registry.npm.taobao.org npm info underscore 如果配置成功，第二步会有字符串返回</content></entry><entry><title>[转]将 Centos 的 yum 源更改为国内的阿里云源</title><url>/2016/07/30/%E8%BD%AC%E5%B0%86-Centos-%E7%9A%84-yum-%E6%BA%90%E6%9B%B4%E6%94%B9%E4%B8%BA%E5%9B%BD%E5%86%85%E7%9A%84%E9%98%BF%E9%87%8C%E4%BA%91%E6%BA%90/</url><categories><category>Linux</category></categories><tags><tag>Linux</tag><tag>CentOS</tag></tags><content type="html">阿里云Linux安装镜像源地址：http://mirrors.aliyun.com/
CentOS系统更换软件安装源
一、备份你的原镜像文件，以免出错后可以恢复。 mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup 二、下载新的 CentOS-Base.repo 到 /etc/yum.repos.d/ 2.1 CentOS 5 wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-5.repo 2.2 CentOS 6 wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo 2.3 CentOS 7 wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 三、运行 yum makecache 生成缓存 yum clean all yum makecache</content></entry><entry><title>Oracle 与 MySQL 知识总结</title><url>/2016/07/28/Oracle-%E4%B8%8E-MySQL-%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/</url><categories><category>SQL</category></categories><tags><tag>MySQL</tag><tag>Oracle</tag></tags><content type="html"><![CDATA[ 对日常工作中使用 Oracle 和 MySQL 数据库知识对比总结。
SQLPlus 连接数据库的方式 cmd 中输入：sqlplus 用户名/密码@数据库实例 sqlplus system/lxr316@oracle sqlplus sys/admin as sysdba 打开 SQLPlus 直接输入用户名和密码 使用命令 connect sys/admin as sysdba connect system/lxr316 超级管理员登录 sys as sysdba 断开数据库 disconnect; # 可简写 disconn MySQL 连接 命令： mysql [–h 服务器地址] –u 用户名 –p [密码]（需要配置 mysql 数据库的 bin 到环境变量中）
mysql -h localhost –u root –p root mysql –u root -p 用户操作 Oracle 创建用户 create user [username] identified by [password]; 修改用户密码 alter user [username] identified by [password]; 账户上锁、解锁 alter user [username] account lock|unlock; 用户首次登录时直接修改密码&ndash;密码失效 alter user [username] password expire; MySQL 创建用户: CREATE USER &#39;username&#39;@&#39;host&#39; IDENTIFIED BY &#39;password&#39;; 备注：
username：你将创建的用户名。 host：指定该用户在哪个主机上可以登陆,如果是本地用户可用localhost, 如果想让该用户可以从任意远程主机登陆,可以使用通配符%。 password：该用户的登陆密码,密码可以为空,如果为空则该用户可以不需要密码登陆服务器。 例如：
CREATE USER &#39;dog&#39;@&#39;localhost&#39;IDENTIFIED BY &#39;123456&#39;; CREATE USER &#39;pig&#39;@&#39;192.168.1.101_&#39;IDENDIFIED BY &#39;123456&#39;; CREATE USER &#39;pig&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; CREATE USER &#39;pig&#39;@&#39;%&#39; IDENTIFIED BY &#39;&#39;; CREATE USER &#39;pig&#39;@&#39;%&#39;; 设置与更改用户密码 SET PASSWORD FOR &#39;username&#39;@&#39;host&#39; = PASSWORD(&#39;newpassword&#39;); 如果是当前登陆用户
SET PASSWORD = PASSWORD(&#34;newpassword&#34;); 删除用户 DROP USER &#39;username&#39;@&#39;host&#39;; 权限管理 Oracle 授权 grant 权限 to 用户名; grant all on 表名 to用户/角色 收回权限 revoke 权限from 用户名; 对象权限可以级联收回，但是系统权限不可以级联收回。
系统权限： A 赋予一个系统权限给 B，B 再授予 C。然后A将该权限从 B 回收，此后 C 仍然有该权限。 对象权限： A 赋予一个对象权限给 B，B 再授予 C。然后A将该权限从 B 回收，此后 C 也失去了该权限。 常用系统权限 create session --用户登录 create table --创建表 unlimited tablespace --无限表空间 常用对象权限 select on 表名; -- 查询权限 update on 表名; -- 更新权限 delete on 表名; -- 删除权限 insert on 表名; -- 插入权限 权限的级联授予 grant 权限 to 用户名 with admin option|with grant option; with admin option：系统权限 with grant option：对象权限 MySQL 授权: GRANT privileges ON databasename.tablename TO &#39;username&#39;@&#39;host&#39; 备注：
privileges：用户的操作权限,如 SELECT , INSERT , UPDATE 等；如果要授予所的权限则使用 ALL。 databasename：数据库名。 tablename：表名 如果要授予该用户对所有数据库和表的相应操作权限则可用 * 表示, 如 *.* 。 例如：
GRANT SELECT, INSERT ON test.user TO &#39;pig&#39;@&#39;%&#39;; GRANT ALL ON *.* TO &#39;pig&#39;@&#39;%&#39;; 注意： 用以上命令授权的用户不能给其它用户授权，如果想让该用户可以授权，用以下命令:
GRANT privileges ON databasename.tablename TO &#39;username&#39;@&#39;host&#39; WITH GRANT OPTION; 撤销用户权限 REVOKE privilege ON databasename.tablename FROM &#39;username&#39;@&#39;host&#39;; 说明： privilege, databasename, tablename - 同授权部分。 例如：
REVOKE SELECT ON *.* FROM &#39;pig&#39;@&#39;%&#39;; 注意： 假如你在给用户 'pig'@'%' 授权的时候是这样的（或类似的）：GRANT SELECT ON test.user TO 'pig'@'%'; 则在使用：REVOKE SELECT ON *.* FROM'pig'@'%'; 命令并不能撤销该用户对 test 数据库中 user 表的 SELECT 操作；相反，如果授权使用的是：GRANT SELECT ON *.* TO'pig'@'%'; 则：REVOKE SELECT ONtest.user FROM 'pig'@'%'; 命令也不能撤销该用户对 test 数据库中 user 表的 SELECT 权限。
具体信息可以用命令：SHOW GRANTS FOR 'pig'@'%'; 查看。
Oracle角色 role 角色：权限的集合
角色数据字典表 查看当前用户中的角色：user_role_privs 查看角色中的系统权限：role_sys_privs 查看角色中的对象权限：role_tab_privs 建一个角色 create role role1; 授权给角色 grant create any table, create procedure to role1; 授予/回收用户角色 grant role1 to user1; revoke role1 from user1; 创建带有口令以角色 在生效带有口令的角色时必须提供口令
create role role1 identified by password1; 修改角色：是否需要口令 alter role role1 not identified; alter role role1 identified by password1; 设置当前用户要生效的角色 set role role1; -- 使 role1 生效 set role role1, role2; -- 使 role1, role2 生效 set role role1 identified by password1; -- 使用带有口令的 role1 生效 set role all; -- 使用该用户的所有角色生效 set role none; -- 设置所有角色失效 set role all except role1; -- 除 role1 外的该用户的所有其它角色生效 select * from SESSION_ROLES; -- 查看当前用户的生效的角色。 修改指定用户，设置其默认角色 alter user user1 default role role1; alter user user1 default role all except role1; 删除角色 drop role role1; 注意： 角色删除后，原来拥用该角色的用户将失去该角色，相应的权限也将会失去。
系统角色 DBA: 拥有全部特权，是系统最高权限角色 RESOURCE：拥有 RESOURCE 角色权限的用户只可以创建实体 CONNECT：拥有 CONNECT 角色权限的用户只可以登录 Oracle 对于普通用户：授予 CONNECT，RESOURCE 角色权限；对于DBA管理用户：授予 CONNECT，RESOURCE， DBA 角色权限。
查看表结构信息命令 Oracle desc(ribe) 表名; -- 如：desc lxr MySQL desc(ribe) 表名; 显示当前登录用户 Oracle show user; MySQL select user(); MySQL 查看用户下的数据库 show databases; 查看表
show tables; Oracle 用户锁定与解锁 ALTER USER username ACCOUNT LOCK; -- 锁定用户 ALTER USER username ACCOUNT UNLOCK; -- 解锁用户 清屏命令 SQLPlus clear screen -- 简写：cl scr cls -- dos 清屏命令 MySQL system clear; -- Linux 环境下 system cls; -- dos 环境下 Oracle 更改当前用户日期格式命令 alter session set nls_date_format = &#39;yyyy-mm-dd&#39;; Oracle 中的基本数据类型 数值：number(5,2)，int 字符：char，varchar2(4000) 日期：date 图片：BLOB (binaryLargeObject) 4G 文本：CLOB (Character LargeObject) 4G Oracle 表结构操作 表重命名 alter table 表名 rename to 新表名; 给表增加注释 comment on table 表名 is &#39;注释内容&#39;; 添加约束方式 方式一 alter table 表名 add constraint 约束名 约束类型(约束的字段名); Constraint 约束类型：
值 名称 primary key 主键 unique 唯一 check 限制 not null 不能为null 方式二 alter table 表名 add constraint 约束名 foreign key(字段名) references 表名(字段名); foreign key 外键： 参照主键中存在的值，可以插入重复的记录、可以插入重复的空值
删除约束方式 alter table 表名 drop constraint 约束名; 删除表结构 drop table 表名; -- 此操作属DDL，会自动提交且不可回滚 表中增加字段 alter table 表名 add 字段名 类型; 删除字段 alter table 表名 drop 字段名; 通常在系统不忙的时候删除不使用的字段，可以先设置字段为 unused
alter table test3 set unused column address; 再执行删除
alter table test3 unused column; 字段重命名 alter table 表名 rename column 字段名 to 新字段名; 修改字段 alter table 表名 modify 字段名 新类型; 添加 not null
alter table 表名 modify 字段名 not null 删除 not null
alter table 表名 modify 字段名 null; Oracle 备份表 在当前的数据库之内进行备份 create table 表名(字段) as select 查询语句; 数据的移动 insert into 表名（字段列表）select 字段列表 from 表名; 数据库服务器之间拷贝表 客户端连接服务器 copy from system/hhl@hhl create hhl_table using select * from scott.emp; 从A服务器拷贝到自己的数据库中 copy from 用户名/密码@主机字符串 create 表名 using 查询语句; 从自己的数据库中拷贝到A服务器 copy to 用户用/密码@主机字符串 create 表名 using 查询语句; 从A服务器拷贝表到B服务器 copy from 用户用/密码@主机字符串 to 用户用/密码@主机字符串 create 表名 using 查询语句; OracleDBLINK 数据库连接 在当前的数据库内直接操作其他服务器中的表做增删改查，格式如下：
create database link 名 connect to 用户名 identified by 密码 using &#39;主机字符串&#39;; MySQL 查看数据库字符集 show variables like &#39;character%&#39;; show variables like &#39;%collation%&#39;; truncate 与 delete 使用格式 truncate table 表名; -- 删除表中全部记录 delete from 表名; truncate 与 delete 的区别 truncate 删除速度比 delete 删除速度快； truncate 不可以回滚，delete 可以回滚。 ]]></content></entry><entry><title>GitHub 更新已经 fork 的项目</title><url>/2016/07/28/update-from-github-fork/</url><categories><category>Git</category></categories><tags><tag>GitHub</tag><tag>Git</tag></tags><content type="html"> GitHub 上有个很方便的功能叫 fork，将别人的工程一键复制到自己账号下。这个功能很方便，但有点不足的是，当源项目更新后，你 fork 的分支并不会一起更新，需要自己手动去更新，下面记录下网上找到的更新的开发方法。
1. 在本地装好 GitHub 客户端，或者 Git 客户端 2. clone 自己的 fork 分支到本地 可以直接使用 GitHub 客户端，clone 到本地，如果使用命令行，命令为：
$ git clone git@github.com:ehlxr/strman-java.git 3. 增加源分支地址到你项目远程分支列表中 此处是关键，先得将原来的仓库指定为 upstream，命令为：
$ git remote add upstream git@github.com:shekhargulati/strman-java.git 此处可使用 git remote -v 查看远程分支列表
$ git remote -v origin git@github.com:ehlxr/strman-java.git (fetch) origin git@github.com:ehlxr/strman-java.git (push) upstream git@github.com:shekhargulati/strman-java.git (fetch) upstream git@github.com:shekhargulati/strman-java.git (push) 4. fetch 源分支的新版本到本地 $ git fetch upstream 5. 合并两个版本的代码 $ git merge upstream/master 6. 将合并后的代码 push 到 GitHub 上去 $ git push origin master 参考网址：
https://help.github.com/articles/fork-a-repo 原文出处</content></entry><entry><title>Sublime Text 插件安装</title><url>/2016/07/25/Sublime-Text-%E6%8F%92%E4%BB%B6%E5%AE%89%E8%A3%85/</url><categories><category>开发工具</category></categories><tags><tag>Sublime</tag></tags><content type="html"><![CDATA[ Sublime Text 官方介绍：
“Sublime Text is a sophisticated text editor for code, markup and prose.You&rsquo;ll love the slick user interface, extraordinary features and amazing performance.”
Sublime Text 是一款优秀的轻量级编辑器，而且支持跨平台，支持Windows、Linux、Mac OS X 等主流操作系统。虽然是一款收费软件，但也允许人们无限期免费使用。（ 官方下载地址 ）
Sublime Text 具有漂亮的用户界面和强大可扩展插件功能，本文介绍如何安装 Sublime Text 插件。
打开 Sublime Text 编辑器的 Console（控制台）使用快捷键 ctrl+` 或者点击菜单 View &gt; Show Console menu 打开 Console，如图：
Sublime Text 2 粘贴以下代码：
import urllib2,os,hashlib; h = &#39;2915d1851351e5ee549c20394736b442&#39; + &#39;8bc59f460fa1548d1514676163dafc88&#39;; pf = &#39;Package Control.sublime-package&#39;; ipp = sublime.installed_packages_path(); os.makedirs( ipp ) if not os.path.exists(ipp) else None; urllib2.install_opener( urllib2.build_opener( urllib2.ProxyHandler()) ); by = urllib2.urlopen( &#39;http://packagecontrol.io/&#39; + pf.replace(&#39; &#39;, &#39;%20&#39;)).read(); dh = hashlib.sha256(by).hexdigest(); open( os.path.join( ipp, pf), &#39;wb&#39; ).write(by) if dh == h else None; print(&#39;Error validating download (got %s instead of %s), please try manual install&#39; % (dh, h) if dh != h else &#39;Please restart Sublime Text to finish installation&#39;) Sublime Text 3 粘贴以下代码：
import urllib.request,os,hashlib; h = &#39;2915d1851351e5ee549c20394736b442&#39; + &#39;8bc59f460fa1548d1514676163dafc88&#39;; pf = &#39;Package Control.sublime-package&#39;; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( &#39;http://packagecontrol.io/&#39; + pf.replace(&#39; &#39;, &#39;%20&#39;)).read(); dh = hashlib.sha256(by).hexdigest(); print(&#39;Error validating download (got %s instead of %s), please try manual install&#39; % (dh, h)) if dh != h else open(os.path.join( ipp, pf), &#39;wb&#39; ).write(by) 敲回车等待安装完成后重启 Sublime Text 即可，按下快捷键 ctrl + shift + p ,输入 pci（Package Control Install 简写）如图：
敲回车即可打开插件安装界面，输入想要安装插件的名称即可开始安装，如图：
Sublime Text 插件安装介绍完了，需要注意的是安装过程中要保持网络畅通。
参考网址 ]]></content></entry><entry><title>Git 同时 push 到多个远程仓库</title><url>/2016/07/24/Git-%E5%90%8C%E6%97%B6-push-%E5%88%B0%E5%A4%9A%E4%B8%AA%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93/</url><categories><category>Git</category></categories><tags><tag>Git</tag></tags><content type="html">方法一 如果一个本地仓库添加多个远程仓库，不想 git push 多次，可以修改 .git/config 文件
vim .git/config 比如以下信息表示在 git@OSC 和 GitHub 两个远程托管
修改为以下信息
则可同时 push 到两个远程仓库
$ git push origin master Everything up-to-date Everything up-to-date 方法二 添加第二个远程地址时使用以下命令：
git remote set-url --add origin git@github.com:ehlxr/ehlxr-Hexo.git 查看远程分支 origin：
$ git remote -v origin git@git.oschina.net:ehlxr/ehlxr-Hexo.git (fetch) origin git@git.oschina.net:ehlxr/ehlxr-Hexo.git (push) origin git@github.com:ehlxr/ehlxr-Hexo.git (push) 也可以同时 push 到多个远程地址
$ git push origin master Everything up-to-date Everything up-to-date</content></entry><entry><title>使用Hexo基于GitHub Pages搭建个人博客（二）</title><url>/2016/07/23/%E4%BD%BF%E7%94%A8Hexo%E5%9F%BA%E4%BA%8EGitHub-Pages%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E4%BA%8C/</url><categories><category>Hexo</category></categories><tags><tag>Hexo</tag><tag>GitHub</tag></tags><content type="html">上一篇介绍了 GitHub Pages 服务开启，这篇文章将介绍 Hexo 的安装和使用以及将博客部署到 GitHub Pages 的操作。
Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 一、Node.js 安装 Hexo 是基于 Node.js 安装，所以安装 Hexo 之前首先要安装 Node.js。下载地址：https://nodejs.org/en/download/ 根据自己的操作系统，下载对应版本即可。Windons 安装过程比较简单，一直下一步即可安装完成。
CentOS 7 安装 Node.js 二、Hexo 安装 打开 Git Bash 输入以下命令开始安装 Hexo
npm install hexo-cli -g 出现以下界面表示安装完成
三、Hexo 建站 安装 Hexo 完成后，在本地磁盘创建一个目录用于存放 Hexo 配置文件和博客源代码，我的 Hexo 存放在 D:/ehlxr 目录下。
分别执行下列命令，Hexo 将会在 D:/ehlxr/Hexo 新建所需要的文件
# 进入 D:/ehlxr 目录 cd /d/ehlxr/ # 初始化所需要的文件到 Hexo 文件夹 hexo init Hexo # 进入 Hexo 文件夹 cd Hexo # 安装 npm install 分别执行以下命令在本地进行预览
# 编译生成博文 hexo generate # 启动Hexo服务器 hexo server 出现以下画面表示服务启动成功
打开浏览器输入 http://localhost:4000/ 便可以看到最原始的博客了
四、Hexo 部署到 GitHub Pages 更新 Hexo 安装文件夹中 _config.yml 文件，如图
更新 _config.yml 文件中的 deploy 节点，如图所示，可添加多个地址（如一个 GitHub、Coding、oschina）
分别执行以下命令即可完成部署
npm install hexo-deployer-git --save # 安装 hexo-deployer-git hexo clean # 可简写为 hexo cl hexo generate # 可简写为 hexo g hexo deploy # 可简写为 hexo d 访问 https://yourname.github.io 即可看到博客模板的内容，对于 Hexo 的一些详细设置可以参考 Hexo官方文档 。本站使用的 Hexo 主题是 NexT，详细设置可参考 Next主题官方网站 。
使用以下命令就可以新建一篇博客文章
hexo new &amp;#34;开始blog，哈哈&amp;#34; 打开 Hexo 目录下的 source\_posts 目录就可以看见创建的文章了，如图所示
文章是 MarkDown 格式文件（关于 Markdown 语法可参考 Markdown 语法说明(简体中文版) ），编辑文章后保存，可以先启动本地 Hexo Server 查看效果，然后执行部署命令就完成了博客文章的发布
Markdown 编辑器我推荐使用 小书匠 至此，使用 Hexo 基于 GitHub Pages 搭建个人博客就介绍完了。</content></entry><entry><title>使用Hexo基于GitHub Pages搭建个人博客（一）</title><url>/2016/07/23/%E4%BD%BF%E7%94%A8Hexo%E5%9F%BA%E4%BA%8EGitHub-Pages%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E4%B8%80/</url><categories><category>Hexo</category></categories><tags><tag>Hexo</tag><tag>GitHub</tag><tag>Git</tag></tags><content type="html"><![CDATA[ GitHub Pages 是 GitHub 提供的免费开源的静态站点托管服务，利用这个服务可以搭建轻量级的博客系统，本文介绍了如何使用 Hexo 结合 GitHub Pages 服务搭建个人博客。
一、安装配置 Git 1.1 安装 Git 使用 GitHub 首先要安装 Git，可根据操作系统下载不同版本的 Git，Windows 下载地址：https://git-scm.com/download/win 安装过程比较简单，选择默认选项配置即可下一步，不再叙述。
1.2 配置 Git 设置本地机器默认 commit 的昵称与 Email，姓名与 Email 只用于日志标识，实际推送到远程仓库时，要用有操作权限的账号登录。
git config --global user.name &#34;ehlxr&#34; git config --global user.email &#34;ehlxr@qq.com&#34; 二、生成 SSH keys SSH Keys 是 GitHub 推荐的公钥、秘钥形式验证用户合法性的机制，添加 SSH Keys 可以省去每次都要输入密码的步骤，下面将演示如何在本地计算机生成 SSH Keys 的公钥、秘钥。
2.1 首先打开安装的 Git Bash 2.2 运行 ssh-keygen 命令 输入以下命令：（替换 &quot;your_email@example.com&quot; 为注册时候的邮箱）
ssh-keygen -t rsa -b 4096 -C &#34;your_email@example.com&#34; 2.3 保存秘钥文件 当出现以下提示时，直接敲回车键选择默认地址即可（保存私钥文件位置，默认保存在当前用户文件夹下的 .ssh 文件夹中，文件名称为 id_rsa ）
Enter file in which to save the key (/c/Users/lt/.ssh/id_rsa): 如果已经存在会提示，输入：y 敲回车替换即可
/c/Users/lt/.ssh/id_rsa already exists. Overwrite (y/n)? 出现以下提示设置密码提示，不用设置密码，敲回车键即可
Enter passphrase (empty for no passphrase): 再次确认密码提示，不用设密码敲回车即可
Enter same passphrase again: 出现以下提示，表明已经成功生成了 SSH Keys
Your identification has been saved in /c/Users/lt/.ssh/id_rsa. Your public key has been saved in /c/Users/lt/.ssh/id_rsa.pub. The key fingerprint is: SHA256:dBtq6OOJs1JQat630kGqh420Y0JSb7smaR6c9jjo7h0 your_email@example.com The key&#39;s randomart image is: +---[RSA 4096]----+ | | | . | | o . o | | = .o o o | | + + o. S . | |.oo.*.o. | |oo=E +oo | |o.#+Oooo | |+OoO==o | +----[SHA256]-----+ Github 官网 SSH Keys 生成教程 三、开启 GitHub Pages 服务 GitHub 账户注册比较简单，注册免费使用的账户就可以了，这里就不细说了，需要注意的是尽量不要使用 126/163 邮箱注册，我在测试的时候收不到验证码，等收到验证码的时候已经第二天了，早就过期了，我使用 Gmail 和 QQ 邮箱都能很快收到验证码。
3.1 添加生成的 SSH Keys 到 GitHub 打开生成的 SSH Keys 公钥
登录注册的 GitHub 账户，按照以下图示 1、2、3、4、5、6、7 步骤操作，复制公钥内容粘贴到下图 步骤 5 的 Key 输入框中，在 步骤 6 输入一个标题，点击 步骤 7 的 Add SSH key 按钮，这时候会提示你输入 GitHub 账号密码，输入密码确认即可添加成功。
3.2 创建 GitHub 仓库 按照下图所示操作，仓库名称按照：yourname.github.io 格式创建，这样就可以通过 yourname.github.io 方式访问你的博客。如果创建名称为：myblog ,则博客的访问路径为 yourname.github.io/myblog
填写完仓库名称，其他选项默认，点击 Create repository 即可完成创建。
3.3 测试 GitHub Pages 服务 接下来在仓库根目录下创建一个 index.html 文件测试 GitHub Pages 服务是否开启成功。打开刚才创建的仓库，因为我们之前已经添加了 SSH Keys，所以选择复制如下图所示的 SSH 仓库连接
在电脑磁盘创建一个文件夹（我创建在：D:/lxr）在 Git Bash 分别输入以下两条命令：
cd /d/lxr/ # 进入创建的文件夹 git init # 初始化本地仓库 在本地文件夹中创建 index.html 文件，随便输入一些内容即可，命令如下：
echo &#34;&lt;h1&gt;Hello World&lt;/h1&gt;&#34; &gt;&gt; index.html # 创建 index.html 文件 然后分别执行以下命令：
git add index.html # 添加文件 git commit -m &#34;create index.html&#34; # 提交文件 git remote add origin git@github.com:ehlxr/ehlxr.github.io.git # 添加 GitHub 仓库地址，注意地址填写自己注册的 GitHub 仓库地址 git push -u origin master # push 到 GitHub 浏览器访问： yourname.github.io 如果出现 index.html 输入的内容，即 GitHub Pages 服务开启成功
]]></content></entry><entry><title>[转] JMS 基本概念</title><url>/2016/07/21/%E8%BD%AC-JMS-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</url><categories><category>Java开发技术</category></categories><tags><tag>Java</tag><tag>JMS</tag></tags><content type="html">JMS（JAVA Message Service,java消息服务）API是一个消息服务的标准或者说是规范，允许应用程序组件基于JavaEE平台创建、发送、接收和读取消息。它使分布式通信耦合度更低，消息服务更加可靠以及异步性。
基本概念 JMS是java的消息服务，JMS的客户端之间可以通过JMS服务进行异步的消息传输。
消息模型 Point-to-Point(P2P) Publish/Subscribe(Pub/Sub) 即点对点和发布订阅模型
P2P模型 P2P模式图 涉及到的概念 消息队列（Queue） 发送者(Sender) 接收者(Receiver) 每个消息都被发送到一个特定的队列，接收者从队列中获取消息。队列保留着消息，直到他们被消费或超时。
P2P模型的特点 每个消息只有一个消费者（Consumer）(即一旦被消费，消息就不再在消息队列中) 发送者和接收者之间在时间上没有依赖性，也就是说当发送者发送了消息之后，不管接收者有没有正在运行，它不会影响到消息被发送到队列 接收者在成功接收消息之后需向队列应答成功 如果你希望发送的每个消息都应该被成功处理的话，那么你需要P2P模式。
Pub/Sub模型 Pub/Sub模式图 涉及到的概念 主题（Topic） 发布者（Publisher） 订阅者（Subscriber） 客户端将消息发送到主题。多个发布者将消息发送到Topic,系统将这些消息传递给多个订阅者。
Pub/Sub模型的特点 每个消息可以有多个消费者 发布者和订阅者之间有时间上的依赖性。针对某个主题（Topic）的订阅者，它必须创建一个订阅者之后，才能消费发布者的消息，而且为了消费消息，订阅者必须保持运行的状态。 为了缓和这样严格的时间相关性，JMS允许订阅者创建一个可持久化的订阅。这样，即使订阅者没有被激活（运行），它也能接收到发布者的消息。 如果你希望发送的消息可以不被做任何处理、或者被一个消息者处理、或者可以被多个消费者处理的话，那么可以采用Pub/Sub模型。
消息的消费 在JMS中，消息的产生和消息是异步的。对于消费来说，JMS的消息者可以通过两种方式来消费消息。
同步 订阅者或接收者调用receive方法来接收消息，receive方法在能够接收到消息之前（或超时之前）将一直阻塞 异步 订阅者或接收者可以注册为一个消息监听器。当消息到达之后，系统自动调用监听器的onMessage方法 JMS编程模型 ConnectionFactory 创建Connection对象的工厂，针对两种不同的jms消息模型，分别有QueueConnectionFactory和TopicConnectionFactory两种。可以通过JNDI来查找ConnectionFactory对象。
Destination Destination的意思是消息生产者的消息发送目标或者说消息消费者的消息来源。对于消息生产者来说，它的Destination是某个队列（Queue）或某个主题（Topic）;对于消息消费者来说，它的Destination也是某个队列或主题（即消息来源）。
所以，Destination实际上就是两种类型的对象：Queue、Topic可以通过JNDI来查找Destination。
Connection Connection表示在客户端和JMS系统之间建立的链接（对TCP/IP socket的包装）。Connection可以产生一个或多个Session。跟ConnectionFactory一样，Connection也有两种类型：QueueConnection和TopicConnection。
Session Session是我们操作消息的接口。可以通过session创建生产者、消费者、消息等。Session提供了事务的功能。当我们需要使用session发送/接收多个消息时，可以将这些发送/接收动作放到一个事务中。同样，也分QueueSession和TopicSession。
消息的生产者 消息生产者由Session创建，并用于将消息发送到Destination。同样，消息生产者分两种类型：QueueSender和TopicPublisher。可以调用消息生产者的方法（send或publish方法）发送消息。
消息消费者 消息消费者由Session创建，用于接收被发送到Destination的消息。两种类型：QueueReceiver和TopicSubscriber。可分别通过session的createReceiver(Queue)或createSubscriber(Topic)来创建。当然，也可以session的creatDurableSubscriber方法来创建持久化的订阅者。
MessageListener 消息监听器。如果注册了消息监听器，一旦消息到达，将自动调用监听器的onMessage方法。EJB中的MDB（Message-Driven Bean）就是一种MessageListener。
企业消息系统的好处 我们先来看看下图，应用程序A将Message发送到服务器上，然后应用程序B从服务器中接收A发来的消息，通过这个图我们一起来分析一下JMS的好处：
提供消息灵活性 松散耦合 异步性 原文地址</content></entry><entry><title>Java List与数组之间的转换</title><url>/2016/07/20/Java-List%E4%B8%8E%E6%95%B0%E7%BB%84%E4%B9%8B%E9%97%B4%E7%9A%84%E8%BD%AC%E6%8D%A2/</url><categories><category>Java开发技术</category></categories><tags><tag>Java</tag><tag>List</tag></tags><content type="html"><![CDATA[1. 数组转换为List String[] arr = new String[] {&#34;str1&#34;, &#34;str2&#34;}; List&lt;String&gt; list = Arrays.asList(arr); 2 .List转换为数组 List&lt;String&gt; list = new ArrayList&lt;String&gt;(); list.add(&#34;str1&#34;); list.add(&#34;str2&#34;); int size = list.size(); String[] arr = list.toArray(new String[size]); ]]></content></entry><entry><title>MySQL日期函数</title><url>/2016/07/20/MySQL%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0/</url><categories><category>SQL</category></categories><tags><tag>MySQL</tag></tags><content type="html"><![CDATA[1. 取得当天 mysql&gt; SELECT curdate(); +------------+ | curdate() | +------------+ | 2013-07-29 | +------------+ 2. 取得当前日期 mysql&gt; select sysdate(); +---------------------+ | sysdate() | +---------------------+ | 2016-05-30 13:58:17 | +---------------------+ 1 row in set mysql&gt; select now(); +---------------------+ | now() | +---------------------+ | 2016-05-30 13:58:29 | +---------------------+ 1 row in set 3. 取得当前时间 mysql&gt; select curtime(); +-----------+ | curtime() | +-----------+ | 13:54:31 | +-----------+ 1 row in set 4. 取得前一天 mysql&gt; select date_sub(curdate(),interval 1 day); +------------------------------------+ | date_sub(curdate(),interval 1 day) | +------------------------------------+ | 2013-07-28 | +------------------------------------+ 5. 取得下一天 mysql&gt; SELECT DATE_ADD(NOW(),INTERVAL 1 DAY); +--------------------------------+ | DATE_ADD(NOW(),INTERVAL 1 DAY) | +--------------------------------+ | 2016-05-31 14:09:16 | +--------------------------------+ 1 row in set 括号中为当天时间的前一天，如果统计前几天就将括号中的’1’改成相应的天数。如果要算月或年，直接将day改为month或year即可
6. 取得前一天的年份 mysql&gt; SELECT YEAR(DATE_SUB(CURDATE(),INTERVAL 1 DAY)); +------------------------------------------+ | YEAR(DATE_SUB(CURDATE(),INTERVAL 1 DAY)) | +------------------------------------------+ | 2013 | +------------------------------------------+ 7. SYSDATE()为：2016-05-30 周一 SELECT WEEK(SYSDATE()); -- 22 SELECT WEEKOFYEAR(SYSDATE()); -- 22 SELECT WEEKDAY(SYSDATE()); -- 0 SELECT DAY(SYSDATE()); -- 30 SELECT MONTH(SYSDATE()); -- 5 SELECT YEAR(SYSDATE()); -- 2016 8. date_sub()函数的例子 今天是2013年5月20日
date_sub(&#39;2012-05-25&#39;, INTERVAL 1 DAY) -- 表示 2012-05-24 date_sub(&#39;2012-05-25&#39;, INTERVAL 0 DAY) -- 表示 2012-05-25 date_sub(&#39;2012-05-25&#39;, INTERVAL - 1 DAY) -- 表示 2012-05-26 date_sub(&#39;2012-05-31&#39;, INTERVAL - 1 DAY) -- 表示 2012-06-01 date_sub(curdate(), INTERVAL 1 DAY) -- 表示 2013-05-19 date_sub(curdate(), INTERVAL - 1 DAY) -- 表示 2013-05-21 date_sub(curdate(), INTERVAL 1 MONTH) -- 表示 2013-04-20 date_sub(curdate(), INTERVAL - 1 MONTH) -- 表示 2013-06-20 date_sub(curdate(), INTERVAL 1 YEAR) -- 表示 2012-05-20 date_sub(curdate(), INTERVAL - 1 YEAR) -- 表示 2014-05-20 ]]></content></entry><entry><title>开始blog，哈哈</title><url>/2016/07/19/%E5%BC%80%E5%A7%8Bblog%E5%93%88%E5%93%88/</url><categories><category>杂谈</category></categories><tags><tag>hello</tag></tags><content type="html">public static void main(String args[]){ System.out.println(&amp;#34;hello world&amp;#34;); }</content></entry><entry><title>Hello World</title><url>/2016/07/19/Hello-World/</url><categories><category>杂谈</category></categories><tags><tag>hello</tag><tag>杂谈</tag></tags><content type="html">Welcome to Hexo ! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub .
Quick Start Create a new post $ hexo new &amp;#34;My New Post&amp;#34; More info: Writing Run server $ hexo server More info: Server Generate static files $ hexo generate More info: Generating Deploy to remote sites $ hexo deploy More info: Deployment</content></entry></search>